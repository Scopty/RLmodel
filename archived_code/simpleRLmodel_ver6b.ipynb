{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5233f921-8873-4bb3-8aba-14cd38bb361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil\n",
    "\n",
    "# Determine number of available (idle) cores\n",
    "IDLE_THRESHOLD = 20.0  # percent\n",
    "cpu_usages = psutil.cpu_percent(percpu=True, interval=1)\n",
    "available_cores = sum(usage < IDLE_THRESHOLD for usage in cpu_usages)\n",
    "available_cores = max(1, available_cores) - 4 # At least 1\n",
    "\n",
    "available_cores = os.cpu_count()\n",
    "\n",
    "import mplfinance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stable-Baselines3 imports\n",
    "from stable_baselines3 import PPO,A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv  # Use this instead of DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "# Gymnasium (updated from Gym)\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env,Wrapper\n",
    "from gymnasium.spaces import Discrete, Box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31586fdb-7feb-43af-a1cf-106a65064180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"MGOL.csv\")  # Replace with actual file\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], format='%m/%d/%y %H:%M')\n",
    "df.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ae36aaa-2abe-42a7-91da-dcd6904e9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "df.index = df.index + pd.Timedelta(hours=3)\n",
    "df.index.name = 'Date'\n",
    "df = df.drop(columns=['symbol', 'frame'])\n",
    "df = df.iloc[:10]  # Select the first 30 rows\n",
    "df_original = df\n",
    "df = df[[\"close\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8135381-1f8b-4bb9-a04f-f83d49d194fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ActionMasker\n",
    "\n",
    "class ActionMasker(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Wrapper for action masking in environments.\n",
    "    Adds action mask as a part of the environment step.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env, mask_fn: callable):\n",
    "        super().__init__(env)\n",
    "        self.mask_fn = mask_fn\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Get the action mask\n",
    "        action_mask = self.mask_fn(self.env)\n",
    "        \n",
    "        # Add the action mask to the info dictionary\n",
    "        info['action_mask'] = action_mask\n",
    "        \n",
    "        return obs, reward, done, truncated, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c508fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedPPOPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.debug = False  # Add debug flag\n",
    "\n",
    "    def forward(self, combined_obs, action_mask=None, deterministic=False):\n",
    "        combined_obs = combined_obs.to(dtype=torch.float32)\n",
    "        \n",
    "        # Debug: Print observation and action mask\n",
    "        if self.debug and np.random.random() < 0.01:  # Print 1% of steps to avoid too much output\n",
    "            print(f\"\\n=== Debug: Forward Pass ===\")\n",
    "            print(f\"Observation shape: {combined_obs.shape}\")\n",
    "            print(f\"Action mask provided: {action_mask is not None}\")\n",
    "\n",
    "        latent_pi, latent_vf = self.mlp_extractor(combined_obs.to(dtype=torch.float32))  \n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        values = self.value_net(latent_vf).to(dtype=torch.float32)\n",
    "\n",
    "        # Extract action mask from observation if not provided\n",
    "        if action_mask is None:\n",
    "            action_mask = combined_obs[:, -3:]  # Assuming last 3 elements are action mask\n",
    "            if self.debug:\n",
    "                print(\"Extracted action mask from obs:\", action_mask)\n",
    "\n",
    "        if action_mask is not None:\n",
    "            action_mask_tensor = torch.as_tensor(action_mask, dtype=torch.float32, device=combined_obs.device)\n",
    "            action_mask_tensor = action_mask_tensor.view(-1, distribution.distribution.logits.shape[-1])\n",
    "            \n",
    "            if self.debug:\n",
    "                print(\"\\nBefore masking:\")\n",
    "                print(\"Logits:\", distribution.distribution.logits)\n",
    "                print(\"Action mask:\", action_mask_tensor)\n",
    "                print(\"Invalid actions (mask=0):\", (action_mask_tensor == 0).nonzero(as_tuple=True))\n",
    "\n",
    "            # Apply mask\n",
    "            distribution.distribution.logits = distribution.distribution.logits.masked_fill(\n",
    "                action_mask_tensor == 0, -1e9\n",
    "            )\n",
    "\n",
    "            if self.debug:\n",
    "                print(\"\\nAfter masking:\")\n",
    "                print(\"Logits:\", distribution.distribution.logits)\n",
    "                print(\"Action probs:\", torch.softmax(distribution.distribution.logits, dim=-1))\n",
    "\n",
    "        # Action selection\n",
    "        if deterministic:\n",
    "            actions = torch.argmax(distribution.distribution.probs, dim=-1)\n",
    "        else:\n",
    "            actions = distribution.sample()\n",
    "\n",
    "        log_probs = distribution.log_prob(actions)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"\\nAction selection:\")\n",
    "            print(\"Selected actions:\", actions)\n",
    "            print(\"Action probabilities:\", torch.softmax(distribution.distribution.logits, dim=-1))\n",
    "            print(\"=\"*50)\n",
    "\n",
    "        return actions, values, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c85e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest DebugCallback\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class DebugCallback(BaseCallback):\n",
    "    def __init__(self, debug_episodes=None, verbose=0, debug=True):\n",
    "        super().__init__(verbose)\n",
    "        self.debug_episodes = set(debug_episodes) if debug_episodes is not None else set()\n",
    "        self.episode_counts = []\n",
    "        self.episode_step_counts = {}\n",
    "        self.debug = debug\n",
    "        self.debug_triggered = False\n",
    "        self.debug_on_step = None\n",
    "        self.last_printed_episodes = []\n",
    "        self.max_episode_steps = {}\n",
    "        self.action_probs = []\n",
    "        self.actions = []\n",
    "        self.episode_data = {}  # Store data per episode\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        n_envs = self.training_env.num_envs\n",
    "        self.episode_counts = [0] * n_envs\n",
    "        self.last_printed_episodes = [None] * n_envs\n",
    "        self.episode_step_counts = {i: 0 for i in range(n_envs)}\n",
    "        self.max_episode_steps = {i: 0 for i in range(n_envs)}\n",
    "        self.episode_data = {i: {'actions': [], 'probs': [], 'rewards': []} for i in range(n_envs)}\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        num_envs = getattr(self.training_env, \"num_envs\", 1)\n",
    "        self.episode_counts = [0] * num_envs\n",
    "        self.current_episode_steps = [0] * num_envs\n",
    "\n",
    "    def on_training_end(self) -> None:\n",
    "        for env_id, max_steps in self.max_episode_steps.items():\n",
    "            if self.debug: \n",
    "                print(f\"\\n=== Training Summary (Env {env_id}) ===\")\n",
    "                print(f\"Max steps in episode: {max_steps}\")\n",
    "                print(f\"Total episodes completed: {self.episode_counts[env_id]}\")\n",
    "                print(f\"Debug episodes: {sorted(self.debug_episodes)}\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        dones = self.locals.get(\"dones\", [])\n",
    "        infos = self.locals.get(\"infos\", [])\n",
    "        actions = self.locals.get(\"actions\", [])\n",
    "        \n",
    "        # Get action probabilities\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.as_tensor(self.locals.get(\"new_obs\"), device=self.model.device)\n",
    "            action_probs = self.model.policy.get_distribution(obs_tensor).distribution.probs\n",
    "            if isinstance(action_probs, torch.Tensor):\n",
    "                action_probs = action_probs.cpu().numpy()\n",
    "\n",
    "        for i in range(len(dones)):\n",
    "            if i not in self.episode_step_counts:\n",
    "                self.episode_step_counts[i] = 0\n",
    "                self.episode_data[i] = {'actions': [], 'probs': [], 'rewards': []}\n",
    "\n",
    "            self.episode_step_counts[i] += 1\n",
    "            episode_num = self.episode_counts[i]\n",
    "\n",
    "            # Store action and probabilities\n",
    "            if i < len(actions) and i < len(action_probs):\n",
    "                self.episode_data[i]['actions'].append(actions[i])\n",
    "                self.episode_data[i]['probs'].append(action_probs[i])\n",
    "                if 'rewards' in self.locals:\n",
    "                    self.episode_data[i]['rewards'].append(self.locals['rewards'][i] if isinstance(self.locals['rewards'], (list, np.ndarray)) else self.locals['rewards'])\n",
    "\n",
    "            # Track max steps\n",
    "            if self.episode_step_counts[i] > self.max_episode_steps[i]:\n",
    "                self.max_episode_steps[i] = self.episode_step_counts[i]\n",
    "\n",
    "            # Debug output for specified episodes\n",
    "            if episode_num in self.debug_episodes and i == 0:\n",
    "                if self.last_printed_episodes[i] != episode_num:\n",
    "                    print(f\"\\n=== Debugging Episode {episode_num} (Env {i}) ===\")\n",
    "                    self.last_printed_episodes[i] = episode_num\n",
    "                \n",
    "                # Print step information\n",
    "                print(f\"\\nStep {self.episode_step_counts[i]}:\")\n",
    "                print(f\"Action: {actions[i] if i < len(actions) else 'N/A'}\")\n",
    "                if i < len(action_probs):\n",
    "                    print(\"Action Probabilities:\")\n",
    "                    for action_idx, prob in enumerate(action_probs[i]):\n",
    "                        print(f\"  Action {action_idx}: {prob:.4f}\")\n",
    "                print(f\"Reward: {infos[i].get('reward', 'N/A') if i < len(infos) else 'N/A'}\")\n",
    "                print(f\"Done: {dones[i] if i < len(dones) else 'N/A'}\")\n",
    "                if i < len(infos):\n",
    "                    print(\"Info:\", {k: v for k, v in infos[i].items() if k != 'terminal_observation'})\n",
    "\n",
    "            # Handle episode completion\n",
    "            if dones[i] and i < len(self.episode_data):\n",
    "                if episode_num in self.debug_episodes and i == 0:\n",
    "                    print(f\"\\n=== Episode {episode_num} (Env {i}) Summary ===\")\n",
    "                    print(f\"Total steps: {self.episode_step_counts[i]}\")\n",
    "                    print(f\"Actions taken: {self.episode_data[i]['actions']}\")\n",
    "                    print(\"Action probabilities history:\")\n",
    "                    for step, probs in enumerate(self.episode_data[i]['probs']):\n",
    "                        print(f\"  Step {step + 1}: {probs}\")\n",
    "                    print(f\"Rewards: {self.episode_data[i]['rewards']}\")\n",
    "                    print(f\"Total reward: {sum(self.episode_data[i]['rewards']):.2f}\")\n",
    "                \n",
    "                # Reset for next episode\n",
    "                self.episode_counts[i] += 1\n",
    "                self.episode_step_counts[i] = 0\n",
    "                self.episode_data[i] = {'actions': [], 'probs': [], 'rewards': []}\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest RewardCallback\n",
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, debug=False, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.debug = debug\n",
    "        self.n_envs = None\n",
    "        self.episode_count = 0\n",
    "        self.episode_count_per_env = None\n",
    "        self.total_steps_per_env = None\n",
    "        \n",
    "        # New tracking\n",
    "        self.current_episode_rewards = None\n",
    "        self.episode_rewards = None\n",
    "        self.invalid_actions = None\n",
    "        self.current_steps = None\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.n_envs = self.training_env.num_envs\n",
    "        self.episode_rewards = []\n",
    "        self.episode_invalid_actions = []\n",
    "        self.episode_count_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.total_steps_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.episode_reward_per_env = [0.0 for _ in range(self.n_envs)]\n",
    "        self.invalid_actions_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.current_steps_per_env = [0 for _ in range(self.n_envs)]\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        infos = self.locals[\"infos\"]\n",
    "\n",
    "        for i in range(self.n_envs):\n",
    "            # Increment step counters for every step\n",
    "            self.current_steps_per_env[i] += 1\n",
    "            self.total_steps += 1\n",
    "            self.total_steps_per_env[i] += 1\n",
    "            \n",
    "            # Update rewards and check for invalid actions\n",
    "            self.episode_reward_per_env[i] += rewards[i]\n",
    "            \n",
    "            if infos[i].get(\"is_invalid\", False):\n",
    "                self.invalid_actions_per_env[i] += 1\n",
    "\n",
    "            # Handle episode completion\n",
    "            if dones[i]:\n",
    "                self.episode_count += 1\n",
    "                self.episode_count_per_env[i] += 1\n",
    "                \n",
    "                if self.debug and i == 0:\n",
    "                    print(f\"[Env {i}] Episode {self.episode_count_per_env[i]} Done. \"\n",
    "                        f\"Steps: {self.current_steps_per_env[i]}, \"\n",
    "                        f\"Reward: {self.episode_reward_per_env[i]:.2f}, \"\n",
    "                        f\"Invalid: {self.invalid_actions_per_env[i]}\")\n",
    "\n",
    "        if all(dones):     \n",
    "            # Log mean reward across all environments\n",
    "            self.episode_rewards.append(sum(self.episode_reward_per_env))\n",
    "            self.episode_invalid_actions.append(sum(self.invalid_actions_per_env))\n",
    "            total_reward = sum(self.episode_rewards)\n",
    "            self.logger.record(\"rewards/total_reward\", total_reward)         \n",
    "\n",
    "        for i in range(self.n_envs):\n",
    "            if dones[i]:\n",
    "                self.current_steps_per_env[i] = 0\n",
    "                self.episode_reward_per_env[i] = 0.0\n",
    "                self.invalid_actions_per_env[i] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        print(\"\\n=== Final Training Summary ===\")\n",
    "        print(f\"Number of environments: {self.n_envs}\")\n",
    "        print(f\"Total episodes completed: {self.episode_count}\")\n",
    "        print(f\"Average episodes per environment: {self.episode_count / self.n_envs:.1f}\")\n",
    "        print(f\"Total steps taken: {self.total_steps}\")\n",
    "        print(f\"Average steps per episode: {self.total_steps / self.episode_count:.1f}\")\n",
    "        \n",
    "        for i in range(self.n_envs):\n",
    "            if self.debug and i == 0:\n",
    "                print(f\"\\n[Env {i}] Summary:\")\n",
    "                print(f\"Total episodes: {self.episode_count_per_env[i]}\")\n",
    "                print(f\"Total steps: {self.total_steps_per_env[i]}\")\n",
    "                print(f\"Average steps per episode: {self.total_steps_per_env[i] / self.episode_count_per_env[i]:.1f}\")\n",
    "        \n",
    "        # Print old format for compatibility\n",
    "        formatted_rewards = ', '.join(f\"{reward:.1f}\" for reward in self.episode_rewards)\n",
    "        formatted_invalid_actions = ', '.join(str(invalid) for invalid in self.episode_invalid_actions)\n",
    "        print(\"\\nTotal rewards per episode per env:\", formatted_rewards)\n",
    "        print(\"Invalid actions per episode per env:\", formatted_invalid_actions)\n",
    "\n",
    "        # Group by iterations (12 environments at a time)\n",
    "        n_envs = self.n_envs\n",
    "        n_iterations = len(self.episode_rewards) // n_envs\n",
    "\n",
    "        \"\"\" print(\"\\n=== Per-Iteration Summary ===\")\n",
    "        for i in range(n_iterations):\n",
    "            start_idx = i * n_envs\n",
    "            end_idx = (i + 1) * n_envs\n",
    "            iter_rewards = self.episode_rewards[start_idx:end_idx]\n",
    "            iter_invalid = self.episode_invalid_actions[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"\\nIteration {i + 1}:\")\n",
    "            print(f\"  Total rewards: {sum(iter_rewards):.1f}\")\n",
    "            print(f\"  Total invalid actions: {sum(iter_invalid)}\")\n",
    "            print(f\"  Avg reward per env: {np.mean(iter_rewards):.1f}\")\n",
    "            print(f\"  Avg invalid per env: {np.mean(iter_invalid):.1f}\") \"\"\"\n",
    "\n",
    "        # Print overall totals\n",
    "        print(\"\\n=== Combined Totals ===\")\n",
    "        print(f\"Total combined rewards: {sum(self.episode_rewards):.1f}\")\n",
    "        print(f\"Total invalid actions: {sum(self.episode_invalid_actions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4b32a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, debug=False, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.debug = debug\n",
    "        self.n_envs = None\n",
    "        self.total_timesteps = 0\n",
    "        self.total_episodes = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_invalid_actions = []\n",
    "        self.normalized_rewards = []\n",
    "        self.unnormalized_rewards = []\n",
    "        self.total_steps_per_env = []\n",
    "        self.normalized_reward_per_env = []\n",
    "        self.unnormalized_reward_per_env = []\n",
    "        self.invalid_actions_per_env = []\n",
    "        self.current_steps_per_env = []\n",
    "        self.current_episode = 0\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.n_envs = self.training_env.num_envs\n",
    "        self.total_steps_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.normalized_reward_per_env = [0.0 for _ in range(self.n_envs)]\n",
    "        self.unnormalized_reward_per_env = [0.0 for _ in range(self.n_envs)]\n",
    "        self.invalid_actions_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.current_steps_per_env = [0 for _ in range(self.n_envs)]\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        infos = self.locals[\"infos\"]\n",
    "        self.total_timesteps += 1\n",
    "\n",
    "        for i in range(self.n_envs):\n",
    "            # Increment step counters\n",
    "            self.current_steps_per_env[i] += 1\n",
    "            self.total_steps_per_env[i] += 1\n",
    "            \n",
    "            # Update rewards\n",
    "            self.normalized_reward_per_env[i] += rewards[i]\n",
    "            unnormalized_reward = infos[i].get('original_reward', rewards[i])\n",
    "            self.unnormalized_reward_per_env[i] += unnormalized_reward\n",
    "            \n",
    "            # Track invalid actions\n",
    "            if infos[i].get(\"is_invalid\", False):\n",
    "                self.invalid_actions_per_env[i] += 1\n",
    "\n",
    "            # Handle episode completion\n",
    "            if dones[i]:\n",
    "                self.total_episodes += 1\n",
    "                self.current_episode += 1\n",
    "                \n",
    "                # Store both rewards and invalid actions when episode ends\n",
    "                self.episode_rewards.append(self.normalized_reward_per_env[i])\n",
    "                self.unnormalized_rewards.append(self.unnormalized_reward_per_env[i])\n",
    "                self.episode_invalid_actions.append(self.invalid_actions_per_env[i])\n",
    "                \n",
    "                if self.debug and i == 0:\n",
    "                    print(f\"[Env {i}] Episode {self.current_episode} Done.\")\n",
    "                    print(f\"  Steps: {self.current_steps_per_env[i]}\")\n",
    "                    print(f\"  Normalized Reward: {self.normalized_reward_per_env[i]:.2f}\")\n",
    "                    print(f\"  Unnormalized Reward: {self.unnormalized_reward_per_env[i]:.2f}\")\n",
    "                    print(f\"  Invalid: {self.invalid_actions_per_env[i]}\")\n",
    "\n",
    "                # Reset episode variables\n",
    "                self.current_steps_per_env[i] = 0\n",
    "                self.normalized_reward_per_env[i] = 0.0\n",
    "                self.unnormalized_reward_per_env[i] = 0.0\n",
    "                self.invalid_actions_per_env[i] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        print(\"\\n=== Final Training Summary ===\")\n",
    "        print(f\"Number of environments: {self.n_envs}\")\n",
    "        print(f\"Total episodes completed: {self.total_episodes}\")\n",
    "        print(f\"Total timesteps: {self.total_timesteps}\")\n",
    "        print(f\"Total steps per environment: {sum(self.total_steps_per_env)}\")\n",
    "        \n",
    "        if self.total_episodes > 0:\n",
    "            # Calculate averages only if we have completed episodes\n",
    "            avg_steps = sum(self.total_steps_per_env) / self.total_episodes\n",
    "            avg_normalized_reward = np.mean(self.episode_rewards)\n",
    "            avg_unnormalized_reward = np.mean(self.unnormalized_rewards)\n",
    "            avg_invalid_actions = np.mean(self.episode_invalid_actions)\n",
    "            \n",
    "            print(f\"Average steps per episode: {avg_steps:.1f}\")\n",
    "            print(f\"Average normalized reward per episode: {avg_normalized_reward:.1f}\")\n",
    "            print(f\"Average unnormalized reward per episode: {avg_unnormalized_reward:.1f}\")\n",
    "            print(f\"Average invalid actions per episode: {avg_invalid_actions:.1f}\")\n",
    "        else:\n",
    "            print(\"No episodes completed\")\n",
    "        \n",
    "        # Print reward statistics\n",
    "        print(\"\\n=== Reward Statistics ===\")\n",
    "        print(f\"Total combined normalized rewards: {sum(self.episode_rewards):.1f}\")\n",
    "        print(f\"Total combined unnormalized rewards: {sum(self.unnormalized_rewards):.1f}\")\n",
    "        \n",
    "        # Print invalid actions statistics\n",
    "        print(\"\\n=== Invalid Actions ===\")\n",
    "        print(f\"Total invalid actions: {sum(self.episode_invalid_actions)}\")\n",
    "        print(f\"Invalid actions per episode: {self.episode_invalid_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f455e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, debug=False, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.debug = debug\n",
    "        self.n_envs = None\n",
    "        self.total_timesteps = 0\n",
    "        self.total_episodes = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_invalid_actions = []\n",
    "        self.normalized_rewards = []\n",
    "        self.unnormalized_rewards = []\n",
    "        self.total_steps_per_env = []\n",
    "        self.normalized_reward_per_env = []\n",
    "        self.unnormalized_reward_per_env = []\n",
    "        self.invalid_actions_per_env = []\n",
    "        self.current_steps_per_env = []\n",
    "        self.current_episode = 0\n",
    "        self.first_step = True\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        # Get the actual number of environments from the training environment\n",
    "        if hasattr(self.training_env, 'venv'):\n",
    "            self.n_envs = self.training_env.venv.num_envs\n",
    "        else:\n",
    "            self.n_envs = 1  # Default for single environment\n",
    "            \n",
    "        self.total_steps_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.normalized_reward_per_env = [0.0 for _ in range(self.n_envs)]\n",
    "        self.unnormalized_reward_per_env = [0.0 for _ in range(self.n_envs)]\n",
    "        self.invalid_actions_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.current_steps_per_env = [0 for _ in range(self.n_envs)]\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"\\n=== Environment Structure ===\")\n",
    "            print(f\"Number of environments: {self.n_envs}\")\n",
    "            print(f\"Training env type: {type(self.training_env)}\")\n",
    "            print(f\"Is CustomVecNormalize: {isinstance(self.training_env, CustomVecNormalize)}\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        infos = self.locals[\"infos\"]\n",
    "        self.total_timesteps += 1\n",
    "\n",
    "        for i in range(self.n_envs):\n",
    "            # Get normalized reward\n",
    "            normalized_reward = rewards[i]\n",
    "            \n",
    "            # Try to get unnormalized reward from info\n",
    "            unnormalized_reward = infos[i].get('original_reward', None)\n",
    "            if unnormalized_reward is None:\n",
    "                # If not found, try to get it from VecNormalize\n",
    "                if hasattr(self.training_env, 'buffer'):\n",
    "                    unnormalized_reward = self.training_env.buffer.rewards[-1][i]\n",
    "                if unnormalized_reward is None:\n",
    "                    unnormalized_reward = rewards[i]  # Fallback to normalized\n",
    "                    if self.debug:\n",
    "                        print(\"Warning: Could not find unnormalized reward, using normalized\")\n",
    "            \n",
    "            # Update rewards\n",
    "            self.normalized_reward_per_env[i] += normalized_reward\n",
    "            self.unnormalized_reward_per_env[i] += unnormalized_reward\n",
    "            \n",
    "            # Track invalid actions\n",
    "            if infos[i].get(\"invalid_action\", False):\n",
    "                self.invalid_actions_per_env[i] += 1\n",
    "\n",
    "            # Handle episode completion\n",
    "            if dones[i]:\n",
    "                self.total_episodes += 1\n",
    "                self.current_episode += 1\n",
    "                \n",
    "                # Store both rewards when episode ends\n",
    "                self.episode_rewards.append(self.normalized_reward_per_env[i])\n",
    "                self.unnormalized_rewards.append(self.unnormalized_reward_per_env[i])\n",
    "                \n",
    "                if self.debug and i == 0:\n",
    "                    print(f\"[Env {i}] Episode {self.current_episode} Done.\")\n",
    "                    print(f\"  Steps: {self.current_steps_per_env[i]}\")\n",
    "                    print(f\"  Normalized Reward: {self.normalized_reward_per_env[i]:.2f}\")\n",
    "                    print(f\"  Unnormalized Reward: {self.unnormalized_reward_per_env[i]:.2f}\")\n",
    "                    print(f\"  Invalid: {self.invalid_actions_per_env[i]}\")\n",
    "\n",
    "                # Reset episode variables\n",
    "                self.current_steps_per_env[i] = 0\n",
    "                self.normalized_reward_per_env[i] = 0.0\n",
    "                self.unnormalized_reward_per_env[i] = 0.0\n",
    "                self.invalid_actions_per_env[i] = 0\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "026fb95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingDebugCallback\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class TrainingDebugCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that logs important training metrics.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0, log_freq=100, debug = False):\n",
    "        super().__init__(verbose)\n",
    "        self.log_freq = log_freq\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_invalid_actions = []\n",
    "        self.current_episode_reward = 0\n",
    "        self.current_episode_length = 0\n",
    "        self.current_episode_invalid_actions = 0\n",
    "        self.n_calls = 0\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        # Initialize lists to track metrics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_invalid_actions = []\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Get info from the environment\n",
    "        infos = self.locals.get('infos', [{}])\n",
    "        dones = self.locals.get('dones', [False])\n",
    "        \n",
    "        # Track metrics for each environment\n",
    "        for i, (done, info) in enumerate(zip(dones, infos)):\n",
    "            # Update metrics\n",
    "            self.current_episode_reward += info.get('reward', 0)\n",
    "            self.current_episode_length += 1\n",
    "            self.current_episode_invalid_actions += 1 if info.get('invalid_action', False) else 0\n",
    "            \n",
    "            # If episode is done\n",
    "            if done:\n",
    "                self.episode_rewards.append(self.current_episode_reward)\n",
    "                self.episode_lengths.append(self.current_episode_length)\n",
    "                self.episode_invalid_actions.append(self.current_episode_invalid_actions)\n",
    "                \n",
    "                # Log metrics\n",
    "                if len(self.episode_rewards) % self.log_freq == 0:\n",
    "                    mean_reward = np.mean(self.episode_rewards[-self.log_freq:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-self.log_freq:])\n",
    "                    mean_invalid = np.mean(self.episode_invalid_actions[-self.log_freq:])\n",
    "                    \n",
    "                    print(f\"Episode {len(self.episode_rewards)}\")\n",
    "                    print(f\"Mean reward: {mean_reward:.2f}\")\n",
    "                    print(f\"Mean episode length: {mean_length:.1f}\")\n",
    "                    print(f\"Mean invalid actions per episode: {mean_invalid:.2f}\")\n",
    "                    print(\"---\")\n",
    "                \n",
    "                # Reset for next episode\n",
    "                self.current_episode_reward = 0\n",
    "                self.current_episode_length = 0\n",
    "                self.current_episode_invalid_actions = 0\n",
    "        \n",
    "        self.n_calls += 1\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        # Final summary\n",
    "        if len(self.episode_rewards) > 0:\n",
    "            print(\"\\n=== Training Complete ===\")\n",
    "            print(f\"Total episodes: {len(self.episode_rewards)}\")\n",
    "            print(f\"Mean reward: {np.mean(self.episode_rewards):.2f} Â± {np.std(self.episode_rewards):.2f}\")\n",
    "            print(f\"Min reward: {np.min(self.episode_rewards):.2f}\")\n",
    "            print(f\"Max reward: {np.max(self.episode_rewards):.2f}\")\n",
    "            print(f\"Mean episode length: {np.mean(self.episode_lengths):.1f}\")\n",
    "            print(f\"Mean invalid actions per episode: {np.mean(self.episode_invalid_actions):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f81dbf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomVecNormalize\n",
    "\n",
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvWrapper, VecEnvStepReturn\n",
    "from stable_baselines3.common.running_mean_std import RunningMeanStd\n",
    "from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import inspect\n",
    "from stable_baselines3.common import utils\n",
    "from gym import spaces\n",
    "\n",
    "class CustomVecNormalize(VecEnvWrapper):\n",
    "    \"\"\"\n",
    "    A custom version of VecNormalize that properly handles action masks in the observation space.\n",
    "    The last 3 elements of the observation are treated as action masks and are not normalized.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        venv: VecEnv,\n",
    "        training: bool = True,\n",
    "        norm_obs: bool = True,\n",
    "        norm_reward: bool = True,\n",
    "        clip_obs: float = 10.0,\n",
    "        clip_reward: float = 10.0,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon: float = 1e-8,\n",
    "        norm_obs_keys: Optional[List[str]] = None,\n",
    "    ):\n",
    "        print('init CustomVecNormalize')\n",
    "        \n",
    "        # Initialize the parent class\n",
    "        VecEnvWrapper.__init__(self, venv)\n",
    "        \n",
    "        # Store parameters\n",
    "        self.norm_obs = norm_obs\n",
    "        self.norm_reward = norm_reward\n",
    "        self.clip_obs = clip_obs\n",
    "        self.clip_reward = clip_reward\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.training = training\n",
    "        self.norm_obs_keys = norm_obs_keys\n",
    "        \n",
    "        # Initialize observation normalization\n",
    "        if self.norm_obs:\n",
    "            if isinstance(self.observation_space, spaces.Dict):\n",
    "                self.obs_spaces = self.observation_space.spaces\n",
    "                self.obs_rms = {key: RunningMeanStd(shape=space.shape) \n",
    "                              for key, space in self.obs_spaces.items()}\n",
    "            else:\n",
    "                # Initialize with the full observation space shape\n",
    "                obs_shape = self.observation_space.shape\n",
    "                # For the main observation space, we'll normalize all but the last 3 elements\n",
    "                self.obs_rms = RunningMeanStd(shape=(obs_shape[0] - 3,))\n",
    "        else:\n",
    "            self.obs_rms = None\n",
    "            \n",
    "        # Initialize reward normalization\n",
    "        self.ret_rms = RunningMeanStd(shape=())\n",
    "        self.returns = np.zeros(self.num_envs)\n",
    "        self.old_obs = None\n",
    "        self.old_reward = np.array([])\n",
    "\n",
    "    def _normalize_obs(self, obs: np.ndarray, obs_rms: RunningMeanStd) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalize observations while preserving the action mask (last 3 elements).\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(obs, dict) and isinstance(obs_rms, dict):\n",
    "            # Handle dictionary observations\n",
    "            normalized_obs = {}\n",
    "            for key in obs.keys():\n",
    "                if key in obs_rms:\n",
    "                    normalized_obs[key] = self._normalize_obs(obs[key], obs_rms[key])\n",
    "                else:\n",
    "                    normalized_obs[key] = obs[key]\n",
    "            return normalized_obs   \n",
    "\n",
    "        if len(obs.shape) == 1:\n",
    "            # For 1D array\n",
    "            if obs.shape[0] <= 3:  # If obs is too small, normalize everything\n",
    "                return np.clip((obs - obs_rms.mean) / np.sqrt(obs_rms.var + self.epsilon), \n",
    "                             -self.clip_obs, self.clip_obs)\n",
    "            \n",
    "            # Separate features and action mask\n",
    "            features = obs[:-3]  # All elements except last 3\n",
    "            action_mask = obs[-3:]  # Last 3 elements (action mask)\n",
    "            \n",
    "            # Normalize only the features\n",
    "            norm_features = np.clip(\n",
    "                (features - obs_rms.mean) / np.sqrt(obs_rms.var + self.epsilon), \n",
    "                -self.clip_obs, \n",
    "                self.clip_obs\n",
    "            )\n",
    "            \n",
    "            # Concatenate normalized features with original action mask\n",
    "            return np.concatenate([norm_features, action_mask])\n",
    "        else:\n",
    "            # For batch processing (2D array)\n",
    "            if obs.shape[1] <= 3:  # If obs is too small, normalize everything\n",
    "                return np.clip(\n",
    "                    (obs - obs_rms.mean) / np.sqrt(obs_rms.var + self.epsilon), \n",
    "                    -self.clip_obs, \n",
    "                    self.clip_obs\n",
    "                )\n",
    "            \n",
    "            features = obs[:, :-3]  # All columns except last 3\n",
    "            action_mask = obs[:, -3:]  # Last 3 columns (action mask)\n",
    "            \n",
    "            # Normalize only the features\n",
    "            norm_features = np.clip(\n",
    "                (features - obs_rms.mean) / np.sqrt(obs_rms.var + self.epsilon), \n",
    "                -self.clip_obs, \n",
    "                self.clip_obs\n",
    "            )\n",
    "            \n",
    "            # Concatenate normalized features with original action mask\n",
    "            return np.concatenate([norm_features, action_mask], axis=1)\n",
    "    \n",
    "    def _unnormalize_obs(self, obs: np.ndarray, obs_rms: RunningMeanStd) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Unnormalize observations while preserving the action mask (last 3 elements).\n",
    "        \"\"\"\n",
    "\n",
    "        if len(obs.shape) == 1:\n",
    "            # For 1D array\n",
    "            if obs.shape[0] <= 3:  # If obs is too small, unnormalize everything\n",
    "                return (obs * np.sqrt(obs_rms.var + self.epsilon)) + obs_rms.mean\n",
    "            \n",
    "            features = obs[:-3]  # All elements except last 3\n",
    "            action_mask = obs[-3:]  # Last 3 elements (action mask)\n",
    "            \n",
    "            # Unnormalize only the features\n",
    "            unnorm_features = (features * np.sqrt(obs_rms.var + self.epsilon)) + obs_rms.mean\n",
    "            \n",
    "            # Concatenate unnormalized features with original action mask\n",
    "            return np.concatenate([unnorm_features, action_mask])\n",
    "        else:\n",
    "            # For batch processing (2D array)\n",
    "            if obs.shape[1] <= 3:  # If obs is too small, unnormalize everything\n",
    "                return (obs * np.sqrt(obs_rms.var + self.epsilon)) + obs_rms.mean\n",
    "            \n",
    "            features = obs[:, :-3]  # All columns except last 3\n",
    "            action_mask = obs[:, -3:]  # Last 3 columns (action mask)\n",
    "            \n",
    "            # Unnormalize only the features\n",
    "            unnorm_features = (features * np.sqrt(obs_rms.var + self.epsilon)) + obs_rms.mean\n",
    "            \n",
    "            # Concatenate unnormalized features with original action mask\n",
    "            return np.concatenate([unnorm_features, action_mask], axis=1)\n",
    "    \n",
    "    def step_wait(self) -> VecEnvStepReturn:\n",
    "        obs, rewards, dones, infos = self.venv.step_wait()\n",
    "        self.old_obs = obs\n",
    "        self.old_reward = rewards\n",
    "        if self.training and self.norm_obs:\n",
    "            # Update the running statistics with the features only (exclude action mask)\n",
    "            if isinstance(obs, dict) and isinstance(self.obs_rms, dict):\n",
    "                for key in self.obs_rms.keys():\n",
    "                    self.obs_rms[key].update(obs[key])\n",
    "            else:\n",
    "                if obs.shape[-1] > 3:  # Only update if we have features to normalize\n",
    "                    self.obs_rms.update(obs[..., :-3])  # Exclude last 3 elements (action mask)\n",
    "\n",
    "        # Normalize the observation (this will handle the action mask)\n",
    "        obs = self.normalize_obs(obs)\n",
    "\n",
    "        if self.training:\n",
    "            self._update_reward(rewards)\n",
    "        rewards = self.normalize_reward(rewards)\n",
    "\n",
    "        # Normalize the terminal observations\n",
    "        for idx, done in enumerate(dones):\n",
    "            if not done:\n",
    "                continue\n",
    "            if \"terminal_observation\" in infos[idx]:\n",
    "                infos[idx][\"terminal_observation\"] = self.normalize_obs(infos[idx][\"terminal_observation\"])\n",
    "\n",
    "        self.returns[dones] = 0\n",
    "        return obs, rewards, dones, infos\n",
    "\n",
    "    def _update_reward(self, reward: np.ndarray) -> None:\n",
    "        \"\"\"Update reward normalization statistics.\"\"\"\n",
    "        self.returns = self.returns * self.gamma + reward\n",
    "        self.ret_rms.update(self.returns)\n",
    "\n",
    "    def normalize_obs(self, obs: Union[np.ndarray, Dict[str, np.ndarray]]) -> Union[np.ndarray, Dict[str, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Normalize observations using this VecNormalize's observations statistics.\n",
    "        Calling this method does not update statistics.\n",
    "        \"\"\"\n",
    "        obs_ = deepcopy(obs)\n",
    "        if self.norm_obs:\n",
    "            if isinstance(obs, dict) and isinstance(self.obs_rms, dict):\n",
    "                for key in self.obs_rms.keys():\n",
    "                    obs_[key] = self._normalize_obs(obs[key], self.obs_rms[key])\n",
    "            else:\n",
    "                obs_ = self._normalize_obs(obs, self.obs_rms)\n",
    "        return obs_\n",
    "\n",
    "    def normalize_reward(self, reward: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalize rewards using this VecNormalize's rewards statistics.\n",
    "        \"\"\"\n",
    "        if self.norm_reward:\n",
    "            reward = np.clip(reward / np.sqrt(self.ret_rms.var + self.epsilon), \n",
    "                           -self.clip_reward, self.clip_reward)\n",
    "        return reward.astype(np.float32)\n",
    "\n",
    "    def reset(self) -> Union[np.ndarray, Dict[str, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Reset all environments\n",
    "        \"\"\"\n",
    "        obs = self.venv.reset()\n",
    "        self.old_obs = obs\n",
    "        self.returns = np.zeros(self.num_envs)\n",
    "        if self.training and self.norm_obs:\n",
    "            if isinstance(obs, dict) and isinstance(self.obs_rms, dict):\n",
    "                for key in self.obs_rms.keys():\n",
    "                    self.obs_rms[key].update(obs[key])\n",
    "            else:\n",
    "                if obs.shape[-1] > 3:  # Only update if we have features to normalize\n",
    "                    self.obs_rms.update(obs[..., :-3])  # Exclude last 3 elements (action mask)\n",
    "        return self.normalize_obs(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(load_path: str, venv: VecEnv) -> 'CustomVecNormalize':\n",
    "        \"\"\"\n",
    "        Load a saved CustomVecNormalize object.\n",
    "        \"\"\"\n",
    "        with open(load_path, \"rb\") as f:\n",
    "            custom_vec_normalize = pickle.load(f)\n",
    "        custom_vec_normalize.set_venv(venv)\n",
    "        return custom_vec_normalize\n",
    "\n",
    "    def save(self, save_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Save current CustomVecNormalize object.\n",
    "        \"\"\"\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f76828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, debug=False, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.debug = debug\n",
    "        self.n_envs = None\n",
    "        self.total_timesteps = 0\n",
    "        self.total_episodes = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_invalid_actions = []\n",
    "        self.normalized_rewards = []\n",
    "        self.unnormalized_rewards = []\n",
    "        self.total_steps_per_env = []\n",
    "        self.normalized_reward_per_env = []\n",
    "        self.unnormalized_reward_per_env = []\n",
    "        self.invalid_actions_per_env = []\n",
    "        self.current_steps_per_env = []\n",
    "        self.current_episode = 0\n",
    "        self.first_step = True\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        print(\"\\n=== DebugCallback Running ===\")\n",
    "        self.total_steps_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.normalized_reward_per_env = [0.0 for _ in range(self.n_envs)]\n",
    "        self.unnormalized_reward_per_env = [0.0 for _ in range(self.n_envs)]\n",
    "        self.invalid_actions_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.current_steps_per_env = [0 for _ in range(self.n_envs)]\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"\\n=== Environment Structure ===\")\n",
    "            print(f\"Number of environments: {self.n_envs}\")\n",
    "            try:\n",
    "                # Start from the outermost wrapper\n",
    "                current_env = self.training_env\n",
    "                env_stack = []\n",
    "                \n",
    "                # Traverse through all wrappers\n",
    "                print(\"Current environment:\", current_env)\n",
    "                while current_env is not None:\n",
    "                    env_type = type(current_env).__name__\n",
    "                    env_stack.append(env_type)\n",
    "                    \n",
    "                    # Try different ways to get the underlying environment\n",
    "                    if hasattr(current_env, 'venv'):\n",
    "                        print(\"Found venv attribute\")\n",
    "                        current_env = current_env.venv\n",
    "                    elif hasattr(current_env, 'env'):\n",
    "                        print(\"Found env attribute\")\n",
    "                        current_env = current_env.env\n",
    "                    elif hasattr(current_env, 'envs'):\n",
    "                        print(\"Found envs[0] attribute\")\n",
    "                        current_env = current_env.envs[0] if current_env.envs else None\n",
    "                    else:\n",
    "                        current_env = None\n",
    "                \n",
    "                # Print the environment stack\n",
    "                print(\"Environment Wrappers (outermost to innermost):\")\n",
    "                for i, env_type in enumerate(env_stack):\n",
    "                    print(f\"  {i+1}. {env_type}\")\n",
    "                \n",
    "                # Print details about the base environment\n",
    "                base_env = self.training_env.env\n",
    "\n",
    "                print(\"\\n=== Base Environment ===\")\n",
    "                print(base_env.get_attr('env'))\n",
    "                while hasattr(base_env, 'env'):\n",
    "                    base_env = base_env.env\n",
    "                \n",
    "                print(\"\\n=== Base Environment Details ===\")\n",
    "                print(f\"Base environment type: {type(base_env)}\")\n",
    "                print(f\"Observation space: {base_env.observation_space}\")\n",
    "                print(f\"Action space: {base_env.action_space}\")\n",
    "                \n",
    "                # Print ActionMasker specific info\n",
    "                if 'ActionMasker' in env_stack:\n",
    "                    print(\"\\n=== ActionMasker Details ===\")\n",
    "                    try:\n",
    "                        # Try to get ActionMasker instance\n",
    "                        action_masker = self.training_env\n",
    "                        while not isinstance(action_masker, ActionMasker) and hasattr(action_masker, 'env'):\n",
    "                            action_masker = action_masker.env\n",
    "                        if isinstance(action_masker, ActionMasker):\n",
    "                            print(f\"Action masker type: {type(action_masker)}\")\n",
    "                            # Add any specific ActionMasker info you want to print\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not access ActionMasker info: {e}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error traversing environment structure: {e}\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        infos = self.locals[\"infos\"]\n",
    "        self.total_timesteps += 1\n",
    "\n",
    "        for i in range(self.n_envs):\n",
    "            # Get normalized reward\n",
    "            normalized_reward = rewards[i]\n",
    "            \n",
    "            # Try to get unnormalized reward from info first\n",
    "            unnormalized_reward = infos[i].get('original_reward', None)\n",
    "            \n",
    "            # If not found in info, try to get from VecNormalize\n",
    "            if unnormalized_reward is None and isinstance(self.training_env, CustomVecNormalize):\n",
    "                original_rewards = self.training_env.get_original_rewards()\n",
    "                if original_rewards is not None:\n",
    "                    unnormalized_reward = original_rewards[i]\n",
    "            \n",
    "            if unnormalized_reward is None:\n",
    "                # If still not found, use normalized as fallback\n",
    "                unnormalized_reward = normalized_reward\n",
    "                if self.debug:\n",
    "                    print(\"Warning: Could not find unnormalized reward, using normalized\")\n",
    "            \n",
    "            # Update rewards\n",
    "            self.normalized_reward_per_env[i] += normalized_reward\n",
    "            self.unnormalized_reward_per_env[i] += unnormalized_reward\n",
    "            \n",
    "            # Track invalid actions\n",
    "            if infos[i].get(\"invalid_action\", False):\n",
    "                self.invalid_actions_per_env[i] += 1\n",
    "\n",
    "            # Handle episode completion\n",
    "            if dones[i]:\n",
    "                self.total_episodes += 1\n",
    "                self.current_episode += 1\n",
    "                \n",
    "                # Store both rewards when episode ends\n",
    "                self.episode_rewards.append(self.normalized_reward_per_env[i])\n",
    "                self.unnormalized_rewards.append(self.unnormalized_reward_per_env[i])\n",
    "                \n",
    "                if self.debug and i == 0:\n",
    "                    print(f\"[Env {i}] Episode {self.current_episode} Done.\")\n",
    "                    print(f\"  Steps: {self.current_steps_per_env[i]}\")\n",
    "                    print(f\"  Normalized Reward: {self.normalized_reward_per_env[i]:.2f}\")\n",
    "                    print(f\"  Unnormalized Reward: {self.unnormalized_reward_per_env[i]:.2f}\")\n",
    "                    print(f\"  Invalid: {self.invalid_actions_per_env[i]}\")\n",
    "\n",
    "                # Reset episode variables\n",
    "                self.current_steps_per_env[i] = 0\n",
    "                self.normalized_reward_per_env[i] = 0.0\n",
    "                self.unnormalized_reward_per_env[i] = 0.0\n",
    "                self.invalid_actions_per_env[i] = 0\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b21998df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Base Environment ===\n",
      "Observation space: Box(-inf, inf, (8,), float32)\n",
      "Action space: Discrete(3)\n",
      "\n",
      "After reset:\n",
      "Sample values: [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Observation shape: (8,)\n",
      "(array([3.265e-01, 0.000e+00, 1.000e+04, 1.000e+04, 1.000e+00, 1.000e+00,\n",
      "       1.000e+00, 0.000e+00]), 0, False, False, {'valid_actions': [0, 1], 'action_mask': array([1., 1., 0.], dtype=float32), 'invalid_action': False})\n",
      "\n",
      "After step:\n",
      "Observation shape: (8,)\n",
      "Sample values: [3.288e-01 0.000e+00 1.000e+04 1.000e+04 2.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Reward: 0\n",
      "Done: False\n"
     ]
    }
   ],
   "source": [
    "# Test the base environment directly\n",
    "print(\"=== Testing Base Environment ===\")\n",
    "base_env = TradingEnv(df)  # Your original environment\n",
    "print(\"Observation space:\", base_env.observation_space)\n",
    "print(\"Action space:\", base_env.action_space)\n",
    "\n",
    "# Test reset\n",
    "obs,_ = base_env.reset()\n",
    "print(\"\\nAfter reset:\")\n",
    "print(\"Sample values:\", obs)\n",
    "print(\"Observation shape:\", obs.shape)\n",
    "\n",
    "# Test step\n",
    "action = 0  # Try hold action\n",
    "print(base_env.step(action))\n",
    "obs, reward, done, truncated, info = base_env.step(action)\n",
    "print(\"\\nAfter step:\")\n",
    "print(\"Observation shape:\", obs.shape)\n",
    "print(\"Sample values:\", obs)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df1626e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing with DummyVecEnv ===\n",
      "DummyVecEnv observation space: Box(-inf, inf, (8,), float32)\n",
      "\n",
      "After DummyVecEnv reset:\n",
      "Observation shape: (1, 8)\n",
      "Sample values: [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "\n",
      "After DummyVecEnv step:\n",
      "Observation shape: (1, 8)\n",
      "Sample values: [3.265e-01 0.000e+00 1.000e+04 1.000e+04 1.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Rewards: [0.]\n"
     ]
    }
   ],
   "source": [
    "# Testing with DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "print(\"\\n=== Testing with DummyVecEnv ===\")\n",
    "dummy_env = DummyVecEnv([lambda: base_env])\n",
    "print(\"DummyVecEnv observation space:\", dummy_env.observation_space)\n",
    "\n",
    "# Test reset\n",
    "obs = dummy_env.reset()\n",
    "print(\"\\nAfter DummyVecEnv reset:\")\n",
    "print(\"Observation shape:\", obs.shape)\n",
    "print(\"Sample values:\", obs[0])\n",
    "\n",
    "# Test step\n",
    "action = [0]  # Action needs to be in a list for vectorized env\n",
    "obs, rewards, dones, infos = dummy_env.step(action)\n",
    "print(\"\\nAfter DummyVecEnv step:\")\n",
    "print(\"Observation shape:\", obs.shape)\n",
    "print(\"Sample values:\", obs[0])\n",
    "print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01ad4ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing with SubProcVecEnv ===\n",
      "SubProcVecEnv observation space: Box(-inf, inf, (8,), float32)\n",
      "\n",
      "After SubProcVecEnv reset:\n",
      "Observation shape: (1, 8)\n",
      "Sample values: [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "\n",
      "After SubProcVecEnv step:\n",
      "Observation shape: (1, 8)\n",
      "Sample values: [3.265e-01 0.000e+00 1.000e+04 1.000e+04 1.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Rewards: [0]\n"
     ]
    }
   ],
   "source": [
    "# Testing with SubProcVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "print(\"\\n=== Testing with SubProcVecEnv ===\")\n",
    "vec_env = SubprocVecEnv([lambda: base_env])\n",
    "print(\"SubProcVecEnv observation space:\", dummy_env.observation_space)\n",
    "\n",
    "# Test reset\n",
    "obs = vec_env.reset()\n",
    "print(\"\\nAfter SubProcVecEnv reset:\")\n",
    "print(\"Observation shape:\", obs.shape)\n",
    "print(\"Sample values:\", obs[0])\n",
    "\n",
    "# Test step\n",
    "action = [0]  # Action needs to be in a list for vectorized env\n",
    "obs, rewards, dones, infos = vec_env.step(action)\n",
    "print(\"\\nAfter SubProcVecEnv step:\")\n",
    "print(\"Observation shape:\", obs.shape)\n",
    "print(\"Sample values:\", obs[0])\n",
    "print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd331a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing with SubprocVecEnv + CustomVecNormalize ===\n",
      "init CustomVecNormalize\n",
      "12\n",
      "\n",
      "=== Testing Reset ===\n",
      "\n",
      "Resetting...\n",
      "Reset in 0.01s\n",
      "Observation shape: (12, 8)\n",
      "Sample values: [2.69907237e-04 0.00000000e+00 2.88675116e-03 2.88675116e-03\n",
      " 0.00000000e+00 1.00000000e+00 1.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Testing with SubprocVecEnv + CustomVecNormalize\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"\\n=== Testing with SubprocVecEnv + CustomVecNormalize ===\")\n",
    "\n",
    "# Create the environment with proper observation space handling\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = TradingEnv(df)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Create vectorized environment\n",
    "vec_env = SubprocVecEnv([make_env() for _ in range(num_cpu)])\n",
    "\n",
    "# Wrap with CustomVecNormalize\n",
    "norm_env = CustomVecNormalize(\n",
    "    vec_env,\n",
    "    norm_obs=True,\n",
    "    norm_reward=True,\n",
    "    clip_obs=10.0,\n",
    "    clip_reward=10.0,\n",
    "    gamma=0.99,\n",
    "    training=True,\n",
    "    epsilon=1e-4  # Increased from 1e-8 for more numerical stability\n",
    ")\n",
    "\n",
    "print(norm_env.num_envs)\n",
    "\n",
    "# Test reset\n",
    "print(\"\\n=== Testing Reset ===\")\n",
    "start = time.time()\n",
    "print(\"\\nResetting...\")\n",
    "obs = norm_env.reset()\n",
    "print(f\"Reset in {time.time() - start:.2f}s\")\n",
    "print(\"Observation shape:\", obs.shape)\n",
    "print(\"Sample values:\", obs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10384969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init CustomVecNormalize\n",
      "Observation shape: (12, 8)\n"
     ]
    }
   ],
   "source": [
    "#basic test\n",
    "\n",
    "num_cpu = 12  # Number of environments\n",
    "\n",
    "# Create the environments\n",
    "vec_env = SubprocVecEnv([make_env() for _ in range(num_cpu)])\n",
    "norm_env = CustomVecNormalize(\n",
    "    vec_env,\n",
    "    norm_obs=True,\n",
    "    norm_reward=False,\n",
    "    clip_obs=10.0,\n",
    "    training=True\n",
    ")\n",
    "\n",
    "# Reset (works for all environments)\n",
    "obs = norm_env.reset()\n",
    "print(f\"Observation shape: {obs.shape}\")  # Should be (12, 8)\n",
    "\n",
    "# Step - IMPORTANT: Provide one action per environment\n",
    "# This is the correct way:\n",
    "actions = [1] * num_cpu  # Buy action for all environments\n",
    "obs, rewards, dones, infos = norm_env.step(actions)\n",
    "\n",
    "# Or if you want different actions for each environment:\n",
    "actions = [i % 3 for i in range(num_cpu)]  # Cycle through actions\n",
    "obs, rewards, dones, infos = norm_env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36fdbb-ec35-4a59-9c03-cfce1d83b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parallel environments and train model\n",
    "import torch\n",
    "import importlib\n",
    "import trading_env_sb3_ver2b\n",
    "importlib.reload(trading_env_sb3_ver2b)\n",
    "from trading_env_sb3_ver2b import TradingEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "# Define the mask_fn to get valid actions from the environment\n",
    "def mask_fn(env: gym.Env) -> np.ndarray:\n",
    "    return env.get_action_mask()  # Get valid action mask from the environment\n",
    "\n",
    "# Define the number of CPU cores to use\n",
    "#num_cpu = available_cores  # Get the number of available CPU cores\n",
    "\n",
    "num_cpu = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4cdcf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init CustomVecNormalize\n",
      "Using cpu device\n",
      "Logging to ./tensorboard_logs/sb3_ppo_debug_79\n",
      "\n",
      "=== DebugCallback Training Start ===\n",
      "<__main__.RewardCallback object at 0x37238fe20>\n",
      "Training environment:\n",
      "<__main__.CustomVecNormalize object at 0x372362b30>\n",
      "Type of training environment:\n",
      "<class '__main__.CustomVecNormalize'>\n",
      "Training environment has venv attribute\n",
      "Number of environments: 1\n",
      "\n",
      "=== Environment Structure ===\n",
      "Number of environments: 1\n",
      "Training env type: <class '__main__.CustomVecNormalize'>\n",
      "Is CustomVecNormalize: True\n",
      "\n",
      "=== Full Environment Structure ===\n",
      "Environment Wrappers (outermost to innermost):\n",
      "  1. CustomVecNormalize\n",
      "  2. VecMonitor\n",
      "  3. SubprocVecEnv\n",
      "Error traversing environment structure: 'SubprocVecEnv' object has no attribute 'env'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SubprocVecEnv' object has no attribute 'get_original_rewards'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# After training is complete, save the model and normalization stats\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[43mppo_masked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msb3_ppo_debug\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# After training is complete\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;66;03m# Save the model\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:224\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[1;32m    223\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_info_buffer(infos, dones)\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:223\u001b[0m, in \u001b[0;36mCallbackList._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# Return False (stop training) if at least one callback returns False\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m continue_training\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m continue_training\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[70], line 119\u001b[0m, in \u001b[0;36mRewardCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# If not found in info, try to get from VecNormalize\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unnormalized_reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_env, CustomVecNormalize):\n\u001b[0;32m--> 119\u001b[0m     original_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_original_rewards\u001b[49m()\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m original_rewards \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m         unnormalized_reward \u001b[38;5;241m=\u001b[39m original_rewards[i]\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:421\u001b[0m, in \u001b[0;36mVecEnvWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    415\u001b[0m     error_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Recursive attribute lookup for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mown_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mambiguous and hides attribute from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblocked_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m     )\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(error_str)\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetattr_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:444\u001b[0m, in \u001b[0;36mVecEnvWrapper.getattr_recursive\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    440\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvenv, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetattr_recursive\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Attribute not present, child is wrapper. Call getattr_recursive rather than getattr\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# to avoid a duplicate call to getattr_depth_check.\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetattr_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# attribute not present, child is an unwrapped VecEnv\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvenv, name)\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:446\u001b[0m, in \u001b[0;36mVecEnvWrapper.getattr_recursive\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    444\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvenv\u001b[38;5;241m.\u001b[39mgetattr_recursive(name)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# attribute not present, child is an unwrapped VecEnv\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attr\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SubprocVecEnv' object has no attribute 'get_original_rewards'"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "# --- Function to create the wrapped environment ---\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = TradingEnv(df)  # Your DataFrame must be accessible here\n",
    "        return ActionMasker(env, mask_fn)\n",
    "    return _init\n",
    "\n",
    "# Create parallel environments using SubprocVecEnv\n",
    "env = SubprocVecEnv([make_env() for _ in range(num_cpu)])\n",
    "\n",
    "# Wrap with VecMonitor first\n",
    "env = VecMonitor(env)\n",
    "\n",
    "# Wrap with VecNormalize\n",
    "env = CustomVecNormalize(\n",
    "    env,\n",
    "    norm_obs=True,\n",
    "    norm_reward=True,\n",
    "    clip_obs=10.0,\n",
    "    clip_reward=10.0,\n",
    "    gamma=0.99,\n",
    "    training=True  # Set to False during evaluation\n",
    ")\n",
    "\n",
    "# Define PPO model with the custom policy using the vectorized environment\n",
    "learning_rate = 1e-4\n",
    "ppo_masked_model = PPO(\n",
    "    MaskedPPOPolicy,  # Custom policy\n",
    "    env,               # Your environment\n",
    "    verbose=1,         \n",
    "    tensorboard_log=\"./tensorboard_logs/\",\n",
    "    learning_rate=learning_rate,\n",
    "    ent_coef=0.001,      # Small exploration penalty\n",
    "    gamma=0.99,       # Discount factor for long-term rewards\n",
    "    gae_lambda=0.95,    # Optimistic advantage estimation (strongly favors long-term)\n",
    "    n_steps=64,      # Large number of timesteps per iteration\n",
    "    clip_range=0.1,    # Clipping to allow more aggressive updates\n",
    "    clip_range_vf=0.1,\n",
    "    n_epochs=20,       # Number of passes over the data (many epochs to overfit)\n",
    "    batch_size=32,    # Large batch size for stability in updates\n",
    "    max_grad_norm=0.5,\n",
    "    vf_coef=0.5,\n",
    "    normalize_advantage=False,  # Enable advantage normalization\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=dict(pi=[256, 256], vf=[256, 256])  # Larger network to memorize patterns\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train the model with the callback\n",
    "from multiprocessing import set_start_method\n",
    "set_start_method('spawn', force=True)\n",
    "\n",
    "# Initialize the debug callback\n",
    "\n",
    "# Modify EvalCallback to include more metrics\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    eval_freq=1000,\n",
    "    n_eval_episodes=10,  # Increased from 5\n",
    "    log_path=\"./eval_logs/\",\n",
    "    best_model_save_path=\"./best_model/\",\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Add checkpoint callback\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=10000,\n",
    "    save_path=\"./checkpoints/\",\n",
    "    name_prefix=\"ppo_model\"\n",
    ")\n",
    "\n",
    "# Combine callbacks\n",
    "#callbacks = CallbackList([eval_callback, checkpoint_callback])\n",
    "\n",
    "training_debug_callback = TrainingDebugCallback()  # Log every 10 episodes\n",
    "reward_callback = RewardCallback(debug=True)\n",
    "debug_episodes = {0, 89}\n",
    "debug_callback = DebugCallback(debug_episodes=debug_episodes)\n",
    "\n",
    "callbacks = CallbackList([reward_callback])\n",
    "\n",
    "# After training is complete, save the model and normalization stats\n",
    "try:\n",
    "    ppo_masked_model.learn(\n",
    "        total_timesteps=1,\n",
    "        progress_bar=False,\n",
    "        tb_log_name=\"sb3_ppo_debug\",\n",
    "        callback=callbacks\n",
    "    )\n",
    "finally:\n",
    "    # After training is complete\n",
    "    try:\n",
    "        # Save the model\n",
    "        ppo_masked_model.save('sb3_ppo_model')\n",
    "        \n",
    "        # Get the first environment from the parallel environment\n",
    "        if isinstance(env, SubprocVecEnv):\n",
    "            # Get the VecNormalize wrapper from the first environment\n",
    "            first_env = env.envs[0]\n",
    "            if isinstance(first_env, VecNormalize):\n",
    "                first_env.save(\"vecnormalize.pkl\")\n",
    "        \n",
    "        # Close the environment\n",
    "        env.close()\n",
    "    except EOFError:\n",
    "        print(\"Warning: env subprocess already crashed, skipping close()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e9562e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Episode 1 ===\n",
      "Initial observation: [[ 2.0362396e+00 -9.0626061e-01  8.0337775e-01  4.9388060e-01\n",
      "  -1.5661801e+00  9.0771191e-06  9.0626061e-01 -9.0626061e-01]]\n",
      "Step 0: BUY at 0.3386\n",
      "Step 1: BUY at 0.3265\n",
      "Step 2: BUY at 0.3288\n",
      "Step 3: BUY at 0.3123\n",
      "Episode 1 completed with reward: -0.50\n",
      "  Buy signals: 4\n",
      "  Sell signals: 0\n",
      "\n",
      "=== Episode 2 ===\n",
      "Initial observation: [[ 2.0362396e+00 -9.0626061e-01  8.0337775e-01  4.9388060e-01\n",
      "  -1.5661801e+00  9.0771191e-06  9.0626061e-01 -9.0626061e-01]]\n",
      "Step 0: BUY at 0.3386\n",
      "Step 1: BUY at 0.3265\n",
      "Step 2: BUY at 0.3288\n",
      "Step 3: BUY at 0.3123\n",
      "Episode 2 completed with reward: -0.50\n",
      "  Buy signals: 4\n",
      "  Sell signals: 0\n",
      "\n",
      "=== Episode 3 ===\n",
      "Initial observation: [[ 2.0362396e+00 -9.0626061e-01  8.0337775e-01  4.9388060e-01\n",
      "  -1.5661801e+00  9.0771191e-06  9.0626061e-01 -9.0626061e-01]]\n",
      "Step 0: BUY at 0.3386\n",
      "Step 1: BUY at 0.3265\n",
      "Step 2: BUY at 0.3288\n",
      "Step 3: BUY at 0.3123\n",
      "Episode 3 completed with reward: -0.50\n",
      "  Buy signals: 4\n",
      "  Sell signals: 0\n",
      "\n",
      "=== Test Results ===\n",
      "Episodes: 3\n",
      "Mean reward: -0.50 Â± 0.00\n",
      "Episode 1: 4 buy signals, 0 sell signals\n",
      "Episode 2: 4 buy signals, 0 sell signals\n",
      "Episode 3: 4 buy signals, 0 sell signals\n",
      "\n",
      "First episode buy signals: [Timestamp('2025-02-07 04:00:00'), Timestamp('2025-02-07 04:01:00'), Timestamp('2025-02-07 04:02:00'), Timestamp('2025-02-07 04:03:00')]\n",
      "First episode sell signals: []\n"
     ]
    }
   ],
   "source": [
    "def test_trading_model(model, vec_norm_path, num_episodes=3, render=False):\n",
    "    \"\"\"Test the trained trading model with a single DummyVecEnv and return signals.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PPO model\n",
    "        vec_norm_path: Path to saved VecNormalize stats (optional)\n",
    "        num_episodes: Number of episodes to run\n",
    "        render: Whether to print debug info\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (all_episodes_results, first_episode_signals)\n",
    "            all_episodes_results: dict containing results from all episodes\n",
    "            first_episode_signals: dict with 'buy_signals' and 'sell_signals' from first episode\n",
    "    \"\"\"\n",
    "    # Create a single test environment\n",
    "    def make_test_env():\n",
    "        def _init():\n",
    "            env = TradingEnv(df)\n",
    "            return ActionMasker(env, mask_fn)\n",
    "        return _init\n",
    "\n",
    "    # Initialize results\n",
    "    all_episodes_results = {\n",
    "        'episode_rewards': [],\n",
    "        'all_buy_signals': [],\n",
    "        'all_sell_signals': [],\n",
    "        'actions': [],\n",
    "        'timesteps': []\n",
    "    }\n",
    "    \n",
    "    # Store first episode signals separately\n",
    "    first_episode_signals = {\n",
    "        'buy_signals': [],\n",
    "        'sell_signals': []\n",
    "    }\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.policy.eval()\n",
    "    \n",
    "    # Create environment\n",
    "    env = DummyVecEnv([make_test_env()])\n",
    "\n",
    "    # Load saved normalization stats instead of creating new ones\n",
    "    if vec_norm_path:\n",
    "        env = VecNormalize.load(vec_norm_path, env)\n",
    "        env.training = False  # Don't update stats during testing\n",
    "        env.norm_reward = False  # Don't normalize rewards during testing\n",
    "    else:\n",
    "        # If no saved stats, create new normalization (not recommended for testing)\n",
    "        env = CustomVecNormalize(\n",
    "            env,\n",
    "            norm_obs=True,\n",
    "            norm_reward=True,\n",
    "            clip_obs=10.0,\n",
    "            clip_reward=10.0,\n",
    "            gamma=0.99,\n",
    "            training=False\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            step = 0\n",
    "            \n",
    "            # Lists to store signals for current episode\n",
    "            episode_buy_signals = []\n",
    "            episode_sell_signals = []\n",
    "            \n",
    "            if render:\n",
    "                print(f\"\\n=== Episode {episode + 1} ===\")\n",
    "                print(f\"Initial observation: {obs}\")\n",
    "            \n",
    "            while not done and step < len(df) - 1:\n",
    "                try:\n",
    "                    # Get action mask from the environment\n",
    "                    action_mask = env.envs[0].env.get_action_mask()\n",
    "                    \n",
    "                    # Get action from model\n",
    "                    with torch.no_grad():\n",
    "                        obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=model.device)\n",
    "                        dist = model.policy.get_distribution(obs_tensor)\n",
    "                        masked_logits = dist.distribution.logits + torch.log(\n",
    "                            torch.as_tensor(action_mask, dtype=torch.float32, device=model.device) + 1e-8\n",
    "                        )\n",
    "                        action = torch.argmax(masked_logits).item()\n",
    "                    \n",
    "                    # Step environment\n",
    "                    new_obs, reward, done, info = env.step([action])\n",
    "                    \n",
    "                    # Track signals\n",
    "                    current_step = info[0].get('current_step', step)\n",
    "                    current_time = df.index[current_step]\n",
    "                    \n",
    "                    if action == 1:  # BUY\n",
    "                        episode_buy_signals.append(current_time)\n",
    "                        if render:\n",
    "                            print(f\"Step {step}: BUY at {df['close'].iloc[current_step]:.4f}\")\n",
    "                    elif action == 2:  # SELL\n",
    "                        episode_sell_signals.append(current_time)\n",
    "                        if render:\n",
    "                            print(f\"Step {step}: SELL at {df['close'].iloc[current_step]:.4f}\")\n",
    "                    \n",
    "                    # Store results\n",
    "                    all_episodes_results['actions'].append(action)\n",
    "                    all_episodes_results['timesteps'].append(current_step)\n",
    "                    episode_reward += reward[0] if isinstance(reward, np.ndarray) else reward\n",
    "                    obs = new_obs\n",
    "                    step += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error at step {step}: {str(e)}\")\n",
    "                    done = True\n",
    "                    break\n",
    "            \n",
    "            # Store signals for this episode\n",
    "            all_episodes_results['all_buy_signals'].append(episode_buy_signals)\n",
    "            all_episodes_results['all_sell_signals'].append(episode_sell_signals)\n",
    "            all_episodes_results['episode_rewards'].append(episode_reward)\n",
    "            \n",
    "            # Store first episode signals separately\n",
    "            if episode == 0:\n",
    "                first_episode_signals['buy_signals'] = episode_buy_signals\n",
    "                first_episode_signals['sell_signals'] = episode_sell_signals\n",
    "            \n",
    "            if render:\n",
    "                print(f\"Episode {episode + 1} completed with reward: {episode_reward:.2f}\")\n",
    "                print(f\"  Buy signals: {len(episode_buy_signals)}\")\n",
    "                print(f\"  Sell signals: {len(episode_sell_signals)}\")\n",
    "            \n",
    "            # Reset for next episode\n",
    "            env.reset()\n",
    "    \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        env.close()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== Test Results ===\")\n",
    "    print(f\"Episodes: {num_episodes}\")\n",
    "    if all_episodes_results['episode_rewards']:\n",
    "        print(f\"Mean reward: {np.mean(all_episodes_results['episode_rewards']):.2f} Â± {np.std(all_episodes_results['episode_rewards']):.2f}\")\n",
    "    \n",
    "    # Print signal counts for all episodes\n",
    "    for i in range(num_episodes):\n",
    "        print(f\"Episode {i+1}: {len(all_episodes_results['all_buy_signals'][i])} buy signals, {len(all_episodes_results['all_sell_signals'][i])} sell signals\")\n",
    "    \n",
    "    return first_episode_signals\n",
    "\n",
    "# Then test the model\n",
    "# Test the model and get results\n",
    "first_episode_signals = test_trading_model(\n",
    "    PPO.load(\"sb3_ppo_model.zip\"),\n",
    "    vec_norm_path=\"vecnormalize.pkl\",\n",
    "    num_episodes=3, \n",
    "    render=True\n",
    ")\n",
    "\n",
    "# Access the signals from the first episode\n",
    "buy_signals = first_episode_signals['buy_signals']\n",
    "sell_signals = first_episode_signals['sell_signals']\n",
    "\n",
    "print(\"\\nFirst episode buy signals:\", buy_signals)\n",
    "print(\"First episode sell signals:\", sell_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3df3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_env_structure(env, indent=0):\n",
    "    \"\"\"Recursively print the environment wrapper structure.\"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    print(f\"{prefix}ââ {type(env).__name__}\")\n",
    "    \n",
    "    # Special handling for common wrapper types\n",
    "    if hasattr(env, 'env'):\n",
    "        print_env_structure(env.env, indent + 1)\n",
    "    elif hasattr(env, 'envs') and hasattr(env.envs, '__getitem__'):\n",
    "        print(f\"{prefix}  ââ [0]\")  # First sub-environment\n",
    "        print_env_structure(env.envs[0], indent + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e1202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(model, env, buy_signals, sell_signals, num_episodes=1):\n",
    "    \"\"\"Test function that handles action masking and tracks rewards.\"\"\"\n",
    "    print(\"Environment structure:\")\n",
    "    print_env_structure(env)\n",
    "    \n",
    "    # Helper to safely get base environment\n",
    "    def get_base_env(env):\n",
    "        current = env\n",
    "        while True:\n",
    "            if hasattr(current, 'envs') and hasattr(current.envs, '__getitem__'):\n",
    "                current = current.envs[0]\n",
    "            elif hasattr(current, 'env'):\n",
    "                current = current.env\n",
    "            else:\n",
    "                return current\n",
    "    \n",
    "    # Get the base environment\n",
    "    base_env = get_base_env(env)\n",
    "    print(f\"\\nBase environment: {type(base_env).__name__}\")\n",
    "    \n",
    "    # Check if environment is wrapped with VecNormalize\n",
    "    vec_norm = None\n",
    "    current = env\n",
    "    while hasattr(current, 'env'):\n",
    "        if hasattr(current, 'obs_rms'):\n",
    "            vec_norm = current\n",
    "            break\n",
    "        current = current.env\n",
    "    \n",
    "    # Track rewards across all episodes\n",
    "    all_episode_rewards = []\n",
    "    all_episode_actions = []\n",
    "    all_episode_prices = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        current_step = 0\n",
    "        episode_buys = []\n",
    "        episode_sells = []\n",
    "        episode_rewards = []\n",
    "        episode_actions = []\n",
    "        episode_prices = []\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"=== Episode {episode + 1} ===\")\n",
    "        print(f\"{'Step':<6} {'Action':<6} {'Price':<10} {'Reward':<10} {'Cumulative':<10}\")\n",
    "        print(\"-\"*45)\n",
    "        \n",
    "        cumulative_reward = 0\n",
    "        \n",
    "        while not done and current_step < len(df):\n",
    "            try:\n",
    "                # Get action mask from the base environment\n",
    "                action_mask = np.ones(3, dtype=np.float32)\n",
    "                if hasattr(base_env, 'get_action_mask'):\n",
    "                    action_mask = base_env.get_action_mask()\n",
    "                \n",
    "                # Get action using the model's policy\n",
    "                with torch.no_grad():\n",
    "                    # Normalize observation if VecNormalize is used\n",
    "                    if vec_norm is not None:\n",
    "                        obs = vec_norm.normalize_obs(obs)\n",
    "                    obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=model.device)\n",
    "                    dist = model.policy.get_distribution(obs_tensor)\n",
    "                    masked_logits = dist.distribution.logits + torch.log(\n",
    "                        torch.as_tensor(action_mask, dtype=torch.float32, device=model.device) + 1e-8\n",
    "                    )\n",
    "                    action = torch.argmax(masked_logits).item()\n",
    "                \n",
    "                # Step environment\n",
    "                new_obs, reward, done, info = env.step([action])\n",
    "                if isinstance(reward, np.ndarray):\n",
    "                    reward = reward.item()\n",
    "                \n",
    "                # Denormalize reward if VecNormalize is used\n",
    "                if vec_norm is not None:\n",
    "                    reward = vec_norm.unnormalize_reward(reward)\n",
    "                \n",
    "                # Track episode data\n",
    "                price = df['close'].iloc[current_step]\n",
    "                cumulative_reward += reward\n",
    "                episode_rewards.append(reward)\n",
    "                episode_actions.append(action)\n",
    "                episode_prices.append(price)\n",
    "                \n",
    "                # Print step info\n",
    "                action_str = \"BUY\" if action == 1 else \"SELL\" if action == 2 else \"HOLD\"\n",
    "                print(f\"{current_step:<6} {action_str:<6} {price:<10.4f} {reward:<10.4f} {cumulative_reward:<10.4f}\")\n",
    "                \n",
    "                # Track signals\n",
    "                if action == 1:\n",
    "                    episode_buys.append(df.index[current_step])\n",
    "                elif action == 2:\n",
    "                    episode_sells.append(df.index[current_step])\n",
    "                \n",
    "                obs = new_obs\n",
    "                current_step += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error at step {current_step}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                break\n",
    "        \n",
    "        # Store episode data\n",
    "        all_episode_rewards.append(episode_rewards)\n",
    "        all_episode_actions.append(episode_actions)\n",
    "        all_episode_prices.append(episode_prices)\n",
    "        \n",
    "        # Print episode summary\n",
    "        total_reward = sum(episode_rewards)\n",
    "        print(f\"\\nEpisode {episode + 1} Summary:\")\n",
    "        print(f\"  Total reward: {total_reward:.4f}\")\n",
    "        if episode_rewards:\n",
    "            print(f\"  Average reward/step: {total_reward/len(episode_rewards):.4f}\")\n",
    "        print(f\"  Buy signals: {len(episode_buys)}\")\n",
    "        print(f\"  Sell signals: {len(episode_sells)}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Add to global signals\n",
    "        buy_signals.extend(episode_buys)\n",
    "        sell_signals.extend(episode_sells)\n",
    "    \n",
    "    return all_episode_rewards, all_episode_actions, all_episode_pricesdef test_agent(model, env, buy_signals, sell_signals, num_episodes=1):\n",
    "    \"\"\"Test function that handles action masking and tracks rewards.\"\"\"\n",
    "    print(\"Environment structure:\")\n",
    "    print_env_structure(env)\n",
    "    \n",
    "    # Helper to safely get base environment\n",
    "    def get_base_env(env):\n",
    "        current = env\n",
    "        while True:\n",
    "            if hasattr(current, 'envs') and hasattr(current.envs, '__getitem__'):\n",
    "                current = current.envs[0]\n",
    "            elif hasattr(current, 'env'):\n",
    "                current = current.env\n",
    "            else:\n",
    "                return current\n",
    "    \n",
    "    # Get the base environment\n",
    "    base_env = get_base_env(env)\n",
    "    print(f\"\\nBase environment: {type(base_env).__name__}\")\n",
    "    \n",
    "    # Check if environment is wrapped with VecNormalize\n",
    "    vec_norm = None\n",
    "    current = env\n",
    "    while hasattr(current, 'env'):\n",
    "        if hasattr(current, 'obs_rms'):\n",
    "            vec_norm = current\n",
    "            break\n",
    "        current = current.env\n",
    "    \n",
    "    # Track rewards across all episodes\n",
    "    all_episode_rewards = []\n",
    "    all_episode_actions = []\n",
    "    all_episode_prices = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        current_step = 0\n",
    "        episode_buys = []\n",
    "        episode_sells = []\n",
    "        episode_rewards = []\n",
    "        episode_actions = []\n",
    "        episode_prices = []\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"=== Episode {episode + 1} ===\")\n",
    "        print(f\"{'Step':<6} {'Action':<6} {'Price':<10} {'Reward':<10} {'Cumulative':<10}\")\n",
    "        print(\"-\"*45)\n",
    "        \n",
    "        cumulative_reward = 0\n",
    "        \n",
    "        while not done and current_step < len(df):\n",
    "            try:\n",
    "                # Get action mask from the base environment\n",
    "                action_mask = np.ones(3, dtype=np.float32)\n",
    "                if hasattr(base_env, 'get_action_mask'):\n",
    "                    action_mask = base_env.get_action_mask()\n",
    "                \n",
    "                # Get action using the model's policy\n",
    "                with torch.no_grad():\n",
    "                    # Normalize observation if VecNormalize is used\n",
    "                    if vec_norm is not None:\n",
    "                        obs = vec_norm.normalize_obs(obs)\n",
    "                    obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=model.device)\n",
    "                    dist = model.policy.get_distribution(obs_tensor)\n",
    "                    masked_logits = dist.distribution.logits + torch.log(\n",
    "                        torch.as_tensor(action_mask, dtype=torch.float32, device=model.device) + 1e-8\n",
    "                    )\n",
    "                    action = torch.argmax(masked_logits).item()\n",
    "                \n",
    "                # Step environment\n",
    "                new_obs, reward, done, info = env.step([action])\n",
    "                if isinstance(reward, np.ndarray):\n",
    "                    reward = reward.item()\n",
    "                \n",
    "                # Denormalize reward if VecNormalize is used\n",
    "                if vec_norm is not None:\n",
    "                    reward = vec_norm.unnormalize_reward(reward)\n",
    "                \n",
    "                # Track episode data\n",
    "                price = df['close'].iloc[current_step]\n",
    "                cumulative_reward += reward\n",
    "                episode_rewards.append(reward)\n",
    "                episode_actions.append(action)\n",
    "                episode_prices.append(price)\n",
    "                \n",
    "                # Print step info\n",
    "                action_str = \"BUY\" if action == 1 else \"SELL\" if action == 2 else \"HOLD\"\n",
    "                print(f\"{current_step:<6} {action_str:<6} {price:<10.4f} {reward:<10.4f} {cumulative_reward:<10.4f}\")\n",
    "                \n",
    "                # Track signals\n",
    "                if action == 1:\n",
    "                    episode_buys.append(df.index[current_step])\n",
    "                elif action == 2:\n",
    "                    episode_sells.append(df.index[current_step])\n",
    "                \n",
    "                obs = new_obs\n",
    "                current_step += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error at step {current_step}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                break\n",
    "        \n",
    "        # Store episode data\n",
    "        all_episode_rewards.append(episode_rewards)\n",
    "        all_episode_actions.append(episode_actions)\n",
    "        all_episode_prices.append(episode_prices)\n",
    "        \n",
    "        # Print episode summary\n",
    "        total_reward = sum(episode_rewards)\n",
    "        print(f\"\\nEpisode {episode + 1} Summary:\")\n",
    "        print(f\"  Total reward: {total_reward:.4f}\")\n",
    "        if episode_rewards:\n",
    "            print(f\"  Average reward/step: {total_reward/len(episode_rewards):.4f}\")\n",
    "        print(f\"  Buy signals: {len(episode_buys)}\")\n",
    "        print(f\"  Sell signals: {len(episode_sells)}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Add to global signals\n",
    "        buy_signals.extend(episode_buys)\n",
    "        sell_signals.extend(episode_sells)\n",
    "    \n",
    "    return all_episode_rewards, all_episode_actions, all_episode_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a58f4e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording signals\n",
      "Environment structure:\n",
      "ââ VecNormalize\n",
      "  ââ [0]\n",
      "    ââ ActionMasker\n",
      "      ââ TradingEnv\n",
      "\n",
      "Base environment: TradingEnv\n",
      "\n",
      "==================================================\n",
      "=== Episode 1 ===\n",
      "Step   Action Price      Reward     Cumulative\n",
      "---------------------------------------------\n",
      "0      HOLD   0.3386     0.0000     0.0000    \n",
      "1      HOLD   0.3265     0.0000     0.0000    \n",
      "2      HOLD   0.3288     0.0000     0.0000    \n",
      "3      HOLD   0.3123     0.0000     0.0000    \n",
      "4      HOLD   0.3124     0.0000     0.0000    \n",
      "5      HOLD   0.3011     0.0000     0.0000    \n",
      "6      HOLD   0.3069     0.0000     0.0000    \n",
      "7      HOLD   0.3115     0.0000     0.0000    \n",
      "8      HOLD   0.3137     0.0000     0.0000    \n",
      "9      HOLD   0.3046     0.0000     0.0000    \n",
      "\n",
      "Episode 1 Summary:\n",
      "  Total reward: 0.0000\n",
      "  Average reward/step: 0.0000\n",
      "  Buy signals: 0\n",
      "  Sell signals: 0\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [[np.float64(0.3386),\n",
       "   np.float64(0.3265),\n",
       "   np.float64(0.3288),\n",
       "   np.float64(0.3123),\n",
       "   np.float64(0.3124),\n",
       "   np.float64(0.3011),\n",
       "   np.float64(0.3069),\n",
       "   np.float64(0.3115),\n",
       "   np.float64(0.3137),\n",
       "   np.float64(0.3046)]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output results from one model\n",
    "\n",
    "# Initialize lists to store buy and sell signals\n",
    "\n",
    "def create_test_env(df):\n",
    "\n",
    "    # Create base environment\n",
    "    env = TradingEnv(df)\n",
    "    \n",
    "    # Apply action masking\n",
    "    env = ActionMasker(env, mask_fn)\n",
    "    \n",
    "    # Convert to vectorized environment (even though it's just 1 env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    env = VecNormalize(\n",
    "        env,\n",
    "        norm_obs=True,\n",
    "        norm_reward=True,\n",
    "        clip_obs=10.0,\n",
    "        clip_reward=10.0,\n",
    "        gamma=0.99,\n",
    "        training=False  # Set to False for testing\n",
    "    )\n",
    "    \n",
    "    return env\n",
    "\n",
    "# Example usage:\n",
    "test_env = create_test_env(df)\n",
    "\n",
    "\n",
    "buy_signals = []\n",
    "sell_signals = []\n",
    "\n",
    "print(\"Recording signals\")\n",
    "test_agent(ppo_masked_model, test_env, buy_signals, sell_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4ef8256b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "spaces must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create test environment\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m test_env \u001b[38;5;241m=\u001b[39m \u001b[43mmake_test_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m     18\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m evaluate_policy(\n\u001b[1;32m     19\u001b[0m     ppo_masked_model,\n\u001b[1;32m     20\u001b[0m     test_env,\n\u001b[1;32m     21\u001b[0m     n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     22\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     23\u001b[0m )\n",
      "Cell \u001b[0;32mIn[172], line 9\u001b[0m, in \u001b[0;36mmake_test_env\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m env \u001b[38;5;241m=\u001b[39m ActionMasker(env, mask_fn)  \u001b[38;5;66;03m# Apply action masking\u001b[39;00m\n\u001b[1;32m      8\u001b[0m env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: env])  \u001b[38;5;66;03m# Vectorized environment\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mVecNormalize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvec_normalize.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load saved normalization\u001b[39;00m\n\u001b[1;32m     10\u001b[0m env\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Important: disable training mode\u001b[39;00m\n\u001b[1;32m     11\u001b[0m env\u001b[38;5;241m.\u001b[39mnorm_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Optional: disable reward normalization if you want raw rewards\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_normalize.py:321\u001b[0m, in \u001b[0;36mVecNormalize.load\u001b[0;34m(load_path, venv)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(load_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_handler:\n\u001b[1;32m    320\u001b[0m     vec_normalize \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file_handler)\n\u001b[0;32m--> 321\u001b[0m \u001b[43mvec_normalize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_venv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvenv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vec_normalize\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_normalize.py:171\u001b[0m, in \u001b[0;36mVecNormalize.set_venv\u001b[0;34m(self, venv)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m=\u001b[39m venv\u001b[38;5;241m.\u001b[39mrender_mode\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Check that the observation_space shape match\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_shape_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturns \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs)\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/utils.py:252\u001b[0m, in \u001b[0;36mcheck_shape_equal\u001b[0;34m(space1, space2)\u001b[0m\n\u001b[1;32m    250\u001b[0m         check_shape_equal(space1\u001b[38;5;241m.\u001b[39mspaces[key], space2\u001b[38;5;241m.\u001b[39mspaces[key])\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(space1, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m space1\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m space2\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspaces must have the same shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: spaces must have the same shape"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Create a test environment with the same wrappers as training\n",
    "def make_test_env():\n",
    "    env = TradingEnv(df)  # Your custom environment\n",
    "    env = ActionMasker(env, mask_fn)  # Apply action masking\n",
    "    env = DummyVecEnv([lambda: env])  # Vectorized environment\n",
    "    env = VecNormalize.load(\"vec_normalize.pkl\", env)  # Load saved normalization\n",
    "    env.training = False  # Important: disable training mode\n",
    "    env.norm_reward = True  # Optional: disable reward normalization if you want raw rewards\n",
    "    return env\n",
    "\n",
    "# Create test environment\n",
    "test_env = make_test_env()\n",
    "\n",
    "# Evaluate\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    ppo_masked_model,\n",
    "    test_env,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"Mean reward over 10 episodes: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6cbf45-6811-4140-9ac2-805d2010a88a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Ensure directories exist\n",
    "import os\n",
    "import joblib  # or import pickle\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "os.makedirs(\"./tensorboard_logs/\", exist_ok=True)\n",
    "os.makedirs(\"./saved_models/\", exist_ok=True)\n",
    "\n",
    "def evaluate_model(model, env, num_episodes=10):\n",
    "    total_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        # Access the environment wrapped by DummyVecEnv and ActionMasker\n",
    "        env_inner = env.get_attr(\"env\", 0)[0]  # Get the first environment\n",
    "        \n",
    "        # Check if env_inner has an 'env' attribute (i.e., it's wrapped)\n",
    "        if hasattr(env_inner, \"env\"):  \n",
    "            env_inner = env_inner.env  # Unwrap ActionMasker if applicable\n",
    "\n",
    "        obs, info = env_inner.reset()  # Reset the environment\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            for i in range(30):  # Run for a fixed number of steps\n",
    "                # Extract the action mask from the original environment\n",
    "                action_mask = env_inner.get_action_mask()  # Access get_action_mask from TradingEnv inside ActionMasker\n",
    "    \n",
    "                # Convert observation and mask to tensors\n",
    "                obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=model.device).unsqueeze(0)\n",
    "                action_mask_tensor = torch.as_tensor(action_mask, dtype=torch.float32, device=model.device).unsqueeze(0)\n",
    "    \n",
    "                # Predict action with action masking\n",
    "                with torch.no_grad():\n",
    "                    action, _, _ = model.policy.forward(obs_tensor, action_mask=action_mask_tensor, deterministic=True)\n",
    "    \n",
    "                action = action.cpu().numpy()[0]  # Convert action tensor to numpy\n",
    "    \n",
    "                # Step the environment\n",
    "                obs, reward, done, truncated, info = env_inner.step(action)\n",
    "    \n",
    "                episode_reward += reward\n",
    "\n",
    "                if done or truncated:  # If the episode ends, reset the environment\n",
    "                    print(f\"ð¯ Total Reward: {episode_reward:.2f}\")\n",
    "                    obs = env_inner.reset()  # Reset environment, which only returns the observation\n",
    "                    break  # Exit loop if episode ends\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "ent_coef_range = [0.0]       # Lower values for stability\n",
    "gamma_range = [0.99]              # Lower values for short-term focus\n",
    "gae_lambda_range = [0.9]        # Wider range to test variance\n",
    "n_steps_range = [4096]           # Larger values for more experiences\n",
    "clip_range_range = [0.1]         # Narrower range for stability\n",
    "n_epochs_range = [5]               # Fewer epochs for faster training\n",
    "batch_size_range = [128]           # Larger batches for better performance\n",
    "\n",
    "force = True\n",
    "\n",
    "timesteps = 500000\n",
    "\n",
    "for ent_coef, gamma, gae_lambda, n_steps, clip_range, n_epochs, batch_size in product(\n",
    "    ent_coef_range, gamma_range, gae_lambda_range, n_steps_range, clip_range_range, n_epochs_range, batch_size_range\n",
    "):\n",
    "    model_filename = f\"./saved_models/model_ent_coef={ent_coef}_gamma={gamma}_gae_lambda={gae_lambda}_n_steps={n_steps}_clip_range={clip_range}_n_epochs={n_epochs}_batch_size={batch_size}_timesteps={timesteps}.zip\"\n",
    "    \n",
    "    if os.path.exists(model_filename) and not force:\n",
    "        print(f\"Model already exists: {model_filename}, loading instead of training...\")\n",
    "        model = PPO.load(model_filename, env=env)  # Load existing model\n",
    "    else:\n",
    "        print(f\"Training new model with: ent_coef={ent_coef}, gamma={gamma}, gae_lambda={gae_lambda}, \"\n",
    "              f\"n_steps={n_steps}, clip_range={clip_range}, n_epochs={n_epochs}, batch_size={batch_size}, timesteps={timesteps}\")\n",
    "    \n",
    "        model = PPO(\n",
    "            MaskedPPOPolicy,  # Replace with your policy (e.g., \"MlpPolicy\" or \"CnnPolicy\")\n",
    "            env,\n",
    "            ent_coef=ent_coef,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            n_steps=n_steps,\n",
    "            clip_range=clip_range,\n",
    "            n_epochs=n_epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,  # Set to 0 for less output\n",
    "            tensorboard_log=\"./tensorboard_logs/\"\n",
    "        )\n",
    "        \n",
    "        # Train for x timesteps\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "    \n",
    "        # Save the model\n",
    "        model.save(model_filename)\n",
    "        print(f\"Model saved: {model_filename}\")\n",
    "    \n",
    "        # Store the model and its parameters\n",
    "        models.append({\n",
    "            \"model_filename\": model_filename,  # Store the path to the saved model\n",
    "            \"params\": {\n",
    "                \"ent_coef\": ent_coef,\n",
    "                \"gamma\": gamma,\n",
    "                \"gae_lambda\": gae_lambda,\n",
    "                \"n_steps\": n_steps,\n",
    "                \"clip_range\": clip_range,\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Evaluate the model\n",
    "    total_reward = evaluate_model(model, env)\n",
    "    print('Average total reward:', total_reward)\n",
    "\n",
    "    env.close()  # Close all subprocesses\n",
    "    \n",
    "    results.append({\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"clip_range\": clip_range,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_reward\": total_reward,\n",
    "        \"model_filename\": model_filename  # Link results to the saved model\n",
    "    })\n",
    "\n",
    "# Save results to a CSV file\n",
    "results_df = pd.DataFrame(results)\n",
    "csv_filename = f\"grid_search_results_{timesteps}_steps.csv\"\n",
    "results_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "base_csv, csv_ext = os.path.splitext(csv_filename)\n",
    "\n",
    "# Append a counter if the file already exists\n",
    "counter = 1\n",
    "while os.path.exists(csv_filename):\n",
    "    csv_filename = f\"{base_csv}_{counter}{csv_ext}\"\n",
    "    counter += 1\n",
    "\n",
    "# Save the results DataFrame to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Results saved to {csv_filename}\")\n",
    "\n",
    "# Now, do the same for the models file\n",
    "models_filename = \"./saved_models/models_list.joblib\"\n",
    "base_model, model_ext = os.path.splitext(models_filename)\n",
    "\n",
    "counter = 1\n",
    "while os.path.exists(models_filename):\n",
    "    models_filename = f\"{base_model}_{counter}{model_ext}\"\n",
    "    counter += 1\n",
    "\n",
    "# Save the models variable\n",
    "joblib.dump(models, models_filename)\n",
    "print(f\"Models saved to {models_filename}\")\n",
    "\n",
    "print(\"Grid search completed. Results saved to\", csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2050ff4-c41c-4847-b210-7b76c430d2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2025-02-07 04:01:00'), Timestamp('2025-02-07 04:04:00')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buy_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f0259a3-365c-4ab6-8960-72a9c66c3959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2025-02-07 04:02:00'), Timestamp('2025-02-07 04:08:00')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sell_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a4db54d-abf3-4ea8-a13f-bff4d876a86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show learned buy and sell signals\n",
    "\n",
    "import mplfinance as mpf\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame, buy_signals_1000, sell_signals_1000, buy_signals_10000, and sell_signals_10000 are your lists of timestamps\n",
    "\n",
    "# Function to plot the signals for a given buy and sell signal list\n",
    "def plot_signals(buy_signals, sell_signals, title):\n",
    "    # Convert buy/sell signals to DatetimeIndex\n",
    "    buy_signals = pd.to_datetime(buy_signals)  \n",
    "    buy_signals = buy_signals[buy_signals.isin(df_original.index)]  # Filter buy signals to match df.index\n",
    "\n",
    "    sell_signals = pd.to_datetime(sell_signals)  \n",
    "    sell_signals = sell_signals[sell_signals.isin(df_original.index)]  # Filter sell signals to match df.index\n",
    "\n",
    "    # Create new columns to mark the buy and sell signals with NaN for non-signals\n",
    "    buy_signal_prices = df_original['close'].copy()\n",
    "    buy_signal_prices[~df_original.index.isin(buy_signals)] = float('nan')  # Set non-buy signals to NaN\n",
    "    \n",
    "    sell_signal_prices = df_original['close'].copy()\n",
    "    sell_signal_prices[~df_original.index.isin(sell_signals)] = float('nan')  # Set non-sell signals to NaN\n",
    "\n",
    "    # Create addplots for buy and sell signals\n",
    "    buy_signal_plot = mpf.make_addplot(buy_signal_prices, type='scatter', markersize=40, marker='^', color='green')\n",
    "    sell_signal_plot = mpf.make_addplot(sell_signal_prices, type='scatter', markersize=40, marker='v', color='red')\n",
    "\n",
    "    # Plot with the added buy and sell signals\n",
    "    fig, axes = mpf.plot(\n",
    "        df_original,\n",
    "        type='ohlc',\n",
    "        datetime_format='%Y-%m-%d %H:%M',\n",
    "        addplot=[buy_signal_plot, sell_signal_plot],\n",
    "        returnfig=True,\n",
    "        figsize=(16, 8),\n",
    "        warn_too_much_data=10000,\n",
    "        title=title #Adjust the size again to maintain consistency\n",
    "    )\n",
    "\n",
    "plot_signals(buy_signals, sell_signals, title=\"Buy/Sell Signals\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728cfc9-e606-41b0-92a8-f4d05d577650",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate profit based on buy and sell signals\n",
    "def calculate_profit(buy_signals, sell_signals, df):\n",
    "    buy_prices = df.loc[buy_signals, 'close']\n",
    "    sell_prices = df.loc[sell_signals, 'close']\n",
    "\n",
    "    # Ensure that the number of buy signals is less than or equal to the number of sell signals\n",
    "    min_length = min(len(buy_prices), len(sell_prices))\n",
    "\n",
    "    # Calculate profit for each pair of buy and sell signals\n",
    "    profits = []\n",
    "    for i in range(min_length):\n",
    "        profit = sell_prices.iloc[i] - buy_prices.iloc[i]  # Profit = Sell Price - Buy Price\n",
    "        profits.append(profit)\n",
    "\n",
    "    total_profit = sum(profits)\n",
    "    return total_profit, profits\n",
    "\n",
    "# Function to plot the signals and compare profits\n",
    "def plot_signals_and_compare_profit(buy_signals, sell_signals, df, title):\n",
    "    # Convert buy/sell signals to DatetimeIndex\n",
    "    buy_signals = pd.to_datetime(buy_signals)  \n",
    "    buy_signals = buy_signals[buy_signals.isin(df.index)]  # Filter buy signals to match df.index\n",
    "\n",
    "    sell_signals = pd.to_datetime(sell_signals)  \n",
    "    sell_signals = sell_signals[sell_signals.isin(df.index)]  # Filter sell signals to match df.index\n",
    "\n",
    "    # Create new columns to mark the buy and sell signals with NaN for non-signals\n",
    "    buy_signal_prices = df['close'].copy()\n",
    "    buy_signal_prices[~df.index.isin(buy_signals)] = float('nan')  # Set non-buy signals to NaN\n",
    "\n",
    "    sell_signal_prices = df['close'].copy()\n",
    "    sell_signal_prices[~df.index.isin(sell_signals)] = float('nan')  # Set non-sell signals to NaN\n",
    "\n",
    "    # Create addplots for buy and sell signals\n",
    "    buy_signal_plot = mpf.make_addplot(buy_signal_prices, type='scatter', markersize=40, marker='^', color='green')\n",
    "    sell_signal_plot = mpf.make_addplot(sell_signal_prices, type='scatter', markersize=40, marker='v', color='red')\n",
    "\n",
    "    # Plot with the added buy and sell signals\n",
    "    fig, axes = mpf.plot(\n",
    "        df,\n",
    "        type='ohlc',\n",
    "        style='charles',\n",
    "        datetime_format='%Y-%m-%d %H:%M',\n",
    "        addplot=[buy_signal_plot, sell_signal_plot],\n",
    "        returnfig=True,\n",
    "        figsize=(20, 10),\n",
    "        warn_too_much_data=1000  # Adjust the size again to maintain consistency\n",
    "    )\n",
    "\n",
    "    # Customize x-axis\n",
    "    #axes[0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    #plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "    # Add legend manually\n",
    "    #plt.legend(['Buy Signals', 'Sell Signals'], loc='upper left')\n",
    "\n",
    "    # Add title\n",
    "    #plt.title(title)\n",
    "\n",
    "    # Show the plot\n",
    "    #plt.show()\n",
    "\n",
    "    # Calculate and print profit for the signals\n",
    "    total_profit, profits = calculate_profit(buy_signals, sell_signals, df)\n",
    "    print(f\"Total Profit for {title}: {total_profit:.2f}\")\n",
    "    return total_profit\n",
    "\n",
    "# Calculate and plot for 1000 timesteps signals\n",
    "profit_1000 = plot_signals_and_compare_profit(buy_signals_1000, sell_signals_1000, df, title=\"Profit for 1000 Timesteps\")\n",
    "\n",
    "# Calculate and plot for 10000 timesteps signals\n",
    "profit_10000 = plot_signals_and_compare_profit(buy_signals_10000, sell_signals_10000, df, title=\"Profit for 10000 Timesteps\")\n",
    "\n",
    "# Calculate and plot for 100000 timesteps signals\n",
    "#profit_100000 = plot_signals_and_compare_profit(buy_signals_100000, sell_signals_100000, df, title=\"Profit for 100000 Timesteps\")\n",
    "\n",
    "\n",
    "# Compare the total profits\n",
    "print(f\"Total Profit for 1000 Timesteps: {profit_1000:.2f}\")\n",
    "print(f\"Total Profit for 10000 Timesteps: {profit_10000:.2f}\")\n",
    "#print(f\"Total Profit for 100000 Timesteps: {profit_100000:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
