{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5233f921-8873-4bb3-8aba-14cd38bb361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil\n",
    "\n",
    "# Determine number of available (idle) cores\n",
    "IDLE_THRESHOLD = 20.0  # percent\n",
    "cpu_usages = psutil.cpu_percent(percpu=True, interval=1)\n",
    "available_cores = sum(usage < IDLE_THRESHOLD for usage in cpu_usages)\n",
    "available_cores = max(1, available_cores) - 4 # At least 1\n",
    "\n",
    "available_cores = os.cpu_count()\n",
    "\n",
    "import mplfinance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stable-Baselines3 imports\n",
    "from stable_baselines3 import PPO,A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv  # Use this instead of DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Gymnasium (updated from Gym)\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env,Wrapper\n",
    "from gymnasium.spaces import Discrete, Box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31586fdb-7feb-43af-a1cf-106a65064180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"MGOL.csv\")  # Replace with actual file\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], format='%m/%d/%y %H:%M')\n",
    "df.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae36aaa-2abe-42a7-91da-dcd6904e9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df.index + pd.Timedelta(hours=3)\n",
    "df.index.name = 'Date'\n",
    "df = df.drop(columns=['symbol', 'frame'])\n",
    "df = df.iloc[:10]  # Select the first 30 rows\n",
    "df_original = df\n",
    "df = df[[\"close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8135381-1f8b-4bb9-a04f-f83d49d194fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionMasker(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Wrapper for action masking in environments.\n",
    "    Adds action mask as a part of the environment step.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env, mask_fn: callable):\n",
    "        super().__init__(env)\n",
    "        self.mask_fn = mask_fn\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Get the action mask\n",
    "        action_mask = self.mask_fn(self.env)\n",
    "        \n",
    "        # Add the action mask to the info dictionary\n",
    "        info['action_mask'] = action_mask\n",
    "        \n",
    "        return obs, reward, done, truncated, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c652578-a67d-410c-aaf2-365c48dfaee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "#import torch\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "class MaskedPPOPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, combined_obs, action_mask=None, deterministic=False):\n",
    "        combined_obs = combined_obs.to(dtype=torch.float32)  # Ensure float32\n",
    "\n",
    "        # Extract latent features\n",
    "        latent_pi, latent_vf = self.mlp_extractor(combined_obs.to(dtype=torch.float32))  \n",
    "\n",
    "        # Compute action distribution and value\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        values = self.value_net(latent_vf).to(dtype=torch.float32)  # Ensure float32\n",
    "\n",
    "        # Ensure action_mask is properly handled\n",
    "        if action_mask is None:\n",
    "            action_mask = combined_obs[:, 5:]  # Extract last 3 elements as action mask\n",
    "\n",
    "        if action_mask is not None:\n",
    "            action_mask_tensor = torch.as_tensor(action_mask, dtype=torch.float32, device=combined_obs.device)\n",
    "\n",
    "            # Ensure shape matches logits\n",
    "            action_mask_tensor = action_mask_tensor.view(-1, distribution.distribution.logits.shape[-1])\n",
    "\n",
    "            # Apply masking to logits\n",
    "            distribution.distribution.logits = distribution.distribution.logits.masked_fill(action_mask_tensor == 0, -1e9)\n",
    "\n",
    "        # Action selection\n",
    "        if deterministic:\n",
    "            actions = torch.argmax(distribution.distribution.probs, dim=-1)\n",
    "        else:\n",
    "            actions = distribution.sample()\n",
    "\n",
    "        log_probs = distribution.log_prob(actions)\n",
    "\n",
    "        if debug: print(f\"  - Selected Actions: {actions}\")\n",
    "\n",
    "        return actions, values, log_probs\n",
    "\n",
    "    def predict(self, observation, state=None, mask=None, action_mask=None, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            observation = torch.as_tensor(observation, device=self.device, dtype=torch.float32)\n",
    "            actions, _, _ = self.forward(observation, action_mask=action_mask, deterministic=deterministic)\n",
    "            actions = actions.cpu().numpy()\n",
    "        return actions, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c78b14-3569-431a-86d4-92d0fdf380e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0, debug=False):\n",
    "        super(RewardCallback, self).__init__(verbose)\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Metrics for tracking actions and rewards\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.iteration_rewards = []\n",
    "        self.iteration_invalid_actions = []\n",
    "        self.invalid_actions = []\n",
    "        self.valid_actions = []\n",
    "        self.current_episode_steps = 0\n",
    "        \n",
    "        # Metrics for TensorBoard logging\n",
    "        self.total_reward = 0\n",
    "        self.reward = 0\n",
    "        self.num_trades = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Collect rewards and actions\n",
    "        rewards = self.locals.get(\"rewards\", [])\n",
    "        actions = self.locals.get(\"actions\", [])\n",
    "        \n",
    "        if len(rewards) > 0:  # Check if rewards is not empty\n",
    "            self.episode_rewards.extend(rewards)\n",
    "        if len(actions) > 0:  # Check if actions is not empty\n",
    "            infos = self.locals.get(\"infos\", [])\n",
    "            for idx, info in enumerate(infos):\n",
    "                valid_actions = info.get(\"valid_actions\", [0, 1, 2])\n",
    "                action = actions[idx]\n",
    "                if action not in valid_actions:\n",
    "                    self.invalid_actions.append(action)\n",
    "                else:\n",
    "                    self.valid_actions.append(action)\n",
    "\n",
    "        self.current_episode_steps += 1\n",
    "\n",
    "        # Access the environment metrics using get_attr for SubprocVecEnv\n",
    "        if isinstance(self.training_env, SubprocVecEnv):\n",
    "            try:\n",
    "                inner_envs = self.training_env.get_attr('env')  # ActionMasker\n",
    "                for env in inner_envs:\n",
    "                    if hasattr(env, 'env'):  # Unwrap ActionMasker\n",
    "                        env = env.env\n",
    "                    self.total_reward += getattr(env, \"total_reward\", 0)\n",
    "                    self.num_trades += getattr(env, \"round_trip_trades\", 0)\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"Failed to access env attributes: {e}\")\n",
    "        else:\n",
    "            # For DummyVecEnv or single environments\n",
    "            for env in self.training_env.envs:\n",
    "                if hasattr(env, 'env'):\n",
    "                    env = env.env\n",
    "                self.total_reward += getattr(env, \"total_reward\", 0)\n",
    "                self.num_trades += getattr(env, \"round_trip_trades\", 0)\n",
    "\n",
    "        # TensorBoard logging\n",
    "        self.logger.record(\"custom/num_trades\", self.num_trades)\n",
    "        self.logger.record(\"custom/total_reward\", self.total_reward)\n",
    "\n",
    "        # Entropy logging\n",
    "        if hasattr(self.model.policy, \"action_dist\"):\n",
    "            action_dist = self.model.policy.action_dist\n",
    "            entropy = action_dist.entropy().mean().item()\n",
    "            self.logger.record(\"policy/entropy\", entropy)\n",
    "        elif hasattr(self.model.policy, \"get_distribution\"):\n",
    "            obs = self.locals.get(\"obs\", [])\n",
    "            if len(obs) > 0:  # Check if observations exist\n",
    "                action_dist = self.model.policy.get_distribution(obs)\n",
    "                entropy = action_dist.entropy().mean().item()\n",
    "                self.logger.record(\"policy/entropy\", entropy)\n",
    "\n",
    "        # Value loss logging\n",
    "        if \"value_loss\" in self.locals:\n",
    "            value_loss = self.locals[\"value_loss\"]\n",
    "            self.logger.record(\"loss/value_loss\", value_loss)\n",
    "\n",
    "        # Episode done handling\n",
    "        dones = self.locals.get(\"dones\", [])\n",
    "        if any(dones):\n",
    "            self.episode_steps.append(self.current_episode_steps)\n",
    "            self.current_episode_steps = 0\n",
    "            total_reward = np.sum(self.episode_rewards)\n",
    "            self.iteration_rewards.append(total_reward)\n",
    "            self.episode_rewards = []\n",
    "\n",
    "            invalid_count = len(self.invalid_actions)\n",
    "            valid_count = len(self.valid_actions)\n",
    "            self.iteration_invalid_actions.append(invalid_count)\n",
    "\n",
    "            if self.debug:\n",
    "                print(f\"Invalid actions in this episode: {invalid_count}\")\n",
    "                print(f\"Valid actions in this episode: {valid_count}\")\n",
    "                print(f\"Invalid actions: {self.invalid_actions}\")\n",
    "\n",
    "            self.invalid_actions = []\n",
    "\n",
    "        return True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb3f137-fe2b-4e0e-af5c-be59bd42dfd9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DebugCallback(BaseCallback):\n",
    "    def __init__(self, debug_episodes=None, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_num = 0\n",
    "        self.debug_episodes = set(debug_episodes or [])\n",
    "\n",
    "    def _on_step(self):\n",
    "        done_array = self.locals.get(\"dones\", [])\n",
    "\n",
    "        for i, done in enumerate(done_array):\n",
    "            if done:\n",
    "                self.episode_num += 1\n",
    "                debug_on = self.episode_num in self.debug_episodes\n",
    "\n",
    "                print(f\"[Debug] Episode {self.episode_num} -- Debug mode {'ON' if debug_on else 'OFF'} for env {i}\")\n",
    "\n",
    "                if debug_on:\n",
    "                # Remotely set the debug flag inside subprocess\n",
    "                    try:\n",
    "                        self.training_env.env_method(\"set_debug\", [debug_on], indices=i)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[Warning] Could not set debug in env {i}: {e}\")\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e5193-7120-4727-a0d5-d6091f6a52c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class DebugCallback(BaseCallback):\n",
    "    def __init__(self, debug_on_step=26, verbose=0, debug_episode = None):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_num = 0\n",
    "        self.debug_on_step = debug_on_step\n",
    "        self.debug_triggered = False\n",
    "        self.debug_episodes = set(debug_episodes or [])\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        dones = self.locals[\"dones\"]        # e.g. array([False,False,...])\n",
    "        infos = self.locals[\"infos\"]        # list of dicts, one per env\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            step_i = infos[i].get(\"current_step\", None)\n",
    "            # print current_step every step for debugging\n",
    "            print(f\"[Debug] Env {i} current_step={step_i}\")\n",
    "\n",
    "            if done:\n",
    "                self.episode_num += 1\n",
    "                print(f\"[Debug] Env {i} Episode {self.episode_num} done at step {step_i}\")\n",
    "\n",
    "                debug_on = self.episode_num in self.debug_episodes\n",
    "                if debug_on:\n",
    "                # Remotely set the debug flag inside subprocess\n",
    "                    try:\n",
    "                        self.training_env.env_method(\"set_debug\", [debug_on], indices=i)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[Warning] Could not set debug in env {i}: {e}\")\n",
    "                        \n",
    "                \n",
    "                # if this episode ended exactly at debug_on_step, turn on debug next time\n",
    "                if step_i == self.debug_on_step and not self.debug_triggered:\n",
    "                    print(f\"[Debug] Next Episode {self.episode_num+1} → debug ON for env {i}\")\n",
    "                    try:\n",
    "                        self.training_env.env_method(\"set_debug\", [True], indices=i)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[Warning] Could not set_debug on env {i}: {e}\")\n",
    "                    self.debug_triggered = True\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # allow it to trigger again in the next rollout\n",
    "        self.debug_triggered = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f60fca-0b10-4b16-8d07-e469bb42603e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class DebugCallback(BaseCallback):\n",
    "    def __init__(self, debug_on_step=26, verbose=0, debug_episodes=None):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_num = 0\n",
    "        self.debug_on_step = debug_on_step\n",
    "        self.debug_triggered = False\n",
    "        self.debug_episodes = set(debug_episodes or [])\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        dones = self.locals[\"dones\"]        # e.g. array([False,False,...])\n",
    "        infos = self.locals[\"infos\"]        # list of dicts, one per env\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            step_i = infos[i].get(\"current_step\", None)\n",
    "            print(f\"[Debug] Env {i} current_step={step_i}\")\n",
    "\n",
    "            if done:\n",
    "                self.episode_num += 1\n",
    "                print(f\"[Debug] Env {i} Episode {self.episode_num} done at step {step_i}\")\n",
    "\n",
    "                # Check if this episode is in the debug_episodes list\n",
    "                debug_on = self.episode_num in self.debug_episodes\n",
    "                if debug_on:\n",
    "                    # Set debug flag remotely in the environment\n",
    "                    try:\n",
    "                        self.training_env.env_method(\"set_debug\", [True], indices=i)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[Warning] Could not set debug in env {i}: {e}\")\n",
    "                        \n",
    "                # If this episode ended exactly at debug_on_step, turn on debug for the next episode\n",
    "                if step_i == self.debug_on_step and not self.debug_triggered:\n",
    "                    print(f\"[Debug] Next Episode {self.episode_num+1} → debug ON for env {i}\")\n",
    "                    try:\n",
    "                        self.training_env.env_method(\"set_debug\", [True], indices=i)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[Warning] Could not set debug in env {i}: {e}\")\n",
    "                    self.debug_triggered = True\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # Reset the trigger for the next rollout\n",
    "        self.debug_triggered = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00c896-6a4d-4356-be69-6f23d6cfd89b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class DebugCallback(BaseCallback):\n",
    "    def __init__(self, debug_on_step=26, verbose=0, debug_episodes=None):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_num = 0\n",
    "        self.debug_on_step = debug_on_step\n",
    "        self.debug_triggered = False\n",
    "        self.debug_episodes = set(debug_episodes or [])\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        dones = self.locals[\"dones\"]        # e.g. array([False,False,...])\n",
    "        infos = self.locals[\"infos\"]        # list of dicts, one per env\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            step_i = infos[i].get(\"current_step\", None)\n",
    "\n",
    "            if done:\n",
    "                self.episode_num += 1\n",
    "                print(f\"[Debug] Env {i} Episode {self.episode_num} done at step {step_i}\")\n",
    "\n",
    "                # Debug only if this episode is in the debug_episodes list\n",
    "                debug_on = self.episode_num in self.debug_episodes\n",
    "                if debug_on:\n",
    "                    # Remotely set the debug flag inside subprocess\n",
    "                    try:\n",
    "                        self.training_env.env_method(\"set_debug\", [debug_on], indices=i)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[Warning] Could not set debug in env {i}: {e}\")\n",
    "                \n",
    "                # if this episode ended exactly at debug_on_step, turn on debug next time\n",
    "                if step_i == self.debug_on_step and not self.debug_triggered:\n",
    "                    print(f\"[Debug] Next Episode {self.episode_num+1} → debug ON for env {i}\")\n",
    "                    try:\n",
    "                        self.training_env.env_method(\"set_debug\", [True], indices=i)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[Warning] Could not set_debug on env {i}: {e}\")\n",
    "                    self.debug_triggered = True\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # Allow it to trigger again in the next rollout\n",
    "        self.debug_triggered = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c054f-ecab-43c7-9b31-fc9eed457a55",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class DebugCallback(BaseCallback):\n",
    "    def __init__(self, debug_episodes=None, verbose=0, debug=False):\n",
    "        super().__init__(verbose)\n",
    "        self.debug_episodes = debug_episodes if debug_episodes is not None else {}\n",
    "        self.episode_counts = []\n",
    "        self.current_episode_steps = []\n",
    "        self.debug = debug\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        num_envs = self.training_env.num_envs if hasattr(self.training_env, \"num_envs\") else 1\n",
    "        self.episode_counts = [0 for _ in range(num_envs)]\n",
    "        self.current_episode_steps = [0 for _ in range(num_envs)]\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        dones = self.locals.get(\"dones\", [])\n",
    "        infos = self.locals.get(\"infos\", [])\n",
    "        \n",
    "    \n",
    "        for i, done in enumerate(dones):\n",
    "            self.current_episode_steps[i] += 1\n",
    "    \n",
    "            if done:\n",
    "                self.episode_counts[i] += 1\n",
    "\n",
    "                # Debug only if this episode is in the debug_episodes list\n",
    "                debug_on = self.episode_num in self.debug_episodes\n",
    "                if debug_on:\n",
    "                    # Remotely set the debug flag inside subprocess\n",
    "                    try:\n",
    "                        self.training_env.env_method(\"set_debug\", [debug_on], indices=i)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[Warning] Could not set debug in env {i}: {e}\")\n",
    "                \n",
    "                # if this episode endedi] += 1 exactly at debug_on_step, turn on debug next time\n",
    "                if step_i == self.debug_on_step and not self.debug_triggered:\n",
    "                    print(f\"[Debug] Next Episode {self.episode_num+1} → debug ON for env {i}\")\n",
    "                    try:\n",
    "                        self.training_env.env_method(\"set_debug\", [True], indices=i)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[Warning] Could not set_debug on env {i}: {e}\")\n",
    "                    self.debug_triggered = True\n",
    "                \n",
    "                episode_num = self.episode_counts[i]\n",
    "                steps = self.current_episode_steps[i]\n",
    "\n",
    "                print(self.debug_episodes)\n",
    "                print(episode_num in self.debug_episodes)\n",
    "                \n",
    "                if not self.debug_episodes or episode_num in self.debug_episodes:\n",
    "                    if self.debug:\n",
    "                        print(f\"[Debug] Env {i} Episode {episode_num} done at step {steps}\")\n",
    "    \n",
    "                # Now reset step counter AFTER printing\n",
    "                self.current_episode_steps[i] = 0\n",
    "    \n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0ef59ec-1cb5-41b1-a1f4-5bb135a3cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class DebugCallback(BaseCallback):\n",
    "    def __init__(self, debug_episodes=None, verbose=0, debug=False):\n",
    "        super().__init__(verbose)\n",
    "        self.debug_episodes = set(debug_episodes) if debug_episodes is not None else set()\n",
    "        self.episode_counts = []\n",
    "        self.episode_step_counts = {}  # Track steps per environment\n",
    "        self.debug = debug\n",
    "        self.debug_triggered = False  # Optional: Only needed if you want to trigger once\n",
    "        , self.debug_on_step = None     # Optional: If you plan to use this\n",
    "        self.last_printed_episodes = []  # To store the last printed episode number\n",
    "        self.max_episode_steps = {}  # Track the max steps per environment\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        n_envs = self.training_env.num_envs\n",
    "        # Initialize the counts and step tracking for all environments\n",
    "        self.episode_counts = [0] * n_envs\n",
    "        self.last_printed_episodes = [None] * n_envs  # Make sure this list is of the correct size\n",
    "        self.episode_step_counts = {i: 0 for i in range(n_envs)}  # Initialize step counts for all environments\n",
    "        self.max_episode_steps = {i: 0 for i in range(n_envs)}  # Initialize the max episode steps per environment\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        num_envs = getattr(self.training_env, \"num_envs\", 1)\n",
    "        self.episode_counts = [0] * num_envs\n",
    "        self.current_episode_steps = [0] * num_envs\n",
    "\n",
    "    def on_training_end(self) -> None:\n",
    "        # Print the maximum steps per environment after training ends\n",
    "        for env_id, max_steps in self.max_episode_steps.items():\n",
    "            if self.debug: print(f\"Max steps in episode for env {env_id}: {max_steps}\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        dones = self.locals.get(\"dones\", [])\n",
    "        infos = self.locals.get(\"infos\", [])\n",
    "\n",
    "        for i in range(len(dones)):\n",
    "            # Ensure that episode_step_counts has the environment index initialized\n",
    "            if i not in self.episode_step_counts:\n",
    "                self.episode_step_counts[i] = 0\n",
    "\n",
    "            # Track steps\n",
    "            self.episode_step_counts[i] += 1\n",
    "\n",
    "            # Update the maximum episode steps for each environment\n",
    "            if self.episode_step_counts[i] > self.max_episode_steps[i]:\n",
    "                self.max_episode_steps[i] = self.episode_step_counts[i]\n",
    "            \n",
    "            episode_num = self.episode_counts[i]\n",
    "\n",
    "            # Print start of episode (optional)\n",
    "            if self.last_printed_episodes[i] != episode_num:\n",
    "                if self.debug: print(f\"Current episode (env {i}): {episode_num}\")\n",
    "                self.last_printed_episodes[i] = episode_num\n",
    "\n",
    "            # Debug output if in debug_episodes list\n",
    "            if episode_num in self.debug_episodes:\n",
    "                if self.debug: print(f\"Current step (env {i}): {self.episode_step_counts[i]}\")\n",
    "                if self.debug: print(f\"[Env {i}] dones: {dones[i]} infos: {infos[i]}\")\n",
    "\n",
    "            # If episode is done, print step count and reset counter\n",
    "            if dones[i]:\n",
    "                if self.debug: print(f\"Episode {episode_num} (env {i}) finished in {self.episode_step_counts[i]} steps.\")\n",
    "                self.episode_counts[i] += 1\n",
    "                self.episode_step_counts[i] = 0  # Reset for next episode\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "087b3646-17fe-4e4c-a797-07f8208c55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0, debug=False):\n",
    "        super(RewardCallback, self).__init__(verbose)\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Episode-level tracking\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.iteration_rewards = []\n",
    "        self.iteration_invalid_actions = []\n",
    "        self.invalid_actions = []\n",
    "        self.valid_actions = []\n",
    "        self.current_episode_steps = 0\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        self.total_reward = 0\n",
    "        self.reward = 0\n",
    "        self.num_trades = 0\n",
    "\n",
    "        self.iteration_rewards_per_env = []\n",
    "        self.iteration_invalid_actions_per_env = []\n",
    "        self.n_envs = None\n",
    "        \n",
    "        # Per-environment tracking\n",
    "        self.per_env_rewards = defaultdict(list)\n",
    "        self.per_env_invalid_actions = defaultdict(list)\n",
    "\n",
    "        self.per_env_rewards_total = defaultdict(list)\n",
    "        self.per_env_invalid_actions_total = defaultdict(list)\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.n_envs = self.training_env.num_envs\n",
    "        self.current_rewards = [0.0 for _ in range(self.n_envs)]\n",
    "        self.invalid_actions = [0 for _ in range(self.n_envs)]\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        print(\"\\n=== Final Per-Environment Summary ===\")\n",
    "        # Summary across all episodes\n",
    "        for env_idx in sorted(self.per_env_rewards_total.keys()):\n",
    "            total_reward = np.sum(self.per_env_rewards_total[env_idx])\n",
    "            total_invalid = len(self.per_env_invalid_actions_total[env_idx])\n",
    "            print(f\"[Env {env_idx}] Total Reward: {total_reward:.2f}, Invalid Actions: {total_invalid}\")\n",
    "\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals.get(\"rewards\", [])\n",
    "        actions = self.locals.get(\"actions\", [])\n",
    "        infos = self.locals.get(\"infos\", [])\n",
    "\n",
    "        for idx, info in enumerate(infos):\n",
    "            valid_actions = info.get(\"valid_actions\", [0, 1, 2])\n",
    "            action = actions[idx]\n",
    "            reward = rewards[idx]\n",
    "        \n",
    "            self.per_env_rewards[idx].append(reward)\n",
    "            self.per_env_rewards_total[idx].append(reward)\n",
    "        \n",
    "            if action not in valid_actions:\n",
    "                self.invalid_actions.append(action)\n",
    "                self.per_env_invalid_actions[idx].append(action)\n",
    "                self.per_env_invalid_actions_total[idx].append(action)\n",
    "            else:\n",
    "                self.valid_actions.append(action)\n",
    "        \n",
    "            self.episode_rewards.append(reward)\n",
    "\n",
    "\n",
    "        self.current_episode_steps += 1\n",
    "\n",
    "        # Log metrics from environment\n",
    "        if isinstance(self.training_env, SubprocVecEnv):\n",
    "            try:\n",
    "                inner_envs = self.training_env.get_attr('env')  # Get ActionMasker\n",
    "                for env in inner_envs:\n",
    "                    if hasattr(env, 'env'):\n",
    "                        env = env.env\n",
    "                    self.total_reward += getattr(env, \"total_reward\", 0)\n",
    "                    self.num_trades += getattr(env, \"round_trip_trades\", 0)\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"Failed to access env attributes: {e}\")\n",
    "        else:\n",
    "            for env in self.training_env.envs:\n",
    "                if hasattr(env, 'env'):\n",
    "                    env = env.env\n",
    "                self.total_reward += getattr(env, \"total_reward\", 0)\n",
    "                self.num_trades += getattr(env, \"round_trip_trades\", 0)\n",
    "\n",
    "        # Logging to TensorBoard\n",
    "        self.logger.record(\"custom/num_trades\", self.num_trades)\n",
    "        self.logger.record(\"custom/total_reward\", self.total_reward)\n",
    "\n",
    "        # Entropy logging\n",
    "        if hasattr(self.model.policy, \"action_dist\"):\n",
    "            action_dist = self.model.policy.action_dist\n",
    "            entropy = action_dist.entropy().mean().item()\n",
    "            self.logger.record(\"policy/entropy\", entropy)\n",
    "        elif hasattr(self.model.policy, \"get_distribution\"):\n",
    "            obs = self.locals.get(\"obs\", [])\n",
    "            if len(obs) > 0:\n",
    "                action_dist = self.model.policy.get_distribution(obs)\n",
    "                entropy = action_dist.entropy().mean().item()\n",
    "                self.logger.record(\"policy/entropy\", entropy)\n",
    "\n",
    "        # Value loss logging\n",
    "        if \"value_loss\" in self.locals:\n",
    "            value_loss = self.locals[\"value_loss\"]\n",
    "            self.logger.record(\"loss/value_loss\", value_loss)\n",
    "\n",
    "        # Episode done\n",
    "        dones = self.locals.get(\"dones\", [])\n",
    "        if any(dones):\n",
    "            self.episode_steps.append(self.current_episode_steps)\n",
    "            self.current_episode_steps = 0\n",
    "            total_reward = np.sum(self.episode_rewards)\n",
    "            self.iteration_rewards.append(total_reward)\n",
    "            self.episode_rewards = []\n",
    "\n",
    "            self.iteration_invalid_actions.append(len(self.invalid_actions))\n",
    "\n",
    "            if self.debug:\n",
    "                print(\"\\n--- Episode finished ---\")\n",
    "                for env_idx in sorted(self.per_env_rewards.keys()):\n",
    "                    total_env_reward = np.sum(self.per_env_rewards[env_idx])\n",
    "                    total_env_invalid = len(self.per_env_invalid_actions[env_idx])\n",
    "                    print(f\"[Env {env_idx}] Total reward: {total_env_reward:.2f}, Invalid actions: {total_env_invalid}\")\n",
    "\n",
    "            # Reset per-episode and per-env stats\n",
    "            self.invalid_actions.clear()\n",
    "            self.valid_actions.clear()\n",
    "            self.per_env_rewards.clear()\n",
    "            self.per_env_invalid_actions.clear()\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431b7e9c-9e13-49f9-bb9e-bc175f884994",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, debug=False, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.iteration_rewards_per_env = []\n",
    "        self.iteration_invalid_actions_per_env = []\n",
    "        self.debug = debug\n",
    "        self.n_envs = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.n_envs = self.training_env.num_envs\n",
    "        self.current_rewards = [0.0 for _ in range(self.n_envs)]\n",
    "        self.invalid_actions = [0 for _ in range(self.n_envs)]\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        infos = self.locals[\"infos\"]\n",
    "\n",
    "        for i in range(self.n_envs):\n",
    "            self.current_rewards[i] += rewards[i]\n",
    "            if infos[i].get(\"invalid_action\", False):\n",
    "                self.invalid_actions[i] += 1\n",
    "\n",
    "            if dones[i]:\n",
    "                if self.debug:\n",
    "                    print(f\"[Env {i}] Done. Reward: {self.current_rewards[i]:.2f}, Invalid: {self.invalid_actions[i]}\")\n",
    "                self.iteration_rewards_per_env.append((i, self.current_rewards[i]))\n",
    "                self.iteration_invalid_actions_per_env.append((i, self.invalid_actions[i]))\n",
    "                self.current_rewards[i] = 0.0\n",
    "                self.invalid_actions[i] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # Compute summary per environment\n",
    "        from collections import defaultdict\n",
    "\n",
    "        reward_sums = defaultdict(float)\n",
    "        invalid_sums = defaultdict(int)\n",
    "\n",
    "        for env_id, reward in self.iteration_rewards_per_env:\n",
    "            reward_sums[env_id] += reward\n",
    "        for env_id, invalid in self.iteration_invalid_actions_per_env:\n",
    "            invalid_sums[env_id] += invalid\n",
    "\n",
    "        print(\"\\n=== Final Per-Environment Summary ===\")\n",
    "        for i in range(self.n_envs):\n",
    "            print(f\"[Env {i}] Total Reward: {reward_sums[i]:.2f}, Invalid Actions: {invalid_sums[i]}\")\n",
    "\n",
    "        # Print full list format for rewards and invalids\n",
    "        rewards_list = [reward_sums[i] for i in range(self.n_envs)]\n",
    "        invalids_list = [invalid_sums[i] for i in range(self.n_envs)]\n",
    "\n",
    "        print(\"\\nRewards per environment (list):\", rewards_list)\n",
    "        print(\"Invalid actions per environment (list):\", invalids_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea36fdbb-ec35-4a59-9c03-cfce1d83b19b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (12x12 and 8x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m reward_callback \u001b[38;5;241m=\u001b[39m RewardCallback(debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mppo_masked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msb3_ppo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallbackList\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreward_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Combine callbacks\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:202\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 202\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mMaskedPPOPolicy.forward\u001b[0;34m(self, combined_obs, action_mask, deterministic)\u001b[0m\n\u001b[1;32m     10\u001b[0m combined_obs \u001b[38;5;241m=\u001b[39m combined_obs\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Ensure float32\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Extract latent features\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_obs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Compute action distribution and value\u001b[39;00m\n\u001b[1;32m     16\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py:257\u001b[0m, in \u001b[0;36mMlpExtractor.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[th\u001b[38;5;241m.\u001b[39mTensor, th\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    253\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    :return: latent_policy, latent_value of the specified network.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m        If all layers are shared, then ``latent_policy == latent_value``\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_critic(features)\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py:260\u001b[0m, in \u001b[0;36mMlpExtractor.forward_actor\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (12x12 and 8x64)"
     ]
    }
   ],
   "source": [
    "# Initialize parallel environments and train model\n",
    "import torch\n",
    "from trading_env_sb3 import TradingEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "\n",
    "\n",
    "# Initialize the custom TradingEnv environment\n",
    "env1 = TradingEnv(df)  # Your custom environment\n",
    "\n",
    "# Define the mask_fn to get valid actions from the environment\n",
    "def mask_fn(env: gym.Env) -> np.ndarray:\n",
    "    return env.get_action_mask()  # Get valid action mask from the environment\n",
    "\n",
    "# Wrap the environment with ActionMasker to apply action masking\n",
    "env_masked = ActionMasker(env1, mask_fn)  # Apply the ActionMasker wrapper\n",
    "\n",
    "# Define the number of CPU cores to use\n",
    "num_cpu = available_cores  # Get the number of available CPU cores\n",
    "\n",
    "#num_cpu = 4\n",
    "\n",
    "# --- Function to create the wrapped environment ---\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = TradingEnv(df)  # Your DataFrame must be accessible here\n",
    "        return ActionMasker(env, mask_fn)\n",
    "    return _init\n",
    "\n",
    "# Create parallel environments using SubprocVecEnv\n",
    "env = SubprocVecEnv([make_env() for _ in range(num_cpu)])\n",
    "\n",
    "\n",
    "# Define PPO model with the custom policy using the vectorized environment\n",
    "ppo_masked_model = PPO(\n",
    "    MaskedPPOPolicy,  # Custom policy\n",
    "    env,               # Your environment\n",
    "    verbose=0,         \n",
    "    tensorboard_log=\"./tensorboard_logs/\",\n",
    "    ent_coef=0.0,      # Small exploration penalty\n",
    "    gamma=0.99,       # Discount factor for long-term rewards\n",
    "    gae_lambda=0.9,    # Optimistic advantage estimation (strongly favors long-term)\n",
    "    n_steps=512,      # Large number of timesteps per iteration\n",
    "    clip_range=0.1,    # Clipping to allow more aggressive updates\n",
    "    n_epochs=5,       # Number of passes over the data (many epochs to overfit)\n",
    "    batch_size=128    # Large batch size for stability in updates\n",
    ")\n",
    "\n",
    "# Train the model with the callback\n",
    "from multiprocessing import set_start_method\n",
    "set_start_method('spawn', force=True)\n",
    "\n",
    "debug_episodes = {}  # Or use range() for patterns\n",
    "debug_callback = DebugCallback(debug_episodes=debug_episodes)\n",
    "\n",
    "# Initialize the callback\n",
    "reward_callback = RewardCallback(debug=False)\n",
    "\n",
    "try:\n",
    "    ppo_masked_model.learn(\n",
    "        total_timesteps=10000,\n",
    "        progress_bar=False,\n",
    "        tb_log_name=\"sb3_ppo\",\n",
    "        callback=CallbackList([reward_callback, debug_callback])  # Combine callbacks\n",
    "    )\n",
    "finally:\n",
    "    try:\n",
    "        env.close()\n",
    "    except EOFError:\n",
    "        print(\"Warning: env subprocess already crashed, skipping close()\")\n",
    "\n",
    "# --- Post-training summary ---\n",
    "formatted_rewards = ', '.join(f\"{reward:.1f}\" for reward in reward_callback.iteration_rewards)\n",
    "formatted_invalid_actions = ', '.join(str(invalid) for invalid in reward_callback.iteration_invalid_actions)\n",
    "\n",
    "print(\"Total rewards per iteration:\", formatted_rewards)\n",
    "print(\"Total invalid actions per iteration:\", formatted_invalid_actions)\n",
    "\n",
    "# Call custom summary logic\n",
    "#reward_callback._on_training_end()\n",
    "\n",
    "# Save the model\n",
    "ppo_masked_model.save('sb3_ppo_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8ff321f-c797-4c8d-bd1f-5619c2d08978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(model, env, buy_signals_list, sell_signals_list, seed=42):\n",
    "    \"\"\"\n",
    "    Test the trained model with action masking.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PPO model.\n",
    "        env: Environment with action masking.\n",
    "        buy_signals_list: List to store buy signal timestamps.\n",
    "        sell_signals_list: List to store sell signal timestamps.\n",
    "\n",
    "    Returns:\n",
    "        total_rewards: Total rewards accumulated during the test.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)  # Ensure deterministic behavior\n",
    "\n",
    "    # Reset the environment, get batch of observations\n",
    "    obs = env.reset()  # This will return a batch of observations for each parallel environment\n",
    "    done = [False] * len(obs)  # Done flags for each environment in the batch\n",
    "    total_rewards = [0] * len(obs)  # Total rewards for each environment\n",
    "\n",
    "    while not all(done):  # Run until all environments are done\n",
    "        for i in range(30):  # Run for a fixed number of steps per environment\n",
    "\n",
    "            # Extract the action mask for each environment (if applicable)\n",
    "            action_mask = [env.get_action_mask() for env in env.envs]  # Get action masks for all parallel environments\n",
    "\n",
    "            # Convert observations and masks to tensors\n",
    "            obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=model.device)\n",
    "            action_mask_tensor = torch.as_tensor(action_mask, dtype=torch.float32, device=model.device)\n",
    "\n",
    "            # Predict actions with action masking\n",
    "            with torch.no_grad():\n",
    "                action, _, _ = model.policy.forward(combined_obs=obs_tensor, action_mask=action_mask_tensor, deterministic=True)\n",
    "\n",
    "            # Convert actions to numpy\n",
    "            action = action.cpu().numpy()\n",
    "\n",
    "            # Step through the environment and get next observations, rewards, done flags, etc.\n",
    "            obs, rewards, dones, truncateds, infos = env.step(action)\n",
    "\n",
    "            # Debugging Output for each environment in the batch\n",
    "            for idx, done_flag in enumerate(dones):\n",
    "                print(f\"Step {i+1}: Environment {idx+1} | Action {action[idx]} | Reward: {rewards[idx]:.2f} | Done: {done_flag}\")\n",
    "\n",
    "                total_rewards[idx] += rewards[idx]\n",
    "\n",
    "                # Record buy or sell signals based on action\n",
    "                current_step = env.envs[idx].current_step  # Access current_step for each environment\n",
    "                if action[idx] == 1:  # Buy signal\n",
    "                    buy_signals_list.append(df.index[current_step])\n",
    "                elif action[idx] == 2:  # Sell signal\n",
    "                    sell_signals_list.append(df.index[current_step])\n",
    "\n",
    "                if done_flag or truncateds[idx]:  # If any environment ends\n",
    "                    print(f\"🎯 Total Reward for Environment {idx+1}: {total_rewards[idx]:.2f}\")\n",
    "                    obs[idx] = env.envs[idx].reset()  # Reset the environment and get new observation for that environment\n",
    "                    done[idx] = True  # Mark that this environment has finished\n",
    "\n",
    "    return sum(total_rewards)  # Return total rewards accumulated from all environments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc87bb8-5c03-4b9b-bb62-4b01cbb7d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single environment inside SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "import numpy as np\n",
    "\n",
    "def make_env():\n",
    "    return TradingEnv(df)\n",
    "\n",
    "# Wrap the environment with SubprocVecEnv (with just one environment for now)\n",
    "env = SubprocVecEnv([make_env])\n",
    "\n",
    "# Reset environment\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.random.choice(env.action_space.n)\n",
    "    action = np.array([action])  # Wrap the action in a numpy array to match the expected format\n",
    "    obs, reward, done, truncated, info = env.step(action)  # Pass the action as an array\n",
    "    print(f\"Action: {action} | Reward: {reward} | Done: {done}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335e62b-1e17-4517-b182-a9b5e5154316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output results from one model\n",
    "\n",
    "buy_signals = []\n",
    "sell_signals = []\n",
    "\n",
    "print(\"Recording signals\")\n",
    "\n",
    "test_agent(PPO.load(\"sb3_ppo_model\"), env, buy_signals, sell_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6cbf45-6811-4140-9ac2-805d2010a88a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Ensure directories exist\n",
    "import os\n",
    "import joblib  # or import pickle\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "os.makedirs(\"./tensorboard_logs/\", exist_ok=True)\n",
    "os.makedirs(\"./saved_models/\", exist_ok=True)\n",
    "\n",
    "def evaluate_model(model, env, num_episodes=10):\n",
    "    total_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        # Access the environment wrapped by DummyVecEnv and ActionMasker\n",
    "        env_inner = env.get_attr(\"env\", 0)[0]  # Get the first environment\n",
    "        \n",
    "        # Check if env_inner has an 'env' attribute (i.e., it's wrapped)\n",
    "        if hasattr(env_inner, \"env\"):  \n",
    "            env_inner = env_inner.env  # Unwrap ActionMasker if applicable\n",
    "\n",
    "        obs, info = env_inner.reset()  # Reset the environment\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            for i in range(30):  # Run for a fixed number of steps\n",
    "                # Extract the action mask from the original environment\n",
    "                action_mask = env_inner.get_action_mask()  # Access get_action_mask from TradingEnv inside ActionMasker\n",
    "    \n",
    "                # Convert observation and mask to tensors\n",
    "                obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=model.device).unsqueeze(0)\n",
    "                action_mask_tensor = torch.as_tensor(action_mask, dtype=torch.float32, device=model.device).unsqueeze(0)\n",
    "    \n",
    "                # Predict action with action masking\n",
    "                with torch.no_grad():\n",
    "                    action, _, _ = model.policy.forward(obs_tensor, action_mask=action_mask_tensor, deterministic=True)\n",
    "    \n",
    "                action = action.cpu().numpy()[0]  # Convert action tensor to numpy\n",
    "    \n",
    "                # Step the environment\n",
    "                obs, reward, done, truncated, info = env_inner.step(action)\n",
    "    \n",
    "                episode_reward += reward\n",
    "\n",
    "                if done or truncated:  # If the episode ends, reset the environment\n",
    "                    print(f\"🎯 Total Reward: {episode_reward:.2f}\")\n",
    "                    obs = env_inner.reset()  # Reset environment, which only returns the observation\n",
    "                    break  # Exit loop if episode ends\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "ent_coef_range = [0.0]       # Lower values for stability\n",
    "gamma_range = [0.99]              # Lower values for short-term focus\n",
    "gae_lambda_range = [0.9]        # Wider range to test variance\n",
    "n_steps_range = [4096]           # Larger values for more experiences\n",
    "clip_range_range = [0.1]         # Narrower range for stability\n",
    "n_epochs_range = [5]               # Fewer epochs for faster training\n",
    "batch_size_range = [128]           # Larger batches for better performance\n",
    "\n",
    "force = True\n",
    "\n",
    "timesteps = 500000\n",
    "\n",
    "for ent_coef, gamma, gae_lambda, n_steps, clip_range, n_epochs, batch_size in product(\n",
    "    ent_coef_range, gamma_range, gae_lambda_range, n_steps_range, clip_range_range, n_epochs_range, batch_size_range\n",
    "):\n",
    "    model_filename = f\"./saved_models/model_ent_coef={ent_coef}_gamma={gamma}_gae_lambda={gae_lambda}_n_steps={n_steps}_clip_range={clip_range}_n_epochs={n_epochs}_batch_size={batch_size}_timesteps={timesteps}.zip\"\n",
    "    \n",
    "    if os.path.exists(model_filename) and not force:\n",
    "        print(f\"Model already exists: {model_filename}, loading instead of training...\")\n",
    "        model = PPO.load(model_filename, env=env)  # Load existing model\n",
    "    else:\n",
    "        print(f\"Training new model with: ent_coef={ent_coef}, gamma={gamma}, gae_lambda={gae_lambda}, \"\n",
    "              f\"n_steps={n_steps}, clip_range={clip_range}, n_epochs={n_epochs}, batch_size={batch_size}, timesteps={timesteps}\")\n",
    "    \n",
    "        model = PPO(\n",
    "            MaskedPPOPolicy,  # Replace with your policy (e.g., \"MlpPolicy\" or \"CnnPolicy\")\n",
    "            env,\n",
    "            ent_coef=ent_coef,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            n_steps=n_steps,\n",
    "            clip_range=clip_range,\n",
    "            n_epochs=n_epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,  # Set to 0 for less output\n",
    "            tensorboard_log=\"./tensorboard_logs/\"\n",
    "        )\n",
    "        \n",
    "        # Train for x timesteps\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "    \n",
    "        # Save the model\n",
    "        model.save(model_filename)\n",
    "        print(f\"Model saved: {model_filename}\")\n",
    "    \n",
    "        # Store the model and its parameters\n",
    "        models.append({\n",
    "            \"model_filename\": model_filename,  # Store the path to the saved model\n",
    "            \"params\": {\n",
    "                \"ent_coef\": ent_coef,\n",
    "                \"gamma\": gamma,\n",
    "                \"gae_lambda\": gae_lambda,\n",
    "                \"n_steps\": n_steps,\n",
    "                \"clip_range\": clip_range,\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Evaluate the model\n",
    "    total_reward = evaluate_model(model, env)\n",
    "    print('Average total reward:', total_reward)\n",
    "\n",
    "    env.close()  # Close all subprocesses\n",
    "    \n",
    "    results.append({\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"clip_range\": clip_range,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_reward\": total_reward,\n",
    "        \"model_filename\": model_filename  # Link results to the saved model\n",
    "    })\n",
    "\n",
    "# Save results to a CSV file\n",
    "results_df = pd.DataFrame(results)\n",
    "csv_filename = f\"grid_search_results_{timesteps}_steps.csv\"\n",
    "results_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "base_csv, csv_ext = os.path.splitext(csv_filename)\n",
    "\n",
    "# Append a counter if the file already exists\n",
    "counter = 1\n",
    "while os.path.exists(csv_filename):\n",
    "    csv_filename = f\"{base_csv}_{counter}{csv_ext}\"\n",
    "    counter += 1\n",
    "\n",
    "# Save the results DataFrame to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Results saved to {csv_filename}\")\n",
    "\n",
    "# Now, do the same for the models file\n",
    "models_filename = \"./saved_models/models_list.joblib\"\n",
    "base_model, model_ext = os.path.splitext(models_filename)\n",
    "\n",
    "counter = 1\n",
    "while os.path.exists(models_filename):\n",
    "    models_filename = f\"{base_model}_{counter}{model_ext}\"\n",
    "    counter += 1\n",
    "\n",
    "# Save the models variable\n",
    "joblib.dump(models, models_filename)\n",
    "print(f\"Models saved to {models_filename}\")\n",
    "\n",
    "print(\"Grid search completed. Results saved to\", csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631d76c-4b56-4592-b082-8b32156deaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2050ff4-c41c-4847-b210-7b76c430d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0259a3-365c-4ab6-8960-72a9c66c3959",
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4db54d-abf3-4ea8-a13f-bff4d876a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show learned buy and sell signals\n",
    "\n",
    "import mplfinance as mpf\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame, buy_signals_1000, sell_signals_1000, buy_signals_10000, and sell_signals_10000 are your lists of timestamps\n",
    "\n",
    "# Function to plot the signals for a given buy and sell signal list\n",
    "def plot_signals(buy_signals, sell_signals, title):\n",
    "    # Convert buy/sell signals to DatetimeIndex\n",
    "    buy_signals = pd.to_datetime(buy_signals)  \n",
    "    buy_signals = buy_signals[buy_signals.isin(df_original.index)]  # Filter buy signals to match df.index\n",
    "\n",
    "    sell_signals = pd.to_datetime(sell_signals)  \n",
    "    sell_signals = sell_signals[sell_signals.isin(df_original.index)]  # Filter sell signals to match df.index\n",
    "\n",
    "    # Create new columns to mark the buy and sell signals with NaN for non-signals\n",
    "    buy_signal_prices = df_original['close'].copy()\n",
    "    buy_signal_prices[~df_original.index.isin(buy_signals)] = float('nan')  # Set non-buy signals to NaN\n",
    "    \n",
    "    sell_signal_prices = df_original['close'].copy()\n",
    "    sell_signal_prices[~df_original.index.isin(sell_signals)] = float('nan')  # Set non-sell signals to NaN\n",
    "\n",
    "    # Create addplots for buy and sell signals\n",
    "    buy_signal_plot = mpf.make_addplot(buy_signal_prices, type='scatter', markersize=40, marker='^', color='green')\n",
    "    sell_signal_plot = mpf.make_addplot(sell_signal_prices, type='scatter', markersize=40, marker='v', color='red')\n",
    "\n",
    "    # Plot with the added buy and sell signals\n",
    "    fig, axes = mpf.plot(\n",
    "        df_original,\n",
    "        type='ohlc',\n",
    "        datetime_format='%Y-%m-%d %H:%M',\n",
    "        addplot=[buy_signal_plot, sell_signal_plot],\n",
    "        returnfig=True,\n",
    "        figsize=(16, 8),\n",
    "        warn_too_much_data=10000,\n",
    "        title=title #Adjust the size again to maintain consistency\n",
    "    )\n",
    "\n",
    "plot_signals(buy_signals, sell_signals, title=\"Buy/Sell Signals\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728cfc9-e606-41b0-92a8-f4d05d577650",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import mplfinance as mpf\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate profit based on buy and sell signals\n",
    "def calculate_profit(buy_signals, sell_signals, df):\n",
    "    buy_prices = df.loc[buy_signals, 'close']\n",
    "    sell_prices = df.loc[sell_signals, 'close']\n",
    "\n",
    "    # Ensure that the number of buy signals is less than or equal to the number of sell signals\n",
    "    min_length = min(len(buy_prices), len(sell_prices))\n",
    "\n",
    "    # Calculate profit for each pair of buy and sell signals\n",
    "    profits = []\n",
    "    for i in range(min_length):\n",
    "        profit = sell_prices.iloc[i] - buy_prices.iloc[i]  # Profit = Sell Price - Buy Price\n",
    "        profits.append(profit)\n",
    "\n",
    "    total_profit = sum(profits)\n",
    "    return total_profit, profits\n",
    "\n",
    "# Function to plot the signals and compare profits\n",
    "def plot_signals_and_compare_profit(buy_signals, sell_signals, df, title):\n",
    "    # Convert buy/sell signals to DatetimeIndex\n",
    "    buy_signals = pd.to_datetime(buy_signals)  \n",
    "    buy_signals = buy_signals[buy_signals.isin(df.index)]  # Filter buy signals to match df.index\n",
    "\n",
    "    sell_signals = pd.to_datetime(sell_signals)  \n",
    "    sell_signals = sell_signals[sell_signals.isin(df.index)]  # Filter sell signals to match df.index\n",
    "\n",
    "    # Create new columns to mark the buy and sell signals with NaN for non-signals\n",
    "    buy_signal_prices = df['close'].copy()\n",
    "    buy_signal_prices[~df.index.isin(buy_signals)] = float('nan')  # Set non-buy signals to NaN\n",
    "\n",
    "    sell_signal_prices = df['close'].copy()\n",
    "    sell_signal_prices[~df.index.isin(sell_signals)] = float('nan')  # Set non-sell signals to NaN\n",
    "\n",
    "    # Create addplots for buy and sell signals\n",
    "    buy_signal_plot = mpf.make_addplot(buy_signal_prices, type='scatter', markersize=40, marker='^', color='green')\n",
    "    sell_signal_plot = mpf.make_addplot(sell_signal_prices, type='scatter', markersize=40, marker='v', color='red')\n",
    "\n",
    "    # Plot with the added buy and sell signals\n",
    "    fig, axes = mpf.plot(\n",
    "        df,\n",
    "        type='ohlc',\n",
    "        style='charles',\n",
    "        datetime_format='%Y-%m-%d %H:%M',\n",
    "        addplot=[buy_signal_plot, sell_signal_plot],\n",
    "        returnfig=True,\n",
    "        figsize=(20, 10),\n",
    "        warn_too_much_data=1000  # Adjust the size again to maintain consistency\n",
    "    )\n",
    "\n",
    "    # Customize x-axis\n",
    "    #axes[0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    #plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "    # Add legend manually\n",
    "    #plt.legend(['Buy Signals', 'Sell Signals'], loc='upper left')\n",
    "\n",
    "    # Add title\n",
    "    #plt.title(title)\n",
    "\n",
    "    # Show the plot\n",
    "    #plt.show()\n",
    "\n",
    "    # Calculate and print profit for the signals\n",
    "    total_profit, profits = calculate_profit(buy_signals, sell_signals, df)\n",
    "    print(f\"Total Profit for {title}: {total_profit:.2f}\")\n",
    "    return total_profit\n",
    "\n",
    "# Calculate and plot for 1000 timesteps signals\n",
    "profit_1000 = plot_signals_and_compare_profit(buy_signals_1000, sell_signals_1000, df, title=\"Profit for 1000 Timesteps\")\n",
    "\n",
    "# Calculate and plot for 10000 timesteps signals\n",
    "profit_10000 = plot_signals_and_compare_profit(buy_signals_10000, sell_signals_10000, df, title=\"Profit for 10000 Timesteps\")\n",
    "\n",
    "# Calculate and plot for 100000 timesteps signals\n",
    "#profit_100000 = plot_signals_and_compare_profit(buy_signals_100000, sell_signals_100000, df, title=\"Profit for 100000 Timesteps\")\n",
    "\n",
    "\n",
    "# Compare the total profits\n",
    "print(f\"Total Profit for 1000 Timesteps: {profit_1000:.2f}\")\n",
    "print(f\"Total Profit for 10000 Timesteps: {profit_10000:.2f}\")\n",
    "#print(f\"Total Profit for 100000 Timesteps: {profit_100000:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c85f7e-e0a3-41ac-ac12-f0759ffe732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f493a97c-deec-4100-8a11-cfa13f23044b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3(84721) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e157551-85b3-4cc2-9656-b972ba1df24f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
