{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5233f921-8873-4bb3-8aba-14cd38bb361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil\n",
    "\n",
    "# Determine number of available (idle) cores\n",
    "IDLE_THRESHOLD = 20.0  # percent\n",
    "cpu_usages = psutil.cpu_percent(percpu=True, interval=1)\n",
    "available_cores = sum(usage < IDLE_THRESHOLD for usage in cpu_usages)\n",
    "available_cores = max(1, available_cores) - 4 # At least 1\n",
    "\n",
    "available_cores = os.cpu_count()\n",
    "\n",
    "import mplfinance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stable-Baselines3 imports\n",
    "from stable_baselines3 import PPO,A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv  # Use this instead of DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "#from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "\n",
    "# Gymnasium (updated from Gym)\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env,Wrapper\n",
    "from gymnasium.spaces import Discrete, Box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31586fdb-7feb-43af-a1cf-106a65064180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"MGOL.csv\")  # Replace with actual file\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], format='%m/%d/%y %H:%M')\n",
    "df.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ae36aaa-2abe-42a7-91da-dcd6904e9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df.index + pd.Timedelta(hours=3)\n",
    "df.index.name = 'Date'\n",
    "df = df.drop(columns=['symbol', 'frame'])\n",
    "df = df.iloc[:10]  # Select the first 30 rows\n",
    "df_original = df\n",
    "df = df[[\"close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdcf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./tensorboard_logs/sb3_ppo_debug_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10       |\n",
      "|    ep_rew_mean     | -0.0391  |\n",
      "| time/              |          |\n",
      "|    fps             | 1763     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10          |\n",
      "|    ep_rew_mean          | -0.0346     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1544        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015041508 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | -0.515      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.178       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.597       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10          |\n",
      "|    ep_rew_mean          | -0.0264     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1442        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022732493 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.63       |\n",
      "|    explained_variance   | 0.59        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0723      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0557     |\n",
      "|    value_loss           | 0.32        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 10         |\n",
      "|    ep_rew_mean          | -0.0161    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1413       |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03382354 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.526     |\n",
      "|    explained_variance   | 0.446      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0412     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0595    |\n",
      "|    value_loss           | 0.305      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 10         |\n",
      "|    ep_rew_mean          | -0.0033    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1413       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10123776 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.377     |\n",
      "|    explained_variance   | 0.333      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0397     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0516    |\n",
      "|    value_loss           | 0.211      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_mask.ppo_mask.MaskablePPO at 0x3504ca800>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import trading_env_sb3_ver2c\n",
    "importlib.reload(trading_env_sb3_ver2c)\n",
    "from trading_env_sb3_ver2c import TradingEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor, VecNormalize\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "\n",
    "# Create directories for saving\n",
    "log_dir = \"./tensorboard_logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "num_cpu = 1\n",
    "\n",
    "def make_env():\n",
    "    env = TradingEnv(df)  # Initialize env\n",
    "    check_env(env, warn=True)\n",
    "    return env\n",
    "\n",
    "# Create parallel environments using SubprocVecEnv\n",
    "env = SubprocVecEnv([make_env for _ in range(num_cpu)])\n",
    "\n",
    "# Wrap with VecMonitor first\n",
    "env = VecMonitor(env)\n",
    "\n",
    "# Wrap with VecNormalize\n",
    "env = VecNormalize(env)\n",
    "\n",
    "model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=1, tensorboard_log=\"./tensorboard_logs/\", )\n",
    "model.learn(total_timesteps=100000, tb_log_name=\"sb3_ppo_debug\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cae10ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval callback initialized!\n",
      "\n",
      "New best reward: -0.0518 at episode 1\n",
      "\n",
      "Training progress at step 100:\n",
      "Learning rate: 0.000300\n",
      "Entropy: 0.6904\n",
      "\n",
      "Recent (last 10) episodes:\n",
      "Mean reward: -0.0518\n",
      "Std reward: 0.0000\n",
      "Min reward: -0.0518\n",
      "Max reward: -0.0518\n",
      "\n",
      "New best reward: -0.0504 at episode 2\n",
      "\n",
      "New best reward: -0.0135 at episode 3\n",
      "\n",
      "Training progress at step 200:\n",
      "Learning rate: 0.000300\n",
      "Entropy: 0.6578\n",
      "\n",
      "Recent (last 10) episodes:\n",
      "Mean reward: -0.0386\n",
      "Std reward: 0.0177\n",
      "Min reward: -0.0518\n",
      "Max reward: -0.0135\n",
      "\n",
      "Training progress at step 300:\n",
      "Learning rate: 0.000300\n",
      "Entropy: 0.6424\n",
      "\n",
      "Recent (last 10) episodes:\n",
      "Mean reward: -0.0343\n",
      "Std reward: 0.0171\n",
      "Min reward: -0.0518\n",
      "Max reward: -0.0135\n",
      "\n",
      "Training progress at step 400:\n",
      "Learning rate: 0.000300\n",
      "Entropy: 0.6890\n",
      "\n",
      "Recent (last 10) episodes:\n",
      "Mean reward: -0.0311\n",
      "Std reward: 0.0152\n",
      "Min reward: -0.0518\n",
      "Max reward: -0.0135\n",
      "\n",
      "Training progress at step 500:\n",
      "Learning rate: 0.000300\n",
      "Entropy: 0.6931\n",
      "\n",
      "Recent (last 10) episodes:\n",
      "Mean reward: -0.0298\n",
      "Std reward: 0.0144\n",
      "Min reward: -0.0518\n",
      "Max reward: -0.0135\n",
      "\n",
      "New best reward: -0.0017 at episode 9\n",
      "\n",
      "Training progress at step 600:\n",
      "Learning rate: 0.000300\n",
      "Entropy: 0.6820\n",
      "\n",
      "Recent (last 10) episodes:\n",
      "Mean reward: -0.0249\n",
      "Std reward: 0.0159\n",
      "Min reward: -0.0518\n",
      "Max reward: -0.0017\n",
      "\n",
      "Training progress at step 700:\n",
      "Learning rate: 0.000300\n",
      "Entropy: 0.6615\n",
      "\n",
      "Recent (last 10) episodes:\n",
      "Mean reward: -0.0246\n",
      "Std reward: 0.0151\n",
      "Min reward: -0.0518\n",
      "Max reward: -0.0017\n",
      "\n",
      "New best reward: 0.0000 at episode 12\n",
      "\n",
      "Training progress at step 800:\n",
      "Learning rate: 0.000300\n",
      "Entropy: 0.6339\n",
      "\n",
      "Recent (last 10) episodes:\n",
      "Mean reward: -0.0154\n",
      "Std reward: 0.0092\n",
      "Min reward: -0.0319\n",
      "Max reward: 0.0000\n",
      "\n",
      "New best reward: 0.0023 at episode 14\n",
      "\n",
      "Training progress at step 900:\n",
      "Learning rate: 0.000300\n",
      "Entropy: 0.6368\n",
      "\n",
      "Recent (last 10) episodes:\n",
      "Mean reward: -0.0116\n",
      "Std reward: 0.0111\n",
      "Min reward: -0.0319\n",
      "Max reward: 0.0023\n",
      "\n",
      "New best reward: 0.0080 at episode 15\n",
      "\n",
      "Training progress at step 1000:\n",
      "Learning rate: 0.000300\n",
      "Entropy: 0.6909\n",
      "\n",
      "Recent (last 10) episodes:\n",
      "Mean reward: -0.0077\n",
      "Std reward: 0.0102\n",
      "Min reward: -0.0221\n",
      "Max reward: 0.0080\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import trading_env_sb3_ver2c\n",
    "importlib.reload(trading_env_sb3_ver2c)\n",
    "from trading_env_sb3_ver2c import TradingEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor, VecNormalize\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Import and reload the callbacks\n",
    "import debug_eval_callback\n",
    "importlib.reload(debug_eval_callback)\n",
    "from debug_eval_callback import DebugEvalCallback\n",
    "\n",
    "import learning_progress_callback\n",
    "importlib.reload(learning_progress_callback)\n",
    "from learning_progress_callback import LearningProgressCallback\n",
    "\n",
    "# Create directories for saving\n",
    "log_dir = \"./tensorboard_logs/PPO_Trading_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "num_cpu = 1\n",
    "\n",
    "def make_env():\n",
    "    env = TradingEnv(df)  # Initialize env\n",
    "    check_env(env, warn=True)\n",
    "    return env\n",
    "\n",
    "def make_eval_env():\n",
    "    env = TradingEnv(df)  # Initialize env\n",
    "    check_env(env, warn=True)\n",
    "    return env\n",
    "\n",
    "# Create parallel environments for training\n",
    "train_env = SubprocVecEnv([make_env for _ in range(num_cpu)])\n",
    "train_env = VecMonitor(train_env)\n",
    "train_env = VecNormalize(train_env, training=True, norm_reward=True, norm_obs=True)\n",
    "\n",
    "# Create parallel environments for evaluation\n",
    "eval_env = SubprocVecEnv([make_eval_env for _ in range(1)])\n",
    "eval_env = VecMonitor(eval_env)\n",
    "eval_env = VecNormalize(eval_env, training=False, norm_reward=False)\n",
    "\n",
    "# Create evaluation callback\n",
    "eval_callback = DebugEvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=log_dir,\n",
    "    log_path=log_dir,\n",
    "    eval_freq=5000,  # Evaluate every 5000 steps\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    n_eval_episodes=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Create learning progress callback\n",
    "learning_progress_callback = LearningProgressCallback()\n",
    "\n",
    "# Create model\n",
    "model = MaskablePPO(\n",
    "    MaskableActorCriticPolicy,\n",
    "    train_env,\n",
    "    verbose=0,  # Set to 1 for progress output\n",
    "    tensorboard_log=log_dir,\n",
    "    ent_coef=0.001,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    n_steps=64,\n",
    "    clip_range=0.1,\n",
    "    clip_range_vf=0.1,\n",
    "    n_epochs=20,\n",
    "    batch_size=32,\n",
    "    max_grad_norm=0.5,\n",
    "    vf_coef=0.5,\n",
    "    normalize_advantage=False,\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=dict(pi=[256, 256], vf=[256, 256])\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(\n",
    "    total_timesteps=1000,\n",
    "    callback=[eval_callback, learning_progress_callback],\n",
    "    log_interval=1\n",
    ")\n",
    "\n",
    "# Save final model and normalization stats\n",
    "model.save(os.path.join(log_dir, \"final_model\"))\n",
    "train_env.save(os.path.join(log_dir, \"vec_normalize.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e9562e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Episode 1 ===\n",
      "Initial observation: [[ 2.0362396e+00 -9.0626061e-01  8.0337775e-01  4.9388060e-01\n",
      "  -1.5661801e+00  9.0771191e-06  9.0626061e-01 -9.0626061e-01]]\n",
      "Step 0: BUY at 0.3386\n",
      "Step 1: BUY at 0.3265\n",
      "Step 2: BUY at 0.3288\n",
      "Step 3: BUY at 0.3123\n",
      "Episode 1 completed with reward: -0.50\n",
      "  Buy signals: 4\n",
      "  Sell signals: 0\n",
      "\n",
      "=== Episode 2 ===\n",
      "Initial observation: [[ 2.0362396e+00 -9.0626061e-01  8.0337775e-01  4.9388060e-01\n",
      "  -1.5661801e+00  9.0771191e-06  9.0626061e-01 -9.0626061e-01]]\n",
      "Step 0: BUY at 0.3386\n",
      "Step 1: BUY at 0.3265\n",
      "Step 2: BUY at 0.3288\n",
      "Step 3: BUY at 0.3123\n",
      "Episode 2 completed with reward: -0.50\n",
      "  Buy signals: 4\n",
      "  Sell signals: 0\n",
      "\n",
      "=== Episode 3 ===\n",
      "Initial observation: [[ 2.0362396e+00 -9.0626061e-01  8.0337775e-01  4.9388060e-01\n",
      "  -1.5661801e+00  9.0771191e-06  9.0626061e-01 -9.0626061e-01]]\n",
      "Step 0: BUY at 0.3386\n",
      "Step 1: BUY at 0.3265\n",
      "Step 2: BUY at 0.3288\n",
      "Step 3: BUY at 0.3123\n",
      "Episode 3 completed with reward: -0.50\n",
      "  Buy signals: 4\n",
      "  Sell signals: 0\n",
      "\n",
      "=== Test Results ===\n",
      "Episodes: 3\n",
      "Mean reward: -0.50 ± 0.00\n",
      "Episode 1: 4 buy signals, 0 sell signals\n",
      "Episode 2: 4 buy signals, 0 sell signals\n",
      "Episode 3: 4 buy signals, 0 sell signals\n",
      "\n",
      "First episode buy signals: [Timestamp('2025-02-07 04:00:00'), Timestamp('2025-02-07 04:01:00'), Timestamp('2025-02-07 04:02:00'), Timestamp('2025-02-07 04:03:00')]\n",
      "First episode sell signals: []\n"
     ]
    }
   ],
   "source": [
    "def test_trading_model(model, vec_norm_path, num_episodes=3, render=False):\n",
    "    \"\"\"Test the trained trading model with a single DummyVecEnv and return signals.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PPO model\n",
    "        vec_norm_path: Path to saved VecNormalize stats (optional)\n",
    "        num_episodes: Number of episodes to run\n",
    "        render: Whether to print debug info\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (all_episodes_results, first_episode_signals)\n",
    "            all_episodes_results: dict containing results from all episodes\n",
    "            first_episode_signals: dict with 'buy_signals' and 'sell_signals' from first episode\n",
    "    \"\"\"\n",
    "    # Create a single test environment\n",
    "    def make_test_env():\n",
    "        def _init():\n",
    "            env = TradingEnv(df)\n",
    "            return ActionMasker(env, mask_fn)\n",
    "        return _init\n",
    "\n",
    "    # Initialize results\n",
    "    all_episodes_results = {\n",
    "        'episode_rewards': [],\n",
    "        'all_buy_signals': [],\n",
    "        'all_sell_signals': [],\n",
    "        'actions': [],\n",
    "        'timesteps': []\n",
    "    }\n",
    "    \n",
    "    # Store first episode signals separately\n",
    "    first_episode_signals = {\n",
    "        'buy_signals': [],\n",
    "        'sell_signals': []\n",
    "    }\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.policy.eval()\n",
    "    \n",
    "    # Create environment\n",
    "    env = DummyVecEnv([make_test_env()])\n",
    "\n",
    "    # Load saved normalization stats instead of creating new ones\n",
    "    if vec_norm_path:\n",
    "        env = VecNormalize.load(vec_norm_path, env)\n",
    "        env.training = False  # Don't update stats during testing\n",
    "        env.norm_reward = False  # Don't normalize rewards during testing\n",
    "    else:\n",
    "        # If no saved stats, create new normalization (not recommended for testing)\n",
    "        env = CustomVecNormalize(\n",
    "            env,\n",
    "            norm_obs=True,\n",
    "            norm_reward=True,\n",
    "            clip_obs=10.0,\n",
    "            clip_reward=10.0,\n",
    "            gamma=0.99,\n",
    "            training=False\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            step = 0\n",
    "            \n",
    "            # Lists to store signals for current episode\n",
    "            episode_buy_signals = []\n",
    "            episode_sell_signals = []\n",
    "            \n",
    "            if render:\n",
    "                print(f\"\\n=== Episode {episode + 1} ===\")\n",
    "                print(f\"Initial observation: {obs}\")\n",
    "            \n",
    "            while not done and step < len(df) - 1:\n",
    "                try:\n",
    "                    # Get action mask from the environment\n",
    "                    action_mask = env.envs[0].env.get_action_mask()\n",
    "                    \n",
    "                    # Get action from model\n",
    "                    with torch.no_grad():\n",
    "                        obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=model.device)\n",
    "                        dist = model.policy.get_distribution(obs_tensor)\n",
    "                        masked_logits = dist.distribution.logits + torch.log(\n",
    "                            torch.as_tensor(action_mask, dtype=torch.float32, device=model.device) + 1e-8\n",
    "                        )\n",
    "                        action = torch.argmax(masked_logits).item()\n",
    "                    \n",
    "                    # Step environment\n",
    "                    new_obs, reward, done, info = env.step([action])\n",
    "                    \n",
    "                    # Track signals\n",
    "                    current_step = info[0].get('current_step', step)\n",
    "                    current_time = df.index[current_step]\n",
    "                    \n",
    "                    if action == 1:  # BUY\n",
    "                        episode_buy_signals.append(current_time)\n",
    "                        if render:\n",
    "                            print(f\"Step {step}: BUY at {df['close'].iloc[current_step]:.4f}\")\n",
    "                    elif action == 2:  # SELL\n",
    "                        episode_sell_signals.append(current_time)\n",
    "                        if render:\n",
    "                            print(f\"Step {step}: SELL at {df['close'].iloc[current_step]:.4f}\")\n",
    "                    \n",
    "                    # Store results\n",
    "                    all_episodes_results['actions'].append(action)\n",
    "                    all_episodes_results['timesteps'].append(current_step)\n",
    "                    episode_reward += reward[0] if isinstance(reward, np.ndarray) else reward\n",
    "                    obs = new_obs\n",
    "                    step += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error at step {step}: {str(e)}\")\n",
    "                    done = True\n",
    "                    break\n",
    "            \n",
    "            # Store signals for this episode\n",
    "            all_episodes_results['all_buy_signals'].append(episode_buy_signals)\n",
    "            all_episodes_results['all_sell_signals'].append(episode_sell_signals)\n",
    "            all_episodes_results['episode_rewards'].append(episode_reward)\n",
    "            \n",
    "            # Store first episode signals separately\n",
    "            if episode == 0:\n",
    "                first_episode_signals['buy_signals'] = episode_buy_signals\n",
    "                first_episode_signals['sell_signals'] = episode_sell_signals\n",
    "            \n",
    "            if render:\n",
    "                print(f\"Episode {episode + 1} completed with reward: {episode_reward:.2f}\")\n",
    "                print(f\"  Buy signals: {len(episode_buy_signals)}\")\n",
    "                print(f\"  Sell signals: {len(episode_sell_signals)}\")\n",
    "            \n",
    "            # Reset for next episode\n",
    "            env.reset()\n",
    "    \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        env.close()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== Test Results ===\")\n",
    "    print(f\"Episodes: {num_episodes}\")\n",
    "    if all_episodes_results['episode_rewards']:\n",
    "        print(f\"Mean reward: {np.mean(all_episodes_results['episode_rewards']):.2f} ± {np.std(all_episodes_results['episode_rewards']):.2f}\")\n",
    "    \n",
    "    # Print signal counts for all episodes\n",
    "    for i in range(num_episodes):\n",
    "        print(f\"Episode {i+1}: {len(all_episodes_results['all_buy_signals'][i])} buy signals, {len(all_episodes_results['all_sell_signals'][i])} sell signals\")\n",
    "    \n",
    "    return first_episode_signals\n",
    "\n",
    "# Then test the model\n",
    "# Test the model and get results\n",
    "first_episode_signals = test_trading_model(\n",
    "    PPO.load(\"sb3_ppo_model.zip\"),\n",
    "    vec_norm_path=\"vecnormalize.pkl\",\n",
    "    num_episodes=3, \n",
    "    render=True\n",
    ")\n",
    "\n",
    "# Access the signals from the first episode\n",
    "buy_signals = first_episode_signals['buy_signals']\n",
    "sell_signals = first_episode_signals['sell_signals']\n",
    "\n",
    "print(\"\\nFirst episode buy signals:\", buy_signals)\n",
    "print(\"First episode sell signals:\", sell_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3df3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_env_structure(env, indent=0):\n",
    "    \"\"\"Recursively print the environment wrapper structure.\"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    print(f\"{prefix}└─ {type(env).__name__}\")\n",
    "    \n",
    "    # Special handling for common wrapper types\n",
    "    if hasattr(env, 'env'):\n",
    "        print_env_structure(env.env, indent + 1)\n",
    "    elif hasattr(env, 'envs') and hasattr(env.envs, '__getitem__'):\n",
    "        print(f\"{prefix}  └─ [0]\")  # First sub-environment\n",
    "        print_env_structure(env.envs[0], indent + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e1202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(model, env, buy_signals, sell_signals, num_episodes=1):\n",
    "    \"\"\"Test function that handles action masking and tracks rewards.\"\"\"\n",
    "    print(\"Environment structure:\")\n",
    "    print_env_structure(env)\n",
    "    \n",
    "    # Helper to safely get base environment\n",
    "    def get_base_env(env):\n",
    "        current = env\n",
    "        while True:\n",
    "            if hasattr(current, 'envs') and hasattr(current.envs, '__getitem__'):\n",
    "                current = current.envs[0]\n",
    "            elif hasattr(current, 'env'):\n",
    "                current = current.env\n",
    "            else:\n",
    "                return current\n",
    "    \n",
    "    # Get the base environment\n",
    "    base_env = get_base_env(env)\n",
    "    print(f\"\\nBase environment: {type(base_env).__name__}\")\n",
    "    \n",
    "    # Check if environment is wrapped with VecNormalize\n",
    "    vec_norm = None\n",
    "    current = env\n",
    "    while hasattr(current, 'env'):\n",
    "        if hasattr(current, 'obs_rms'):\n",
    "            vec_norm = current\n",
    "            break\n",
    "        current = current.env\n",
    "    \n",
    "    # Track rewards across all episodes\n",
    "    all_episode_rewards = []\n",
    "    all_episode_actions = []\n",
    "    all_episode_prices = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        current_step = 0\n",
    "        episode_buys = []\n",
    "        episode_sells = []\n",
    "        episode_rewards = []\n",
    "        episode_actions = []\n",
    "        episode_prices = []\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"=== Episode {episode + 1} ===\")\n",
    "        print(f\"{'Step':<6} {'Action':<6} {'Price':<10} {'Reward':<10} {'Cumulative':<10}\")\n",
    "        print(\"-\"*45)\n",
    "        \n",
    "        cumulative_reward = 0\n",
    "        \n",
    "        while not done and current_step < len(df):\n",
    "            try:\n",
    "                # Get action mask from the base environment\n",
    "                action_mask = np.ones(3, dtype=np.float32)\n",
    "                if hasattr(base_env, 'get_action_mask'):\n",
    "                    action_mask = base_env.get_action_mask()\n",
    "                \n",
    "                # Get action using the model's policy\n",
    "                with torch.no_grad():\n",
    "                    # Normalize observation if VecNormalize is used\n",
    "                    if vec_norm is not None:\n",
    "                        obs = vec_norm.normalize_obs(obs)\n",
    "                    obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=model.device)\n",
    "                    dist = model.policy.get_distribution(obs_tensor)\n",
    "                    masked_logits = dist.distribution.logits + torch.log(\n",
    "                        torch.as_tensor(action_mask, dtype=torch.float32, device=model.device) + 1e-8\n",
    "                    )\n",
    "                    action = torch.argmax(masked_logits).item()\n",
    "                \n",
    "                # Step environment\n",
    "                new_obs, reward, done, info = env.step([action])\n",
    "                if isinstance(reward, np.ndarray):\n",
    "                    reward = reward.item()\n",
    "                \n",
    "                # Denormalize reward if VecNormalize is used\n",
    "                if vec_norm is not None:\n",
    "                    reward = vec_norm.unnormalize_reward(reward)\n",
    "                \n",
    "                # Track episode data\n",
    "                price = df['close'].iloc[current_step]\n",
    "                cumulative_reward += reward\n",
    "                episode_rewards.append(reward)\n",
    "                episode_actions.append(action)\n",
    "                episode_prices.append(price)\n",
    "                \n",
    "                # Print step info\n",
    "                action_str = \"BUY\" if action == 1 else \"SELL\" if action == 2 else \"HOLD\"\n",
    "                print(f\"{current_step:<6} {action_str:<6} {price:<10.4f} {reward:<10.4f} {cumulative_reward:<10.4f}\")\n",
    "                \n",
    "                # Track signals\n",
    "                if action == 1:\n",
    "                    episode_buys.append(df.index[current_step])\n",
    "                elif action == 2:\n",
    "                    episode_sells.append(df.index[current_step])\n",
    "                \n",
    "                obs = new_obs\n",
    "                current_step += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error at step {current_step}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                break\n",
    "        \n",
    "        # Store episode data\n",
    "        all_episode_rewards.append(episode_rewards)\n",
    "        all_episode_actions.append(episode_actions)\n",
    "        all_episode_prices.append(episode_prices)\n",
    "        \n",
    "        # Print episode summary\n",
    "        total_reward = sum(episode_rewards)\n",
    "        print(f\"\\nEpisode {episode + 1} Summary:\")\n",
    "        print(f\"  Total reward: {total_reward:.4f}\")\n",
    "        if episode_rewards:\n",
    "            print(f\"  Average reward/step: {total_reward/len(episode_rewards):.4f}\")\n",
    "        print(f\"  Buy signals: {len(episode_buys)}\")\n",
    "        print(f\"  Sell signals: {len(episode_sells)}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Add to global signals\n",
    "        buy_signals.extend(episode_buys)\n",
    "        sell_signals.extend(episode_sells)\n",
    "    \n",
    "    return all_episode_rewards, all_episode_actions, all_episode_pricesdef test_agent(model, env, buy_signals, sell_signals, num_episodes=1):\n",
    "    \"\"\"Test function that handles action masking and tracks rewards.\"\"\"\n",
    "    print(\"Environment structure:\")\n",
    "    print_env_structure(env)\n",
    "    \n",
    "    # Helper to safely get base environment\n",
    "    def get_base_env(env):\n",
    "        current = env\n",
    "        while True:\n",
    "            if hasattr(current, 'envs') and hasattr(current.envs, '__getitem__'):\n",
    "                current = current.envs[0]\n",
    "            elif hasattr(current, 'env'):\n",
    "                current = current.env\n",
    "            else:\n",
    "                return current\n",
    "    \n",
    "    # Get the base environment\n",
    "    base_env = get_base_env(env)\n",
    "    print(f\"\\nBase environment: {type(base_env).__name__}\")\n",
    "    \n",
    "    # Check if environment is wrapped with VecNormalize\n",
    "    vec_norm = None\n",
    "    current = env\n",
    "    while hasattr(current, 'env'):\n",
    "        if hasattr(current, 'obs_rms'):\n",
    "            vec_norm = current\n",
    "            break\n",
    "        current = current.env\n",
    "    \n",
    "    # Track rewards across all episodes\n",
    "    all_episode_rewards = []\n",
    "    all_episode_actions = []\n",
    "    all_episode_prices = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        current_step = 0\n",
    "        episode_buys = []\n",
    "        episode_sells = []\n",
    "        episode_rewards = []\n",
    "        episode_actions = []\n",
    "        episode_prices = []\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"=== Episode {episode + 1} ===\")\n",
    "        print(f\"{'Step':<6} {'Action':<6} {'Price':<10} {'Reward':<10} {'Cumulative':<10}\")\n",
    "        print(\"-\"*45)\n",
    "        \n",
    "        cumulative_reward = 0\n",
    "        \n",
    "        while not done and current_step < len(df):\n",
    "            try:\n",
    "                # Get action mask from the base environment\n",
    "                action_mask = np.ones(3, dtype=np.float32)\n",
    "                if hasattr(base_env, 'get_action_mask'):\n",
    "                    action_mask = base_env.get_action_mask()\n",
    "                \n",
    "                # Get action using the model's policy\n",
    "                with torch.no_grad():\n",
    "                    # Normalize observation if VecNormalize is used\n",
    "                    if vec_norm is not None:\n",
    "                        obs = vec_norm.normalize_obs(obs)\n",
    "                    obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=model.device)\n",
    "                    dist = model.policy.get_distribution(obs_tensor)\n",
    "                    masked_logits = dist.distribution.logits + torch.log(\n",
    "                        torch.as_tensor(action_mask, dtype=torch.float32, device=model.device) + 1e-8\n",
    "                    )\n",
    "                    action = torch.argmax(masked_logits).item()\n",
    "                \n",
    "                # Step environment\n",
    "                new_obs, reward, done, info = env.step([action])\n",
    "                if isinstance(reward, np.ndarray):\n",
    "                    reward = reward.item()\n",
    "                \n",
    "                # Denormalize reward if VecNormalize is used\n",
    "                if vec_norm is not None:\n",
    "                    reward = vec_norm.unnormalize_reward(reward)\n",
    "                \n",
    "                # Track episode data\n",
    "                price = df['close'].iloc[current_step]\n",
    "                cumulative_reward += reward\n",
    "                episode_rewards.append(reward)\n",
    "                episode_actions.append(action)\n",
    "                episode_prices.append(price)\n",
    "                \n",
    "                # Print step info\n",
    "                action_str = \"BUY\" if action == 1 else \"SELL\" if action == 2 else \"HOLD\"\n",
    "                print(f\"{current_step:<6} {action_str:<6} {price:<10.4f} {reward:<10.4f} {cumulative_reward:<10.4f}\")\n",
    "                \n",
    "                # Track signals\n",
    "                if action == 1:\n",
    "                    episode_buys.append(df.index[current_step])\n",
    "                elif action == 2:\n",
    "                    episode_sells.append(df.index[current_step])\n",
    "                \n",
    "                obs = new_obs\n",
    "                current_step += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error at step {current_step}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                break\n",
    "        \n",
    "        # Store episode data\n",
    "        all_episode_rewards.append(episode_rewards)\n",
    "        all_episode_actions.append(episode_actions)\n",
    "        all_episode_prices.append(episode_prices)\n",
    "        \n",
    "        # Print episode summary\n",
    "        total_reward = sum(episode_rewards)\n",
    "        print(f\"\\nEpisode {episode + 1} Summary:\")\n",
    "        print(f\"  Total reward: {total_reward:.4f}\")\n",
    "        if episode_rewards:\n",
    "            print(f\"  Average reward/step: {total_reward/len(episode_rewards):.4f}\")\n",
    "        print(f\"  Buy signals: {len(episode_buys)}\")\n",
    "        print(f\"  Sell signals: {len(episode_sells)}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Add to global signals\n",
    "        buy_signals.extend(episode_buys)\n",
    "        sell_signals.extend(episode_sells)\n",
    "    \n",
    "    return all_episode_rewards, all_episode_actions, all_episode_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a58f4e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording signals\n",
      "Environment structure:\n",
      "└─ VecNormalize\n",
      "  └─ [0]\n",
      "    └─ ActionMasker\n",
      "      └─ TradingEnv\n",
      "\n",
      "Base environment: TradingEnv\n",
      "\n",
      "==================================================\n",
      "=== Episode 1 ===\n",
      "Step   Action Price      Reward     Cumulative\n",
      "---------------------------------------------\n",
      "0      HOLD   0.3386     0.0000     0.0000    \n",
      "1      HOLD   0.3265     0.0000     0.0000    \n",
      "2      HOLD   0.3288     0.0000     0.0000    \n",
      "3      HOLD   0.3123     0.0000     0.0000    \n",
      "4      HOLD   0.3124     0.0000     0.0000    \n",
      "5      HOLD   0.3011     0.0000     0.0000    \n",
      "6      HOLD   0.3069     0.0000     0.0000    \n",
      "7      HOLD   0.3115     0.0000     0.0000    \n",
      "8      HOLD   0.3137     0.0000     0.0000    \n",
      "9      HOLD   0.3046     0.0000     0.0000    \n",
      "\n",
      "Episode 1 Summary:\n",
      "  Total reward: 0.0000\n",
      "  Average reward/step: 0.0000\n",
      "  Buy signals: 0\n",
      "  Sell signals: 0\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [[np.float64(0.3386),\n",
       "   np.float64(0.3265),\n",
       "   np.float64(0.3288),\n",
       "   np.float64(0.3123),\n",
       "   np.float64(0.3124),\n",
       "   np.float64(0.3011),\n",
       "   np.float64(0.3069),\n",
       "   np.float64(0.3115),\n",
       "   np.float64(0.3137),\n",
       "   np.float64(0.3046)]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output results from one model\n",
    "\n",
    "# Initialize lists to store buy and sell signals\n",
    "\n",
    "def create_test_env(df):\n",
    "\n",
    "    # Create base environment\n",
    "    env = TradingEnv(df)\n",
    "    \n",
    "    # Apply action masking\n",
    "    env = ActionMasker(env, mask_fn)\n",
    "    \n",
    "    # Convert to vectorized environment (even though it's just 1 env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    env = VecNormalize(\n",
    "        env,\n",
    "        norm_obs=True,\n",
    "        norm_reward=True,\n",
    "        clip_obs=10.0,\n",
    "        clip_reward=10.0,\n",
    "        gamma=0.99,\n",
    "        training=False  # Set to False for testing\n",
    "    )\n",
    "    \n",
    "    return env\n",
    "\n",
    "# Example usage:\n",
    "test_env = create_test_env(df)\n",
    "\n",
    "\n",
    "buy_signals = []\n",
    "sell_signals = []\n",
    "\n",
    "print(\"Recording signals\")\n",
    "test_agent(ppo_masked_model, test_env, buy_signals, sell_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4ef8256b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "spaces must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create test environment\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m test_env \u001b[38;5;241m=\u001b[39m \u001b[43mmake_test_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m     18\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m evaluate_policy(\n\u001b[1;32m     19\u001b[0m     ppo_masked_model,\n\u001b[1;32m     20\u001b[0m     test_env,\n\u001b[1;32m     21\u001b[0m     n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     22\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     23\u001b[0m )\n",
      "Cell \u001b[0;32mIn[172], line 9\u001b[0m, in \u001b[0;36mmake_test_env\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m env \u001b[38;5;241m=\u001b[39m ActionMasker(env, mask_fn)  \u001b[38;5;66;03m# Apply action masking\u001b[39;00m\n\u001b[1;32m      8\u001b[0m env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: env])  \u001b[38;5;66;03m# Vectorized environment\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mVecNormalize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvec_normalize.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load saved normalization\u001b[39;00m\n\u001b[1;32m     10\u001b[0m env\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Important: disable training mode\u001b[39;00m\n\u001b[1;32m     11\u001b[0m env\u001b[38;5;241m.\u001b[39mnorm_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Optional: disable reward normalization if you want raw rewards\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_normalize.py:321\u001b[0m, in \u001b[0;36mVecNormalize.load\u001b[0;34m(load_path, venv)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(load_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_handler:\n\u001b[1;32m    320\u001b[0m     vec_normalize \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file_handler)\n\u001b[0;32m--> 321\u001b[0m \u001b[43mvec_normalize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_venv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvenv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vec_normalize\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_normalize.py:171\u001b[0m, in \u001b[0;36mVecNormalize.set_venv\u001b[0;34m(self, venv)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m=\u001b[39m venv\u001b[38;5;241m.\u001b[39mrender_mode\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Check that the observation_space shape match\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_shape_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturns \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs)\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/utils.py:252\u001b[0m, in \u001b[0;36mcheck_shape_equal\u001b[0;34m(space1, space2)\u001b[0m\n\u001b[1;32m    250\u001b[0m         check_shape_equal(space1\u001b[38;5;241m.\u001b[39mspaces[key], space2\u001b[38;5;241m.\u001b[39mspaces[key])\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(space1, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m space1\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m space2\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspaces must have the same shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: spaces must have the same shape"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Create a test environment with the same wrappers as training\n",
    "def make_test_env():\n",
    "    env = TradingEnv(df)  # Your custom environment\n",
    "    env = ActionMasker(env, mask_fn)  # Apply action masking\n",
    "    env = DummyVecEnv([lambda: env])  # Vectorized environment\n",
    "    env = VecNormalize.load(\"vec_normalize.pkl\", env)  # Load saved normalization\n",
    "    env.training = False  # Important: disable training mode\n",
    "    env.norm_reward = True  # Optional: disable reward normalization if you want raw rewards\n",
    "    return env\n",
    "\n",
    "# Create test environment\n",
    "test_env = make_test_env()\n",
    "\n",
    "# Evaluate\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    ppo_masked_model,\n",
    "    test_env,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"Mean reward over 10 episodes: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6cbf45-6811-4140-9ac2-805d2010a88a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Ensure directories exist\n",
    "import os\n",
    "import joblib  # or import pickle\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "os.makedirs(\"./tensorboard_logs/\", exist_ok=True)\n",
    "os.makedirs(\"./saved_models/\", exist_ok=True)\n",
    "\n",
    "def evaluate_model(model, env, num_episodes=10):\n",
    "    total_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        # Access the environment wrapped by DummyVecEnv and ActionMasker\n",
    "        env_inner = env.get_attr(\"env\", 0)[0]  # Get the first environment\n",
    "        \n",
    "        # Check if env_inner has an 'env' attribute (i.e., it's wrapped)\n",
    "        if hasattr(env_inner, \"env\"):  \n",
    "            env_inner = env_inner.env  # Unwrap ActionMasker if applicable\n",
    "\n",
    "        obs, info = env_inner.reset()  # Reset the environment\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            for i in range(30):  # Run for a fixed number of steps\n",
    "                # Extract the action mask from the original environment\n",
    "                action_mask = env_inner.get_action_mask()  # Access get_action_mask from TradingEnv inside ActionMasker\n",
    "    \n",
    "                # Convert observation and mask to tensors\n",
    "                obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=model.device).unsqueeze(0)\n",
    "                action_mask_tensor = torch.as_tensor(action_mask, dtype=torch.float32, device=model.device).unsqueeze(0)\n",
    "    \n",
    "                # Predict action with action masking\n",
    "                with torch.no_grad():\n",
    "                    action, _, _ = model.policy.forward(obs_tensor, action_mask=action_mask_tensor, deterministic=True)\n",
    "    \n",
    "                action = action.cpu().numpy()[0]  # Convert action tensor to numpy\n",
    "    \n",
    "                # Step the environment\n",
    "                obs, reward, done, truncated, info = env_inner.step(action)\n",
    "    \n",
    "                episode_reward += reward\n",
    "\n",
    "                if done or truncated:  # If the episode ends, reset the environment\n",
    "                    print(f\"🎯 Total Reward: {episode_reward:.2f}\")\n",
    "                    obs = env_inner.reset()  # Reset environment, which only returns the observation\n",
    "                    break  # Exit loop if episode ends\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "ent_coef_range = [0.0]       # Lower values for stability\n",
    "gamma_range = [0.99]              # Lower values for short-term focus\n",
    "gae_lambda_range = [0.9]        # Wider range to test variance\n",
    "n_steps_range = [4096]           # Larger values for more experiences\n",
    "clip_range_range = [0.1]         # Narrower range for stability\n",
    "n_epochs_range = [5]               # Fewer epochs for faster training\n",
    "batch_size_range = [128]           # Larger batches for better performance\n",
    "\n",
    "force = True\n",
    "\n",
    "timesteps = 500000\n",
    "\n",
    "for ent_coef, gamma, gae_lambda, n_steps, clip_range, n_epochs, batch_size in product(\n",
    "    ent_coef_range, gamma_range, gae_lambda_range, n_steps_range, clip_range_range, n_epochs_range, batch_size_range\n",
    "):\n",
    "    model_filename = f\"./saved_models/model_ent_coef={ent_coef}_gamma={gamma}_gae_lambda={gae_lambda}_n_steps={n_steps}_clip_range={clip_range}_n_epochs={n_epochs}_batch_size={batch_size}_timesteps={timesteps}.zip\"\n",
    "    \n",
    "    if os.path.exists(model_filename) and not force:\n",
    "        print(f\"Model already exists: {model_filename}, loading instead of training...\")\n",
    "        model = PPO.load(model_filename, env=env)  # Load existing model\n",
    "    else:\n",
    "        print(f\"Training new model with: ent_coef={ent_coef}, gamma={gamma}, gae_lambda={gae_lambda}, \"\n",
    "              f\"n_steps={n_steps}, clip_range={clip_range}, n_epochs={n_epochs}, batch_size={batch_size}, timesteps={timesteps}\")\n",
    "    \n",
    "        model = PPO(\n",
    "            MaskedPPOPolicy,  # Replace with your policy (e.g., \"MlpPolicy\" or \"CnnPolicy\")\n",
    "            env,\n",
    "            ent_coef=ent_coef,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            n_steps=n_steps,\n",
    "            clip_range=clip_range,\n",
    "            n_epochs=n_epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,  # Set to 0 for less output\n",
    "            tensorboard_log=\"./tensorboard_logs/\"\n",
    "        )\n",
    "        \n",
    "        # Train for x timesteps\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "    \n",
    "        # Save the model\n",
    "        model.save(model_filename)\n",
    "        print(f\"Model saved: {model_filename}\")\n",
    "    \n",
    "        # Store the model and its parameters\n",
    "        models.append({\n",
    "            \"model_filename\": model_filename,  # Store the path to the saved model\n",
    "            \"params\": {\n",
    "                \"ent_coef\": ent_coef,\n",
    "                \"gamma\": gamma,\n",
    "                \"gae_lambda\": gae_lambda,\n",
    "                \"n_steps\": n_steps,\n",
    "                \"clip_range\": clip_range,\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Evaluate the model\n",
    "    total_reward = evaluate_model(model, env)\n",
    "    print('Average total reward:', total_reward)\n",
    "\n",
    "    env.close()  # Close all subprocesses\n",
    "    \n",
    "    results.append({\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"clip_range\": clip_range,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_reward\": total_reward,\n",
    "        \"model_filename\": model_filename  # Link results to the saved model\n",
    "    })\n",
    "\n",
    "# Save results to a CSV file\n",
    "results_df = pd.DataFrame(results)\n",
    "csv_filename = f\"grid_search_results_{timesteps}_steps.csv\"\n",
    "results_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "base_csv, csv_ext = os.path.splitext(csv_filename)\n",
    "\n",
    "# Append a counter if the file already exists\n",
    "counter = 1\n",
    "while os.path.exists(csv_filename):\n",
    "    csv_filename = f\"{base_csv}_{counter}{csv_ext}\"\n",
    "    counter += 1\n",
    "\n",
    "# Save the results DataFrame to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Results saved to {csv_filename}\")\n",
    "\n",
    "# Now, do the same for the models file\n",
    "models_filename = \"./saved_models/models_list.joblib\"\n",
    "base_model, model_ext = os.path.splitext(models_filename)\n",
    "\n",
    "counter = 1\n",
    "while os.path.exists(models_filename):\n",
    "    models_filename = f\"{base_model}_{counter}{model_ext}\"\n",
    "    counter += 1\n",
    "\n",
    "# Save the models variable\n",
    "joblib.dump(models, models_filename)\n",
    "print(f\"Models saved to {models_filename}\")\n",
    "\n",
    "print(\"Grid search completed. Results saved to\", csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2050ff4-c41c-4847-b210-7b76c430d2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2025-02-07 04:01:00'), Timestamp('2025-02-07 04:04:00')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buy_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f0259a3-365c-4ab6-8960-72a9c66c3959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2025-02-07 04:02:00'), Timestamp('2025-02-07 04:08:00')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sell_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a4db54d-abf3-4ea8-a13f-bff4d876a86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show learned buy and sell signals\n",
    "\n",
    "import mplfinance as mpf\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame, buy_signals_1000, sell_signals_1000, buy_signals_10000, and sell_signals_10000 are your lists of timestamps\n",
    "\n",
    "# Function to plot the signals for a given buy and sell signal list\n",
    "def plot_signals(buy_signals, sell_signals, title):\n",
    "    # Convert buy/sell signals to DatetimeIndex\n",
    "    buy_signals = pd.to_datetime(buy_signals)  \n",
    "    buy_signals = buy_signals[buy_signals.isin(df_original.index)]  # Filter buy signals to match df.index\n",
    "\n",
    "    sell_signals = pd.to_datetime(sell_signals)  \n",
    "    sell_signals = sell_signals[sell_signals.isin(df_original.index)]  # Filter sell signals to match df.index\n",
    "\n",
    "    # Create new columns to mark the buy and sell signals with NaN for non-signals\n",
    "    buy_signal_prices = df_original['close'].copy()\n",
    "    buy_signal_prices[~df_original.index.isin(buy_signals)] = float('nan')  # Set non-buy signals to NaN\n",
    "    \n",
    "    sell_signal_prices = df_original['close'].copy()\n",
    "    sell_signal_prices[~df_original.index.isin(sell_signals)] = float('nan')  # Set non-sell signals to NaN\n",
    "\n",
    "    # Create addplots for buy and sell signals\n",
    "    buy_signal_plot = mpf.make_addplot(buy_signal_prices, type='scatter', markersize=40, marker='^', color='green')\n",
    "    sell_signal_plot = mpf.make_addplot(sell_signal_prices, type='scatter', markersize=40, marker='v', color='red')\n",
    "\n",
    "    # Plot with the added buy and sell signals\n",
    "    fig, axes = mpf.plot(\n",
    "        df_original,\n",
    "        type='ohlc',\n",
    "        datetime_format='%Y-%m-%d %H:%M',\n",
    "        addplot=[buy_signal_plot, sell_signal_plot],\n",
    "        returnfig=True,\n",
    "        figsize=(16, 8),\n",
    "        warn_too_much_data=10000,\n",
    "        title=title #Adjust the size again to maintain consistency\n",
    "    )\n",
    "\n",
    "plot_signals(buy_signals, sell_signals, title=\"Buy/Sell Signals\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728cfc9-e606-41b0-92a8-f4d05d577650",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate profit based on buy and sell signals\n",
    "def calculate_profit(buy_signals, sell_signals, df):\n",
    "    buy_prices = df.loc[buy_signals, 'close']\n",
    "    sell_prices = df.loc[sell_signals, 'close']\n",
    "\n",
    "    # Ensure that the number of buy signals is less than or equal to the number of sell signals\n",
    "    min_length = min(len(buy_prices), len(sell_prices))\n",
    "\n",
    "    # Calculate profit for each pair of buy and sell signals\n",
    "    profits = []\n",
    "    for i in range(min_length):\n",
    "        profit = sell_prices.iloc[i] - buy_prices.iloc[i]  # Profit = Sell Price - Buy Price\n",
    "        profits.append(profit)\n",
    "\n",
    "    total_profit = sum(profits)\n",
    "    return total_profit, profits\n",
    "\n",
    "# Function to plot the signals and compare profits\n",
    "def plot_signals_and_compare_profit(buy_signals, sell_signals, df, title):\n",
    "    # Convert buy/sell signals to DatetimeIndex\n",
    "    buy_signals = pd.to_datetime(buy_signals)  \n",
    "    buy_signals = buy_signals[buy_signals.isin(df.index)]  # Filter buy signals to match df.index\n",
    "\n",
    "    sell_signals = pd.to_datetime(sell_signals)  \n",
    "    sell_signals = sell_signals[sell_signals.isin(df.index)]  # Filter sell signals to match df.index\n",
    "\n",
    "    # Create new columns to mark the buy and sell signals with NaN for non-signals\n",
    "    buy_signal_prices = df['close'].copy()\n",
    "    buy_signal_prices[~df.index.isin(buy_signals)] = float('nan')  # Set non-buy signals to NaN\n",
    "\n",
    "    sell_signal_prices = df['close'].copy()\n",
    "    sell_signal_prices[~df.index.isin(sell_signals)] = float('nan')  # Set non-sell signals to NaN\n",
    "\n",
    "    # Create addplots for buy and sell signals\n",
    "    buy_signal_plot = mpf.make_addplot(buy_signal_prices, type='scatter', markersize=40, marker='^', color='green')\n",
    "    sell_signal_plot = mpf.make_addplot(sell_signal_prices, type='scatter', markersize=40, marker='v', color='red')\n",
    "\n",
    "    # Plot with the added buy and sell signals\n",
    "    fig, axes = mpf.plot(\n",
    "        df,\n",
    "        type='ohlc',\n",
    "        style='charles',\n",
    "        datetime_format='%Y-%m-%d %H:%M',\n",
    "        addplot=[buy_signal_plot, sell_signal_plot],\n",
    "        returnfig=True,\n",
    "        figsize=(20, 10),\n",
    "        warn_too_much_data=1000  # Adjust the size again to maintain consistency\n",
    "    )\n",
    "\n",
    "    # Customize x-axis\n",
    "    #axes[0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    #plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "    # Add legend manually\n",
    "    #plt.legend(['Buy Signals', 'Sell Signals'], loc='upper left')\n",
    "\n",
    "    # Add title\n",
    "    #plt.title(title)\n",
    "\n",
    "    # Show the plot\n",
    "    #plt.show()\n",
    "\n",
    "    # Calculate and print profit for the signals\n",
    "    total_profit, profits = calculate_profit(buy_signals, sell_signals, df)\n",
    "    print(f\"Total Profit for {title}: {total_profit:.2f}\")\n",
    "    return total_profit\n",
    "\n",
    "# Calculate and plot for 1000 timesteps signals\n",
    "profit_1000 = plot_signals_and_compare_profit(buy_signals_1000, sell_signals_1000, df, title=\"Profit for 1000 Timesteps\")\n",
    "\n",
    "# Calculate and plot for 10000 timesteps signals\n",
    "profit_10000 = plot_signals_and_compare_profit(buy_signals_10000, sell_signals_10000, df, title=\"Profit for 10000 Timesteps\")\n",
    "\n",
    "# Calculate and plot for 100000 timesteps signals\n",
    "#profit_100000 = plot_signals_and_compare_profit(buy_signals_100000, sell_signals_100000, df, title=\"Profit for 100000 Timesteps\")\n",
    "\n",
    "\n",
    "# Compare the total profits\n",
    "print(f\"Total Profit for 1000 Timesteps: {profit_1000:.2f}\")\n",
    "print(f\"Total Profit for 10000 Timesteps: {profit_10000:.2f}\")\n",
    "#print(f\"Total Profit for 100000 Timesteps: {profit_100000:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
