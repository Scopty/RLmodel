{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d4f9613a-043d-4f96-8881-46432c817ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.read_csv(\"/Users/larka/GitHub/RLmodel/MGOL.csv\")  # Replace with actual file\n",
    "df = df.drop(columns=['datetime','symbol', 'frame'])\n",
    "#df['datetime'] = pd.to_datetime(df['datetime'], format='%m/%d/%y %H:%M')\n",
    "#df.set_index('datetime', inplace=True)\n",
    "#df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4dca0adb-67cf-4508-b376-ddef3646a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(Env):\n",
    "    def __init__(self, df):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.current_step = 0\n",
    "        self.balance = 10000  # Starting cash\n",
    "        self.shares = 0\n",
    "        self.buy_price = 0\n",
    "        self.sell_price = 0\n",
    "        self.total_reward = 0\n",
    "        self.position_open = False  # Track open positions\n",
    "        self.round_trip_trades = 0  # Counter for buy-sell cycles\n",
    "        reward = 0\n",
    "\n",
    "        # Action Space: 0 = Hold, 1 = Buy, 2 = Sell\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "        # Observation Space: Open, High, Low, Close, Volume\n",
    "        #self.observation_space = Box(\n",
    "        #    low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32\n",
    "        #)\n",
    "        self.observation_space = Box(\n",
    "            low=-np.inf, high=np.inf, shape=(9,), dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        valid_actions = [0]  # Hold is always valid\n",
    "        if not self.position_open:  # Allow Buy only if no position is open\n",
    "            valid_actions.append(1)\n",
    "        if self.position_open:  # Allow Sell only if a position is open\n",
    "            valid_actions.append(2)\n",
    "        return valid_actions\n",
    "\n",
    "    def get_obs(self):\n",
    "        # Include the current position and valid actions in the observation\n",
    "        position = [1] if self.position_open else [0]  # Shape: (1,)\n",
    "        valid_actions = self.get_valid_actions()\n",
    "        action_mask = [1 if i in valid_actions else 0 for i in range(self.action_space.n)]  # Shape: (3,)\n",
    "        \n",
    "        # Combine all features into a single observation array\n",
    "        obs = np.concatenate([\n",
    "            self.df.iloc[self.current_step][[\"open\", \"high\", \"low\", \"close\", \"volume\"]].values,  # Shape: (5,)\n",
    "            position,  # Shape: (1,)\n",
    "            action_mask  # Shape: (3,)\n",
    "        ])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.balance = 10000\n",
    "        self.shares = 0\n",
    "        self.position_open = False\n",
    "        self.round_trip_trades = 0\n",
    "        self.buy_price = 0\n",
    "        self.sell_price = 0\n",
    "        self.net_profit = 0\n",
    "        reward = 0\n",
    "        obs = self.get_obs()  # Use the updated observation method\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_step >= len(self.df) - 1:\n",
    "            return np.zeros(self.observation_space.shape), 0, True, False, {}\n",
    "    \n",
    "        current_price = self.df.iloc[self.current_step][\"close\"]\n",
    "        reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "    \n",
    "        valid_actions = self.get_valid_actions()\n",
    "        if action not in valid_actions:\n",
    "            #print('convert action',action,'to hold.')\n",
    "            action = 0  # Force to hold if invalid action\n",
    "            reward -= 0.1  # Small penalty for holding\n",
    "        else:\n",
    "            if action == 1:  # Buy\n",
    "                self.shares = 1\n",
    "                self.balance -= current_price\n",
    "                self.position_open = True\n",
    "                self.buy_price = current_price  # Store the buy price\n",
    "                reward += 10#.001  # Small penalty to avoid random buying\n",
    "    \n",
    "            elif action == 2:  # Sell\n",
    "                if self.position_open:  # Ensure position is open before selling\n",
    "                    self.shares = 0\n",
    "                    self.balance += current_price\n",
    "                    self.sell_price = current_price\n",
    "                    reward = abs(current_price - self.buy_price) * 1000  # Reward profit\n",
    "                    self.net_profit = self.sell_price - self.buy_price\n",
    "                    self.buy_price = 0\n",
    "                    self.position_open = False\n",
    "                    self.round_trip_trades += 1  # Only increment after completing a round trip\n",
    "                    #print(f\"Round trip trade completed. Total round trips: {self.round_trip_trades}\")\n",
    "\n",
    "                else:\n",
    "                    reward -= 0.1  # Small penalty for trying to sell without position\n",
    "            else:\n",
    "                reward -= 0.1  # Small penalty for trying to sell without position\n",
    "\n",
    "        # Update the total reward for the episode\n",
    "        self.total_reward += reward\n",
    "    \n",
    "        # Terminate after 10 round-trip trades or end of data\n",
    "        if self.round_trip_trades >= 10:\n",
    "            done = True\n",
    "        if self.current_step >= len(self.df) - 1:\n",
    "            done = True\n",
    "    \n",
    "        self.current_step += 1\n",
    "        obs = self.get_obs()  # Use the updated observation method\n",
    "        info = {\"valid_actions\": valid_actions}  # Include valid actions in info for debugging\n",
    "    \n",
    "        return obs, reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3e54f217-bba0-4cc0-93d6-1e5dec0fa062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = TradingEnv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99edc5b6-5504-4186-b9ee-4f165d5eabb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_policy(env):\n",
    "    # Alternate between buying and selling\n",
    "    if env.position_open:\n",
    "        return 2  # Sell\n",
    "    else:\n",
    "        return 1  # Buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0aa17a16-121c-4d07-a99e-d8b91f19f417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.039s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sell first\n",
      "[3.3860e-01 3.3880e-01 3.2650e-01 3.2650e-01 1.1337e+04 0.0000e+00\n",
      " 1.0000e+00 1.0000e+00 0.0000e+00]\n",
      "0\n",
      "False\n",
      "False\n",
      "{'valid_actions': [0, 1]}\n",
      "Counter: 0\n",
      "Step number: 0\n",
      "Round trip trades: 1\n",
      "Counter: 1\n",
      "Step number: 2\n",
      "Round trip trades: 2\n",
      "Counter: 2\n",
      "Step number: 4\n",
      "Round trip trades: 3\n",
      "Counter: 3\n",
      "Step number: 6\n",
      "Round trip trades: 4\n",
      "Counter: 4\n",
      "Step number: 8\n",
      "Round trip trades: 5\n",
      "Counter: 5\n",
      "Step number: 10\n",
      "Round trip trades: 6\n",
      "Counter: 6\n",
      "Step number: 12\n",
      "Round trip trades: 7\n",
      "Counter: 7\n",
      "Step number: 14\n",
      "Round trip trades: 8\n",
      "Counter: 8\n",
      "Step number: 16\n",
      "Round trip trades: 9\n",
      "Counter: 9\n",
      "Step number: 18\n",
      "Round trip trades: 10\n",
      "test buy, then sell\n",
      "[3.290e-01 3.352e-01 3.240e-01 3.288e-01 8.203e+03 0.000e+00 1.000e+00\n",
      " 1.000e+00 0.000e+00]\n",
      "-12.1\n",
      "False\n",
      "False\n",
      "{'valid_actions': [0, 2]}\n",
      "0\n",
      "0.3265\n",
      "-0.0121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=6 errors=0 failures=0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestTradingEnv(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Create a sample DataFrame\n",
    "        data = {\n",
    "            'open': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121],\n",
    "            'high': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122],\n",
    "            'low': [99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120],\n",
    "            'close': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121],\n",
    "            'volume': [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000]\n",
    "        }\n",
    "        data = df\n",
    "\n",
    "\n",
    "        self.df = pd.DataFrame(data)\n",
    "        self.env = TradingEnv(self.df)\n",
    "\n",
    "    def test_initial_state(self):\n",
    "        obs, _ = self.env.reset()\n",
    "        self.assertEqual(self.env.current_step, 0)\n",
    "        self.assertEqual(self.env.balance, 10000)\n",
    "        self.assertEqual(self.env.shares, 0)\n",
    "        self.assertFalse(self.env.position_open)\n",
    "        self.assertEqual(self.env.round_trip_trades, 0)\n",
    "        self.assertEqual(self.env.buy_price, 0)\n",
    "        self.assertEqual(obs.shape, (9,))  # Check observation shape\n",
    "\n",
    "    def test_buy_action(self):\n",
    "        self.env.reset()\n",
    "        action = 1  # Buy\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        self.assertTrue(self.env.position_open)\n",
    "        self.assertEqual(self.env.shares, 1)\n",
    "        self.assertEqual(self.env.balance, 10000 - self.df.iloc[0][\"close\"])\n",
    "        self.assertEqual(self.env.buy_price, self.df.iloc[0][\"close\"])\n",
    "        self.assertEqual(obs.shape, (9,))  # Check observation shape\n",
    "\n",
    "    def test_sell_action(self):\n",
    "        print('test buy, then sell')\n",
    "        self.env.reset()\n",
    "        self.env.step(1)  # Buy first\n",
    "        action = 2  # Sell\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        print(obs)\n",
    "        print(reward)\n",
    "        print(done)\n",
    "        print(truncated)\n",
    "        print(info)\n",
    "        self.assertFalse(self.env.position_open)\n",
    "        self.assertEqual(self.env.shares, 0)\n",
    "        self.assertEqual(obs.shape, (9,))  # Check observation shape\n",
    "        print(self.env.buy_price)\n",
    "        print(self.env.sell_price)\n",
    "        print(self.env.net_profit)\n",
    "\n",
    "    def test_hold_action(self):\n",
    "        self.env.reset()\n",
    "        action = 0  # Hold\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        self.assertFalse(self.env.position_open)\n",
    "        self.assertEqual(self.env.shares, 0)\n",
    "        self.assertEqual(self.env.balance, 10000)\n",
    "        self.assertEqual(obs.shape, (9,))  # Check observation shape\n",
    "\n",
    "    def test_invalid_action(self):\n",
    "        print('test sell first')\n",
    "        self.env.reset()\n",
    "        action = 2  # Sell (invalid since no position is open)\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        print(obs)\n",
    "        print(reward)\n",
    "        print(done)\n",
    "        print(truncated)\n",
    "        print(info)\n",
    "        self.assertFalse(self.env.position_open)\n",
    "        self.assertEqual(self.env.shares, 0)\n",
    "        self.assertEqual(self.env.balance, 10000)\n",
    "        self.assertEqual(obs.shape, (9,))  # Check observation shape\n",
    "\n",
    "    def test_round_trip_trades(self):\n",
    "        self.env.reset()\n",
    "        for _ in range(10):\n",
    "            print(f\"Counter: {_}\")\n",
    "            print(f\"Step number: {self.env.current_step}\")\n",
    "            \n",
    "            # Step 1: Buy action\n",
    "            obs, reward, done, truncated, info = self.env.step(1)  # Buy\n",
    "            \n",
    "            # Step 2: Sell action\n",
    "            obs, reward, done, truncated, info = self.env.step(2)  # Sell\n",
    "            \n",
    "            print(f\"Round trip trades: {self.env.round_trip_trades}\")\n",
    "        self.assertTrue(self.env.round_trip_trades >= 10)\n",
    "        self.assertTrue(done)\n",
    "\n",
    "# Create a test suite\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestTradingEnv)\n",
    "\n",
    "# Run the tests\n",
    "unittest.TextTestRunner().run(suite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fa9c3c73-188e-47d7-8a84-0355c418df3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -85.79999999999937\n",
      "Episode 2: Total Reward = -85.79999999999937\n",
      "Episode 3: Total Reward = -85.79999999999937\n",
      "Episode 4: Total Reward = -85.79999999999937\n",
      "Episode 5: Total Reward = -85.79999999999937\n",
      "Episode 6: Total Reward = -85.79999999999937\n",
      "Episode 7: Total Reward = -85.79999999999937\n",
      "Episode 8: Total Reward = -85.79999999999937\n",
      "Episode 9: Total Reward = -85.79999999999937\n",
      "Episode 10: Total Reward = -85.79999999999937\n",
      "Average reward over 10 episodes: -85.79999999999937\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "# Create a model using Stable Baselines3\n",
    "model_a2c = A2C('MlpPolicy', env, tensorboard_log=\"./tensorboard_logs/\", ent_coef=0.1)\n",
    "\n",
    "# Train the model for 10000 timesteps\n",
    "model_a2c.learn(total_timesteps=50000)\n",
    "\n",
    "# Evaluate the model\n",
    "total_reward = 0\n",
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    obs, info = env.reset()  # Corrected: Unpack reset() correctly\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model_a2c.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)  # Correct unpacking\n",
    "        episode_reward += reward\n",
    "\n",
    "    total_reward += episode_reward\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
    "\n",
    "print(f\"Average reward over {episodes} episodes: {total_reward / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07c01e5b-83bd-41b5-b863-163e04bff531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -95.89999999999883\n",
      "Episode 2: Total Reward = -95.89999999999883\n",
      "Episode 3: Total Reward = -95.89999999999883\n",
      "Episode 4: Total Reward = -95.89999999999883\n",
      "Episode 5: Total Reward = -95.89999999999883\n",
      "Episode 6: Total Reward = -95.89999999999883\n",
      "Episode 7: Total Reward = -95.89999999999883\n",
      "Episode 8: Total Reward = -95.89999999999883\n",
      "Episode 9: Total Reward = -95.89999999999883\n",
      "Episode 10: Total Reward = -95.89999999999883\n",
      "Average reward over 10 episodes: -95.89999999999883\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create a model using Stable Baselines3 PPO\n",
    "model_ppo = PPO('MlpPolicy', env, tensorboard_log=\"./tensorboard_logs/\", ent_coef=0.1)\n",
    "\n",
    "# Train the model for 50000 timesteps\n",
    "model_ppo.learn(total_timesteps=50000)\n",
    "\n",
    "# Evaluate the model\n",
    "total_reward = 0\n",
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    obs, info = env.reset()  # Corrected: Unpack reset() correctly\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model_ppo.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)  # Correct unpacking\n",
    "        episode_reward += reward\n",
    "\n",
    "    total_reward += episode_reward\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
    "\n",
    "print(f\"Average reward over {episodes} episodes: {total_reward / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738b1c3-9888-48e5-98c0-469640d357ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
