{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5233f921-8873-4bb3-8aba-14cd38bb361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil\n",
    "\n",
    "# Determine number of available (idle) cores\n",
    "IDLE_THRESHOLD = 20.0  # percent\n",
    "cpu_usages = psutil.cpu_percent(percpu=True, interval=1)\n",
    "available_cores = sum(usage < IDLE_THRESHOLD for usage in cpu_usages)\n",
    "available_cores = max(1, available_cores) - 4 # At least 1\n",
    "\n",
    "available_cores = os.cpu_count()\n",
    "\n",
    "import mplfinance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stable-Baselines3 imports\n",
    "from stable_baselines3 import PPO,A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv  # Use this instead of DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Gymnasium (updated from Gym)\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env,Wrapper\n",
    "from gymnasium.spaces import Discrete, Box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31586fdb-7feb-43af-a1cf-106a65064180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"MGOL.csv\")  # Replace with actual file\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], format='%m/%d/%y %H:%M')\n",
    "df.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae36aaa-2abe-42a7-91da-dcd6904e9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "df.index = df.index + pd.Timedelta(hours=3)\n",
    "df.index.name = 'Date'\n",
    "df = df.drop(columns=['symbol', 'frame'])\n",
    "df = df.iloc[:10]  # Select the first 30 rows\n",
    "df_original = df\n",
    "df = df[[\"close\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8135381-1f8b-4bb9-a04f-f83d49d194fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ActionMasker\n",
    "\n",
    "class ActionMasker(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Wrapper for action masking in environments.\n",
    "    Adds action mask as a part of the environment step.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env, mask_fn: callable):\n",
    "        super().__init__(env)\n",
    "        self.mask_fn = mask_fn\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Get the action mask\n",
    "        action_mask = self.mask_fn(self.env)\n",
    "        \n",
    "        # Add the action mask to the info dictionary\n",
    "        info['action_mask'] = action_mask\n",
    "        \n",
    "        return obs, reward, done, truncated, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c652578-a67d-410c-aaf2-365c48dfaee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaskedPPOPolicy\n",
    "\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "#import torch\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "class MaskedPPOPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, combined_obs, action_mask=None, deterministic=False):\n",
    "        combined_obs = combined_obs.to(dtype=torch.float32)  # Ensure float32\n",
    "\n",
    "        # Extract latent features\n",
    "        latent_pi, latent_vf = self.mlp_extractor(combined_obs.to(dtype=torch.float32))  \n",
    "\n",
    "        # Compute action distribution and value\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        values = self.value_net(latent_vf).to(dtype=torch.float32)  # Ensure float32\n",
    "\n",
    "        # Ensure action_mask is properly handled\n",
    "        if action_mask is None:\n",
    "            action_mask = combined_obs[:, 5:]  # Extract last 3 elements as action mask\n",
    "\n",
    "        if action_mask is not None:\n",
    "            action_mask_tensor = torch.as_tensor(action_mask, dtype=torch.float32, device=combined_obs.device)\n",
    "\n",
    "            # Ensure shape matches logits\n",
    "            action_mask_tensor = action_mask_tensor.view(-1, distribution.distribution.logits.shape[-1])\n",
    "\n",
    "            # Apply masking to logits\n",
    "            distribution.distribution.logits = distribution.distribution.logits.masked_fill(action_mask_tensor == 0, -1e9)\n",
    "\n",
    "        # Action selection\n",
    "        if deterministic:\n",
    "            actions = torch.argmax(distribution.distribution.probs, dim=-1)\n",
    "        else:\n",
    "            actions = distribution.sample()\n",
    "\n",
    "        log_probs = distribution.log_prob(actions)\n",
    "\n",
    "        if debug: print(f\"  - Selected Actions: {actions}\")\n",
    "\n",
    "        return actions, values, log_probs\n",
    "\n",
    "    def predict(self, observation, state=None, mask=None, action_mask=None, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            observation = torch.as_tensor(observation, device=self.device, dtype=torch.float32)\n",
    "            actions, _, _ = self.forward(observation, action_mask=action_mask, deterministic=deterministic)\n",
    "            actions = actions.cpu().numpy()\n",
    "        return actions, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c78b14-3569-431a-86d4-92d0fdf380e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0, debug=False):\n",
    "        super(RewardCallback, self).__init__(verbose)\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Metrics for tracking actions and rewards\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.iteration_rewards = []\n",
    "        self.iteration_invalid_actions = []\n",
    "        self.invalid_actions = []\n",
    "        self.valid_actions = []\n",
    "        self.current_episode_steps = 0\n",
    "        \n",
    "        # Metrics for TensorBoard logging\n",
    "        self.total_reward = 0\n",
    "        self.reward = 0\n",
    "        self.num_trades = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Collect rewards and actions\n",
    "        rewards = self.locals.get(\"rewards\", [])\n",
    "        actions = self.locals.get(\"actions\", [])\n",
    "        \n",
    "        if len(rewards) > 0:  # Check if rewards is not empty\n",
    "            self.episode_rewards.extend(rewards)\n",
    "        if len(actions) > 0:  # Check if actions is not empty\n",
    "            infos = self.locals.get(\"infos\", [])\n",
    "            for idx, info in enumerate(infos):\n",
    "                valid_actions = info.get(\"valid_actions\", [0, 1, 2])\n",
    "                action = actions[idx]\n",
    "                if action not in valid_actions:\n",
    "                    self.invalid_actions.append(action)\n",
    "                else:\n",
    "                    self.valid_actions.append(action)\n",
    "\n",
    "        self.current_episode_steps += 1\n",
    "\n",
    "        # Access the environment metrics using get_attr for SubprocVecEnv\n",
    "        if isinstance(self.training_env, SubprocVecEnv):\n",
    "            try:\n",
    "                inner_envs = self.training_env.get_attr('env')  # ActionMasker\n",
    "                for env in inner_envs:\n",
    "                    if hasattr(env, 'env'):  # Unwrap ActionMasker\n",
    "                        env = env.env\n",
    "                    self.total_reward += getattr(env, \"total_reward\", 0)\n",
    "                    self.num_trades += getattr(env, \"round_trip_trades\", 0)\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"Failed to access env attributes: {e}\")\n",
    "        else:\n",
    "            # For DummyVecEnv or single environments\n",
    "            for env in self.training_env.envs:\n",
    "                if hasattr(env, 'env'):\n",
    "                    env = env.env\n",
    "                self.total_reward += getattr(env, \"total_reward\", 0)\n",
    "                self.num_trades += getattr(env, \"round_trip_trades\", 0)\n",
    "\n",
    "        # TensorBoard logging\n",
    "        self.logger.record(\"custom/num_trades\", self.num_trades)\n",
    "        self.logger.record(\"custom/total_reward\", self.total_reward)\n",
    "\n",
    "        # Entropy logging\n",
    "        if hasattr(self.model.policy, \"action_dist\"):\n",
    "            action_dist = self.model.policy.action_dist\n",
    "            entropy = action_dist.entropy().mean().item()\n",
    "            self.logger.record(\"policy/entropy\", entropy)\n",
    "        elif hasattr(self.model.policy, \"get_distribution\"):\n",
    "            obs = self.locals.get(\"obs\", [])\n",
    "            if len(obs) > 0:  # Check if observations exist\n",
    "                action_dist = self.model.policy.get_distribution(obs)\n",
    "                entropy = action_dist.entropy().mean().item()\n",
    "                self.logger.record(\"policy/entropy\", entropy)\n",
    "\n",
    "        # Value loss logging\n",
    "        if \"value_loss\" in self.locals:\n",
    "            value_loss = self.locals[\"value_loss\"]\n",
    "            self.logger.record(\"loss/value_loss\", value_loss)\n",
    "\n",
    "        # Episode done handling\n",
    "        dones = self.locals.get(\"dones\", [])\n",
    "        if any(dones):\n",
    "            self.episode_steps.append(self.current_episode_steps)\n",
    "            self.current_episode_steps = 0\n",
    "            total_reward = np.sum(self.episode_rewards)\n",
    "            self.iteration_rewards.append(total_reward)\n",
    "            self.episode_rewards = []\n",
    "\n",
    "            invalid_count = len(self.invalid_actions)\n",
    "            valid_count = len(self.valid_actions)\n",
    "            self.iteration_invalid_actions.append(invalid_count)\n",
    "\n",
    "            if self.debug:\n",
    "                print(f\"Invalid actions in this episode: {invalid_count}\")\n",
    "                print(f\"Valid actions in this episode: {valid_count}\")\n",
    "                print(f\"Invalid actions: {self.invalid_actions}\")\n",
    "\n",
    "            self.invalid_actions = []\n",
    "\n",
    "        return True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0ef59ec-1cb5-41b1-a1f4-5bb135a3cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest DebugCallback\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class DebugCallback(BaseCallback):\n",
    "    def __init__(self, debug_episodes=None, verbose=0, debug=False):\n",
    "        super().__init__(verbose)\n",
    "        self.debug_episodes = set(debug_episodes) if debug_episodes is not None else set()\n",
    "        self.episode_counts = []\n",
    "        self.episode_step_counts = {}  # Track steps per environment\n",
    "        self.debug = debug\n",
    "        self.debug_triggered = False  # Optional: Only needed if you want to trigger once\n",
    "        self.debug_on_step = None     # Optional: If you plan to use this\n",
    "        self.last_printed_episodes = []  # To store the last printed episode number\n",
    "        self.max_episode_steps = {}  # Track the max steps per environment\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        n_envs = self.training_env.num_envs\n",
    "        # Initialize the counts and step tracking for all environments\n",
    "        self.episode_counts = [0] * n_envs\n",
    "        self.last_printed_episodes = [None] * n_envs  # Make sure this list is of the correct size\n",
    "        self.episode_step_counts = {i: 0 for i in range(n_envs)}  # Initialize step counts for all environments\n",
    "        self.max_episode_steps = {i: 0 for i in range(n_envs)}  # Initialize the max episode steps per environment\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        num_envs = getattr(self.training_env, \"num_envs\", 1)\n",
    "        self.episode_counts = [0] * num_envs\n",
    "        self.current_episode_steps = [0] * num_envs\n",
    "\n",
    "    def on_training_end(self) -> None:\n",
    "        # Print the maximum steps per environment after training ends\n",
    "        for env_id, max_steps in self.max_episode_steps.items():\n",
    "            if self.debug: print(f\"Max steps in episode for env {env_id}: {max_steps}\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        dones = self.locals.get(\"dones\", [])\n",
    "        infos = self.locals.get(\"infos\", [])\n",
    "\n",
    "        for i in range(len(dones)):\n",
    "            # Ensure that episode_step_counts has the environment index initialized\n",
    "            if i not in self.episode_step_counts:\n",
    "                self.episode_step_counts[i] = 0\n",
    "\n",
    "            # Track steps\n",
    "            self.episode_step_counts[i] += 1\n",
    "\n",
    "            # Update the maximum episode steps for each environment\n",
    "            if self.episode_step_counts[i] > self.max_episode_steps[i]:\n",
    "                self.max_episode_steps[i] = self.episode_step_counts[i]\n",
    "            \n",
    "            episode_num = self.episode_counts[i]\n",
    "\n",
    "            # Print start of episode (optional)\n",
    "            if self.last_printed_episodes[i] != episode_num:\n",
    "                if self.debug: print(f\"Current episode (env {i}): {episode_num}\")\n",
    "                self.last_printed_episodes[i] = episode_num\n",
    "\n",
    "            # Debug output if in debug_episodes list\n",
    "            if episode_num in self.debug_episodes:\n",
    "                if self.debug: print(f\"Current step (env {i}): {self.episode_step_counts[i]}\")\n",
    "                if self.debug: print(f\"[Env {i}] dones: {dones[i]} infos: {infos[i]}\")\n",
    "\n",
    "            # If episode is done, print step count and reset counter\n",
    "            if dones[i]:\n",
    "                if self.debug: print(f\"Episode {episode_num} (env {i}) finished in {self.episode_step_counts[i]} steps.\")\n",
    "                self.episode_counts[i] += 1\n",
    "                self.episode_step_counts[i] = 0  # Reset for next episode\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b7e9c-9e13-49f9-bb9e-bc175f884994",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, debug=False, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.iteration_rewards_per_env = []\n",
    "        self.iteration_invalid_actions_per_env = []\n",
    "        self.debug = debug\n",
    "        self.n_envs = None\n",
    "\n",
    "        self.current_episode_steps = 0\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.n_envs = self.training_env.num_envs\n",
    "        self.current_rewards = [0.0 for _ in range(self.n_envs)]\n",
    "        self.invalid_actions = [0 for _ in range(self.n_envs)]\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        infos = self.locals[\"infos\"]\n",
    "\n",
    "        for i in range(self.n_envs):\n",
    "            self.current_rewards[i] += rewards[i]\n",
    "            if infos[i].get(\"invalid_action\", False):\n",
    "                self.invalid_actions[i] += 1\n",
    "\n",
    "            if dones[i]:\n",
    "                if self.debug:\n",
    "                    print(f\"[Env {i}] Done. Number of steps: {self.current_episode_steps}, Reward: {self.current_rewards[i]:.2f}, Invalid: {self.invalid_actions[i]}\")\n",
    "                self.iteration_rewards_per_env.append((i, self.current_rewards[i]))\n",
    "                self.iteration_invalid_actions_per_env.append((i, self.invalid_actions[i]))\n",
    "                self.current_rewards[i] = 0.0\n",
    "                self.invalid_actions[i] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # Compute summary per environment\n",
    "        from collections import defaultdict\n",
    "\n",
    "        reward_sums = defaultdict(float)\n",
    "        invalid_sums = defaultdict(int)\n",
    "\n",
    "        for env_id, reward in self.iteration_rewards_per_env:\n",
    "            reward_sums[env_id] += reward\n",
    "        for env_id, invalid in self.iteration_invalid_actions_per_env:\n",
    "            invalid_sums[env_id] += invalid\n",
    "\n",
    "        print(\"\\n=== Final Per-Environment Summary ===\")\n",
    "        for i in range(self.n_envs):\n",
    "            print(f\"[Env {i}] Total Reward: {reward_sums[i]:.2f}, Invalid Actions: {invalid_sums[i]}\")\n",
    "\n",
    "        # Print full list format for rewards and invalids\n",
    "        rewards_list = [reward_sums[i] for i in range(self.n_envs)]\n",
    "        invalids_list = [invalid_sums[i] for i in range(self.n_envs)]\n",
    "\n",
    "        print(\"\\nRewards per environment (list):\", rewards_list)\n",
    "        print(\"Invalid actions per environment (list):\", invalids_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ab45131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0, debug=False):\n",
    "        super(RewardCallback, self).__init__(verbose)\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Episode-level tracking\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.iteration_rewards = []\n",
    "        self.iteration_invalid_actions = []\n",
    "        self.invalid_actions = []\n",
    "        self.valid_actions = []\n",
    "        self.current_episode_steps = 0\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        self.total_reward = 0\n",
    "        self.reward = 0\n",
    "        self.num_trades = 0\n",
    "\n",
    "        self.iteration_rewards_per_env = []\n",
    "        self.iteration_invalid_actions_per_env = []\n",
    "        self.n_envs = None\n",
    "        \n",
    "        # Per-environment tracking\n",
    "        self.per_env_rewards = defaultdict(list)\n",
    "        self.per_env_invalid_actions = defaultdict(list)\n",
    "\n",
    "        self.per_env_rewards_total = defaultdict(list)\n",
    "        self.per_env_invalid_actions_total = defaultdict(list)\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.n_envs = self.training_env.num_envs\n",
    "        self.current_rewards = [0.0 for _ in range(self.n_envs)]\n",
    "        self.invalid_actions = [0 for _ in range(self.n_envs)]\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        print(\"\\n=== Final Per-Environment Summary ===\")\n",
    "        # Summary across all episodes\n",
    "        for env_idx in sorted(self.per_env_rewards_total.keys()):\n",
    "            total_reward = np.sum(self.per_env_rewards_total[env_idx])\n",
    "            total_invalid = len(self.per_env_invalid_actions_total[env_idx])\n",
    "            print(f\"[Env {env_idx}] Total Reward: {total_reward:.2f}, Invalid Actions: {total_invalid}\")\n",
    "\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals.get(\"rewards\", [])\n",
    "        actions = self.locals.get(\"actions\", [])\n",
    "        infos = self.locals.get(\"infos\", [])\n",
    "\n",
    "        for idx, info in enumerate(infos):\n",
    "            valid_actions = info.get(\"valid_actions\", [0, 1, 2])\n",
    "            action = actions[idx]\n",
    "            reward = rewards[idx]\n",
    "        \n",
    "            self.per_env_rewards[idx].append(reward)\n",
    "            self.per_env_rewards_total[idx].append(reward)\n",
    "        \n",
    "            if action not in valid_actions:\n",
    "                self.invalid_actions.append(action)\n",
    "                self.per_env_invalid_actions[idx].append(action)\n",
    "                self.per_env_invalid_actions_total[idx].append(action)\n",
    "            else:\n",
    "                self.valid_actions.append(action)\n",
    "        \n",
    "            self.episode_rewards.append(reward)\n",
    "\n",
    "\n",
    "        self.current_episode_steps += 1\n",
    "\n",
    "        # Log metrics from environment\n",
    "        if isinstance(self.training_env, SubprocVecEnv):\n",
    "            try:\n",
    "                inner_envs = self.training_env.get_attr('env')  # Get ActionMasker\n",
    "                for env in inner_envs:\n",
    "                    if hasattr(env, 'env'):\n",
    "                        env = env.env\n",
    "                    self.total_reward += getattr(env, \"total_reward\", 0)\n",
    "                    self.num_trades += getattr(env, \"round_trip_trades\", 0)\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"Failed to access env attributes: {e}\")\n",
    "        else:\n",
    "            for env in self.training_env.envs:\n",
    "                if hasattr(env, 'env'):\n",
    "                    env = env.env\n",
    "                self.total_reward += getattr(env, \"total_reward\", 0)\n",
    "                self.num_trades += getattr(env, \"round_trip_trades\", 0)\n",
    "\n",
    "        # Logging to TensorBoard\n",
    "        self.logger.record(\"custom/num_trades\", self.num_trades)\n",
    "        self.logger.record(\"custom/total_reward\", self.total_reward)\n",
    "\n",
    "        # Entropy logging\n",
    "        if hasattr(self.model.policy, \"action_dist\"):\n",
    "            action_dist = self.model.policy.action_dist\n",
    "            entropy = action_dist.entropy().mean().item()\n",
    "            self.logger.record(\"policy/entropy\", entropy)\n",
    "        elif hasattr(self.model.policy, \"get_distribution\"):\n",
    "            obs = self.locals.get(\"obs\", [])\n",
    "            if len(obs) > 0:\n",
    "                action_dist = self.model.policy.get_distribution(obs)\n",
    "                entropy = action_dist.entropy().mean().item()\n",
    "                self.logger.record(\"policy/entropy\", entropy)\n",
    "\n",
    "        # Value loss logging\n",
    "        if \"value_loss\" in self.locals:\n",
    "            value_loss = self.locals[\"value_loss\"]\n",
    "            self.logger.record(\"loss/value_loss\", value_loss)\n",
    "\n",
    "        # Episode done\n",
    "        dones = self.locals.get(\"dones\", [])\n",
    "        if any(dones):\n",
    "            self.episode_steps.append(self.current_episode_steps)\n",
    "            self.current_episode_steps = 0\n",
    "            total_reward = np.sum(self.episode_rewards)\n",
    "            self.iteration_rewards.append(total_reward)\n",
    "            self.episode_rewards = []\n",
    "\n",
    "            self.iteration_invalid_actions.append(len(self.invalid_actions))\n",
    "\n",
    "            if self.debug:\n",
    "                print(\"\\n--- Episode finished ---\")\n",
    "                for env_idx in sorted(self.per_env_rewards.keys()):\n",
    "                    total_env_reward = np.sum(self.per_env_rewards[env_idx])\n",
    "                    total_env_invalid = len(self.per_env_invalid_actions[env_idx])\n",
    "                    print(f\"[Env {env_idx}] Total reward: {total_env_reward:.2f}, Invalid actions: {total_env_invalid}\")\n",
    "\n",
    "            # Reset per-episode and per-env stats\n",
    "            self.invalid_actions.clear()\n",
    "            self.valid_actions.clear()\n",
    "            self.per_env_rewards.clear()\n",
    "            self.per_env_invalid_actions.clear()\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b1e9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, debug=False, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.debug = debug\n",
    "        self.n_envs = None\n",
    "        self.episode_count = 0  # Total episode counter\n",
    "        self.episode_count_per_env = None  # Episode counter per environment\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.n_envs = self.training_env.num_envs\n",
    "        self.episode_count_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.current_rewards = [0.0 for _ in range(self.n_envs)]\n",
    "        self.invalid_actions = [0 for _ in range(self.n_envs)]\n",
    "        self.current_steps = [0 for _ in range(self.n_envs)]  # Steps counter per environment\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        infos = self.locals[\"infos\"]\n",
    "\n",
    "        for i in range(self.n_envs):\n",
    "            # Track steps for each environment\n",
    "            if not dones[i]:\n",
    "                self.current_steps[i] += 1\n",
    "            \n",
    "            # Update reward and invalid actions\n",
    "            self.current_rewards[i] += rewards[i]\n",
    "            if infos[i].get(\"invalid_action\", False):\n",
    "                self.invalid_actions[i] += 1\n",
    "\n",
    "            if dones[i]:\n",
    "                self.episode_count += 1\n",
    "                self.episode_count_per_env[i] += 1\n",
    "                \n",
    "                if self.debug:\n",
    "                    print(f\"[Env {i}] Episode {self.episode_count_per_env[i]} Done. \"\n",
    "                          f\"Steps: {self.current_steps[i]}, \"\n",
    "                          f\"Reward: {self.current_rewards[i]:.2f}, \"\n",
    "                          f\"Invalid: {self.invalid_actions[i]}\")\n",
    "                \n",
    "                # Reset episode-specific metrics\n",
    "                self.current_steps[i] = 0\n",
    "                self.current_rewards[i] = 0.0\n",
    "                self.invalid_actions[i] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        print(\"\\n=== Final Training Summary ===\")\n",
    "        print(f\"Total episodes completed: {self.episode_count}\")\n",
    "        for i in range(self.n_envs):\n",
    "            print(f\"[Env {i}] Total episodes: {self.episode_count_per_env[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03c9395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, debug=False, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.debug = debug\n",
    "        self.n_envs = None\n",
    "        self.episode_count = 0\n",
    "        self.episode_count_per_env = None\n",
    "        \n",
    "        # New tracking\n",
    "        self.current_rewards = None\n",
    "        self.invalid_actions = None\n",
    "        self.current_steps = None\n",
    "        \n",
    "        # Old tracking (for compatibility)\n",
    "        self.iteration_rewards = []\n",
    "        self.iteration_invalid_actions = []\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.n_envs = self.training_env.num_envs\n",
    "        self.episode_count_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.current_rewards = [0.0 for _ in range(self.n_envs)]\n",
    "        self.invalid_actions = [0 for _ in range(self.n_envs)]\n",
    "        self.current_steps = [0 for _ in range(self.n_envs)]\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        infos = self.locals[\"infos\"]\n",
    "\n",
    "        for i in range(self.n_envs):\n",
    "            if not dones[i]:\n",
    "                self.current_steps[i] += 1\n",
    "            self.current_rewards[i] += rewards[i]\n",
    "            \n",
    "            if infos[i].get(\"invalid_action\", False):\n",
    "                self.invalid_actions[i] += 1\n",
    "\n",
    "            if dones[i]:\n",
    "                self.episode_count += 1\n",
    "                self.episode_count_per_env[i] += 1\n",
    "                \n",
    "                if self.debug:\n",
    "                    print(f\"[Env {i}] Episode {self.episode_count_per_env[i]} Done. \"\n",
    "                          f\"Steps: {self.current_steps[i]}, \"\n",
    "                          f\"Reward: {self.current_rewards[i]:.2f}, \"\n",
    "                          f\"Invalid: {self.invalid_actions[i]}\")\n",
    "                \n",
    "                # Add to old tracking system\n",
    "                self.iteration_rewards.append(self.current_rewards[i])\n",
    "                self.iteration_invalid_actions.append(self.invalid_actions[i])\n",
    "                \n",
    "                # Reset episode-specific metrics\n",
    "                self.current_steps[i] = 0\n",
    "                self.current_rewards[i] = 0.0\n",
    "                self.invalid_actions[i] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        print(\"\\n=== Final Training Summary ===\")\n",
    "        print(f\"Number of environments: {self.n_envs}\")\n",
    "        print(f\"Total episodes completed: {self.episode_count}\")\n",
    "        print(f\"Average episodes per environment: {self.episode_count / self.n_envs:.1f}\")\n",
    "        print(f\"Total steps taken: {self.total_steps}\")\n",
    "        print(f\"Average steps per episode: {self.total_steps / self.episode_count:.1f}\")\n",
    "        \n",
    "        for i in range(self.n_envs):\n",
    "            print(f\"\\n[Env {i}] Summary:\")\n",
    "            print(f\"Total episodes: {self.episode_count_per_env[i]}\")\n",
    "            print(f\"Total steps: {self.total_steps_per_env[i]}\")\n",
    "            print(f\"Average steps per episode: {self.total_steps_per_env[i] / self.episode_count_per_env[i]:.1f}\")\n",
    "        \n",
    "        # Print old format for compatibility\n",
    "        formatted_rewards = ', '.join(f\"{reward:.1f}\" for reward in self.iteration_rewards)\n",
    "        formatted_invalid_actions = ', '.join(str(invalid) for invalid in self.iteration_invalid_actions)\n",
    "        print(\"\\nTotal rewards per iteration:\", formatted_rewards)\n",
    "        print(\"Invalid actions per iteration:\", formatted_invalid_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f59bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest RewardCallback\n",
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, debug=False, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.debug = debug\n",
    "        self.n_envs = None\n",
    "        self.episode_count = 0\n",
    "        self.episode_count_per_env = None\n",
    "        self.total_steps_per_env = None\n",
    "        \n",
    "        # New tracking\n",
    "        self.current_episode_rewards = None\n",
    "        self.episode_rewards = None\n",
    "        self.invalid_actions = None\n",
    "        self.current_steps = None\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.n_envs = self.training_env.num_envs\n",
    "        self.episode_rewards = []\n",
    "        self.episode_invalid_actions = []\n",
    "        self.episode_count_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.total_steps_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.episode_reward_per_env = [0.0 for _ in range(self.n_envs)]\n",
    "        self.invalid_actions_per_env = [0 for _ in range(self.n_envs)]\n",
    "        self.current_steps_per_env = [0 for _ in range(self.n_envs)]\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        infos = self.locals[\"infos\"]\n",
    "\n",
    "        for i in range(self.n_envs):\n",
    "            # Increment step counters for every step\n",
    "            self.current_steps_per_env[i] += 1\n",
    "            self.total_steps += 1\n",
    "            self.total_steps_per_env[i] += 1\n",
    "            \n",
    "            # Update rewards and check for invalid actions\n",
    "            self.episode_reward_per_env[i] += rewards[i]\n",
    "            \n",
    "            if infos[i].get(\"is_invalid\", False):\n",
    "                self.invalid_actions_per_env[i] += 1\n",
    "\n",
    "            # Handle episode completion\n",
    "            if dones[i]:\n",
    "                self.episode_count += 1\n",
    "                self.episode_count_per_env[i] += 1\n",
    "                \n",
    "                if self.debug:\n",
    "                    print(f\"[Env {i}] Episode {self.episode_count_per_env[i]} Done. \"\n",
    "                        f\"Steps: {self.current_steps_per_env[i]}, \"\n",
    "                        f\"Reward: {self.episode_reward_per_env[i]:.2f}, \"\n",
    "                        f\"Invalid: {self.invalid_actions_per_env[i]}\")\n",
    "\n",
    "        if all(dones):     \n",
    "            # Log mean reward across all environments\n",
    "            self.episode_rewards.append(sum(self.episode_reward_per_env))\n",
    "            self.episode_invalid_actions.append(sum(self.invalid_actions_per_env))\n",
    "            total_reward = sum(self.episode_rewards)\n",
    "            self.logger.record(\"rewards/total_reward\", total_reward)         \n",
    "\n",
    "        for i in range(self.n_envs):\n",
    "            if dones[i]:\n",
    "                self.current_steps_per_env[i] = 0\n",
    "                self.episode_reward_per_env[i] = 0.0\n",
    "                self.invalid_actions_per_env[i] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        print(\"\\n=== Final Training Summary ===\")\n",
    "        print(f\"Number of environments: {self.n_envs}\")\n",
    "        print(f\"Total episodes completed: {self.episode_count}\")\n",
    "        print(f\"Average episodes per environment: {self.episode_count / self.n_envs:.1f}\")\n",
    "        print(f\"Total steps taken: {self.total_steps}\")\n",
    "        print(f\"Average steps per episode: {self.total_steps / self.episode_count:.1f}\")\n",
    "        \n",
    "        for i in range(self.n_envs):\n",
    "            print(f\"\\n[Env {i}] Summary:\")\n",
    "            print(f\"Total episodes: {self.episode_count_per_env[i]}\")\n",
    "            print(f\"Total steps: {self.total_steps_per_env[i]}\")\n",
    "            print(f\"Average steps per episode: {self.total_steps_per_env[i] / self.episode_count_per_env[i]:.1f}\")\n",
    "        \n",
    "        # Print old format for compatibility\n",
    "        formatted_rewards = ', '.join(f\"{reward:.1f}\" for reward in self.episode_rewards)\n",
    "        formatted_invalid_actions = ', '.join(str(invalid) for invalid in self.episode_invalid_actions)\n",
    "        print(\"\\nTotal rewards per episode per env:\", formatted_rewards)\n",
    "        print(\"Invalid actions per episode per env:\", formatted_invalid_actions)\n",
    "\n",
    "        # Group by iterations (12 environments at a time)\n",
    "        n_envs = self.n_envs\n",
    "        n_iterations = len(self.episode_rewards) // n_envs\n",
    "\n",
    "        \"\"\" print(\"\\n=== Per-Iteration Summary ===\")\n",
    "        for i in range(n_iterations):\n",
    "            start_idx = i * n_envs\n",
    "            end_idx = (i + 1) * n_envs\n",
    "            iter_rewards = self.episode_rewards[start_idx:end_idx]\n",
    "            iter_invalid = self.episode_invalid_actions[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"\\nIteration {i + 1}:\")\n",
    "            print(f\"  Total rewards: {sum(iter_rewards):.1f}\")\n",
    "            print(f\"  Total invalid actions: {sum(iter_invalid)}\")\n",
    "            print(f\"  Avg reward per env: {np.mean(iter_rewards):.1f}\")\n",
    "            print(f\"  Avg invalid per env: {np.mean(iter_invalid):.1f}\") \"\"\"\n",
    "\n",
    "        # Print overall totals\n",
    "        print(\"\\n=== Combined Totals ===\")\n",
    "        print(f\"Total combined rewards: {sum(self.episode_rewards):.1f}\")\n",
    "        print(f\"Total invalid actions: {sum(self.episode_invalid_actions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36fdbb-ec35-4a59-9c03-cfce1d83b19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Observation with mask:Observation with mask:  Observation with mask: Observation with mask:Observation with mask: Observation with mask: Observation with mask: [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Step 0:[3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Observation with mask:Step 0:\n",
      "  [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Step 0:\n",
      "  - Original Observation (shape (5,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00]\n",
      "[3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]  - Original Observation (shape (5,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00]\n",
      "\n",
      "Step 0:\n",
      "  - Action Mask (shape (3,)): [1. 1. 0.]\n",
      "[3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Step 0:\n",
      "  - Action Mask (shape (3,)): [1. 1. 0.]\n",
      "  - Original Observation (shape (5,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00]\n",
      "  - Combined Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Original Observation (shape (5,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00]\n",
      "  - Combined Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Action Mask (shape (3,)): [1. 1. 0.]\n",
      "Environment reset: Initial Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Action Mask (shape (3,)): [1. 1. 0.]\n",
      "Environment reset: Initial Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Combined Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "[3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Combined Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Step 0:\n",
      "[3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Observation with mask:Environment reset: Initial Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00] Step 0:\n",
      "\n",
      "Environment reset: Initial Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Observation with mask: \n",
      "  - Original Observation (shape (5,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00]\n",
      "  - Original Observation (shape (5,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00]\n",
      "[3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Step 0:\n",
      "[3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]  - Action Mask (shape (3,)): [1. 1. 0.]\n",
      "\n",
      "Step 0:\n",
      "Observation with mask:   - Original Observation (shape (5,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00]\n",
      "Observation with mask:[3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00] \n",
      "Step 0:\n",
      "  - Combined Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Action Mask (shape (3,)): [1. 1. 0.]  - Original Observation (shape (5,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00]\n",
      "  - Original Observation (shape (5,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00]\n",
      "\n",
      "Environment reset: Initial Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Original Observation (shape (5,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00]\n",
      "[3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Step 0:\n",
      "  - Action Mask (shape (3,)): [1. 1. 0.]\n",
      "[3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]  - Action Mask (shape (3,)): [1. 1. 0.]\n",
      "\n",
      "Step 0:\n",
      "  - Action Mask (shape (3,)): [1. 1. 0.]\n",
      "  - Original Observation (shape (5,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00]\n",
      "  - Combined Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Combined Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Original Observation (shape (5,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00]\n",
      "  - Combined Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Action Mask (shape (3,)): [1. 1. 0.]\n",
      "Environment reset: Initial Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Environment reset: Initial Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Action Mask (shape (3,)): [1. 1. 0.]\n",
      "Environment reset: Initial Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Combined Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Combined Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Action Mask (shape (3,)): [1. 1. 0.]\n",
      "Environment reset: Initial Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Combined Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Environment reset: Initial Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "  - Combined Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]Environment reset: Initial Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "\n",
      "Environment reset: Initial Observation (shape (8,)): [3.386e-01 0.000e+00 1.000e+04 1.000e+04 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00]\n",
      "Logging to ./tensorboard_logs/sb3_ppo_12\n",
      "Current Step: 0\n",
      " Close_price: 0.3386\n",
      "Current Step: 0\n",
      "Current Step: 0\n",
      " Close_price:  Close_price: 0.3386\n",
      "0.3386\n",
      "Current Step: 0\n",
      "Current Step: 0\n",
      " Close_price:Current Step: 0\n",
      "Current Step: 0\n",
      " Close_price: 0.3386\n",
      "Current Step: 0\n",
      " Close_price: 0.3386\n",
      " Close_price: 0.3386\n",
      "Current Step: 0\n",
      " Close_price: 0.3386\n",
      "Current Step: 0\n",
      " Close_price: 0.3386\n",
      "Current Step: 0\n",
      "Current Step: 0\n",
      " Close_price: 0.3386\n",
      " Close_price: 0.3386\n",
      " Close_price: 0.3386\n",
      " 0.3386\n",
      "Warning: env subprocess already crashed, skipping close()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-52:\n",
      "Process ForkServerProcess-49:\n",
      "Process ForkServerProcess-50:\n",
      "Process ForkServerProcess-55:\n",
      "Process ForkServerProcess-56:\n",
      "Process ForkServerProcess-54:\n",
      "Process ForkServerProcess-51:\n",
      "Process ForkServerProcess-57:\n",
      "Process ForkServerProcess-58:\n",
      "Process ForkServerProcess-60:\n",
      "Process ForkServerProcess-59:\n",
      "Process ForkServerProcess-53:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/larka/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "  File \"/var/folders/h5/0dxd7qkj3ls9c04wvb460nhm0000gr/T/ipykernel_51843/59336617.py\", line 11, in step\n",
      "  File \"/Users/larka/GitHub/RLmodel/trading_env_sb3_ver1.py\", line 121, in step\n",
      "    if debug: print(' Action', action)\n",
      "NameError: name 'debug' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/larka/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "  File \"/var/folders/h5/0dxd7qkj3ls9c04wvb460nhm0000gr/T/ipykernel_51843/59336617.py\", line 11, in step\n",
      "  File \"/Users/larka/GitHub/RLmodel/trading_env_sb3_ver1.py\", line 121, in step\n",
      "    if debug: print(' Action', action)\n",
      "NameError: name 'debug' is not defined\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/larka/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "  File \"/var/folders/h5/0dxd7qkj3ls9c04wvb460nhm0000gr/T/ipykernel_51843/59336617.py\", line 11, in step\n",
      "  File \"/Users/larka/GitHub/RLmodel/trading_env_sb3_ver1.py\", line 121, in step\n",
      "    if debug: print(' Action', action)\n",
      "NameError: name 'debug' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/larka/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "  File \"/var/folders/h5/0dxd7qkj3ls9c04wvb460nhm0000gr/T/ipykernel_51843/59336617.py\", line 11, in step\n",
      "  File \"/Users/larka/GitHub/RLmodel/trading_env_sb3_ver1.py\", line 121, in step\n",
      "    if debug: print(' Action', action)\n",
      "NameError: name 'debug' is not defined\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/larka/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "  File \"/var/folders/h5/0dxd7qkj3ls9c04wvb460nhm0000gr/T/ipykernel_51843/59336617.py\", line 11, in step\n",
      "  File \"/Users/larka/GitHub/RLmodel/trading_env_sb3_ver1.py\", line 121, in step\n",
      "    if debug: print(' Action', action)\n",
      "NameError: name 'debug' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/larka/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "  File \"/var/folders/h5/0dxd7qkj3ls9c04wvb460nhm0000gr/T/ipykernel_51843/59336617.py\", line 11, in step\n",
      "  File \"/Users/larka/GitHub/RLmodel/trading_env_sb3_ver1.py\", line 121, in step\n",
      "    if debug: print(' Action', action)\n",
      "NameError: name 'debug' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/larka/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "  File \"/var/folders/h5/0dxd7qkj3ls9c04wvb460nhm0000gr/T/ipykernel_51843/59336617.py\", line 11, in step\n",
      "  File \"/Users/larka/GitHub/RLmodel/trading_env_sb3_ver1.py\", line 121, in step\n",
      "    if debug: print(' Action', action)\n",
      "NameError: name 'debug' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/larka/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "  File \"/var/folders/h5/0dxd7qkj3ls9c04wvb460nhm0000gr/T/ipykernel_51843/59336617.py\", line 11, in step\n",
      "  File \"/Users/larka/GitHub/RLmodel/trading_env_sb3_ver1.py\", line 121, in step\n",
      "    if debug: print(' Action', action)\n",
      "NameError: name 'debug' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/larka/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "  File \"/var/folders/h5/0dxd7qkj3ls9c04wvb460nhm0000gr/T/ipykernel_51843/59336617.py\", line 11, in step\n",
      "  File \"/Users/larka/GitHub/RLmodel/trading_env_sb3_ver1.py\", line 121, in step\n",
      "    if debug: print(' Action', action)\n",
      "NameError: name 'debug' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/larka/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "  File \"/var/folders/h5/0dxd7qkj3ls9c04wvb460nhm0000gr/T/ipykernel_51843/59336617.py\", line 11, in step\n",
      "  File \"/Users/larka/GitHub/RLmodel/trading_env_sb3_ver1.py\", line 121, in step\n",
      "    if debug: print(' Action', action)\n",
      "NameError: name 'debug' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/larka/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "  File \"/var/folders/h5/0dxd7qkj3ls9c04wvb460nhm0000gr/T/ipykernel_51843/59336617.py\", line 11, in step\n",
      "  File \"/Users/larka/GitHub/RLmodel/trading_env_sb3_ver1.py\", line 121, in step\n",
      "    if debug: print(' Action', action)\n",
      "NameError: name 'debug' is not defined\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/larka/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "  File \"/var/folders/h5/0dxd7qkj3ls9c04wvb460nhm0000gr/T/ipykernel_51843/59336617.py\", line 11, in step\n",
      "  File \"/Users/larka/GitHub/RLmodel/trading_env_sb3_ver1.py\", line 121, in step\n",
      "    if debug: print(' Action', action)\n",
      "NameError: name 'debug' is not defined\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m reward_callback \u001b[38;5;241m=\u001b[39m RewardCallback(debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mppo_masked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msb3_ppo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallbackList\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreward_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Combine callbacks\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:207\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:129\u001b[0m, in \u001b[0;36mSubprocVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m--> 129\u001b[0m     results \u001b[38;5;241m=\u001b[39m [remote\u001b[38;5;241m.\u001b[39mrecv() \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes]\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     obs, rews, dones, infos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:129\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m--> 129\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\u001b[43mremote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes]\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     obs, rews, dones, infos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/multiprocessing/connection.py:383\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m==\u001b[39m size:\n\u001b[0;32m--> 383\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot end of file during message\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize parallel environments and train model\n",
    "import torch\n",
    "import importlib\n",
    "import trading_env_sb3_ver1\n",
    "importlib.reload(trading_env_sb3_ver1)\n",
    "from trading_env_sb3_ver1 import TradingEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "\n",
    "# Initialize the custom TradingEnv environment\n",
    "env1 = TradingEnv(df)  # Your custom environment\n",
    "\n",
    "# Define the mask_fn to get valid actions from the environment\n",
    "def mask_fn(env: gym.Env) -> np.ndarray:\n",
    "    return env.get_action_mask()  # Get valid action mask from the environment\n",
    "\n",
    "# Wrap the environment with ActionMasker to apply action masking\n",
    "env_masked = ActionMasker(env1, mask_fn)  # Apply the ActionMasker wrapper\n",
    "\n",
    "# Define the number of CPU cores to use\n",
    "num_cpu = available_cores  # Get the number of available CPU cores\n",
    "\n",
    "#num_cpu = 4\n",
    "\n",
    "# --- Function to create the wrapped environment ---\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = TradingEnv(df)  # Your DataFrame must be accessible here\n",
    "        return ActionMasker(env, mask_fn)\n",
    "    return _init\n",
    "\n",
    "# Create parallel environments using SubprocVecEnv\n",
    "env = SubprocVecEnv([make_env() for _ in range(num_cpu)])\n",
    "\n",
    "# Define PPO model with the custom policy using the vectorized environment\n",
    "ppo_masked_model = PPO(\n",
    "    MaskedPPOPolicy,  # Custom policy\n",
    "    env,               # Your environment\n",
    "    verbose=1,         \n",
    "    tensorboard_log=\"./tensorboard_logs/\",\n",
    "    ent_coef=0.0,      # Small exploration penalty\n",
    "    gamma=0.99,       # Discount factor for long-term rewards\n",
    "    gae_lambda=0.9,    # Optimistic advantage estimation (strongly favors long-term)\n",
    "    n_steps=512,      # Large number of timesteps per iteration\n",
    "    clip_range=0.1,    # Clipping to allow more aggressive updates\n",
    "    n_epochs=5,       # Number of passes over the data (many epochs to overfit)\n",
    "    batch_size=128    # Large batch size for stability in updates\n",
    ")\n",
    "\n",
    "# Train the model with the callback\n",
    "from multiprocessing import set_start_method\n",
    "set_start_method('spawn', force=True)\n",
    "\n",
    "debug_episodes = {}  # Or use range() for patterns\n",
    "debug_callback = DebugCallback(debug_episodes=debug_episodes, debug=False)\n",
    "\n",
    "# Initialize the callback\n",
    "reward_callback = RewardCallback(debug=True)\n",
    "\n",
    "try:\n",
    "    ppo_masked_model.learn(\n",
    "        total_timesteps=500000,\n",
    "        progress_bar=False,\n",
    "        tb_log_name=\"sb3_ppo\",\n",
    "        callback=CallbackList([reward_callback, debug_callback])  # Combine callbacks\n",
    "    )\n",
    "finally:\n",
    "    try:\n",
    "        env.close()\n",
    "    except EOFError:\n",
    "        print(\"Warning: env subprocess already crashed, skipping close()\")\n",
    "\n",
    "# Save the model\n",
    "ppo_masked_model.save('sb3_ppo_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8ff321f-c797-4c8d-bd1f-5619c2d08978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(model, env, buy_signals_list, sell_signals_list, seed=42):\n",
    "    \"\"\"\n",
    "    Test the trained model with action masking.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PPO model.\n",
    "        env: Environment with action masking.\n",
    "        buy_signals_list: List to store buy signal timestamps.\n",
    "        sell_signals_list: List to store sell signal timestamps.\n",
    "\n",
    "    Returns:\n",
    "        total_rewards: Total rewards accumulated during the test.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)  # Ensure deterministic behavior\n",
    "\n",
    "    # Reset the environment, get batch of observations\n",
    "    obs = env.reset()  # This will return a batch of observations for each parallel environment\n",
    "    done = [False] * len(obs)  # Done flags for each environment in the batch\n",
    "    total_rewards = [0] * len(obs)  # Total rewards for each environment\n",
    "\n",
    "    while not all(done):  # Run until all environments are done\n",
    "        for i in range(30):  # Run for a fixed number of steps per environment\n",
    "\n",
    "            # Extract the action mask for each environment (if applicable)\n",
    "            action_mask = [env.get_action_mask() for env in env.envs]  # Get action masks for all parallel environments\n",
    "\n",
    "            # Convert observations and masks to tensors\n",
    "            obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=model.device)\n",
    "            action_mask_tensor = torch.as_tensor(action_mask, dtype=torch.float32, device=model.device)\n",
    "\n",
    "            # Predict actions with action masking\n",
    "            with torch.no_grad():\n",
    "                action, _, _ = model.policy.forward(combined_obs=obs_tensor, action_mask=action_mask_tensor, deterministic=True)\n",
    "\n",
    "            # Convert actions to numpy\n",
    "            action = action.cpu().numpy()\n",
    "\n",
    "            # Step through the environment and get next observations, rewards, done flags, etc.\n",
    "            obs, rewards, dones, truncateds, infos = env.step(action)\n",
    "\n",
    "            # Debugging Output for each environment in the batch\n",
    "            for idx, done_flag in enumerate(dones):\n",
    "                print(f\"Step {i+1}: Environment {idx+1} | Action {action[idx]} | Reward: {rewards[idx]:.2f} | Done: {done_flag}\")\n",
    "\n",
    "                total_rewards[idx] += rewards[idx]\n",
    "\n",
    "                # Record buy or sell signals based on action\n",
    "                current_step = env.envs[idx].current_step  # Access current_step for each environment\n",
    "                if action[idx] == 1:  # Buy signal\n",
    "                    buy_signals_list.append(df.index[current_step])\n",
    "                elif action[idx] == 2:  # Sell signal\n",
    "                    sell_signals_list.append(df.index[current_step])\n",
    "\n",
    "                if done_flag or truncateds[idx]:  # If any environment ends\n",
    "                    print(f\"🎯 Total Reward for Environment {idx+1}: {total_rewards[idx]:.2f}\")\n",
    "                    obs[idx] = env.envs[idx].reset()  # Reset the environment and get new observation for that environment\n",
    "                    done[idx] = True  # Mark that this environment has finished\n",
    "\n",
    "    return sum(total_rewards)  # Return total rewards accumulated from all environments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc87bb8-5c03-4b9b-bb62-4b01cbb7d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single environment inside SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "import numpy as np\n",
    "\n",
    "def make_env():\n",
    "    return TradingEnv(df)\n",
    "\n",
    "# Wrap the environment with SubprocVecEnv (with just one environment for now)\n",
    "env = SubprocVecEnv([make_env])\n",
    "\n",
    "# Reset environment\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.random.choice(env.action_space.n)\n",
    "    action = np.array([action])  # Wrap the action in a numpy array to match the expected format\n",
    "    obs, reward, done, truncated, info = env.step(action)  # Pass the action as an array\n",
    "    print(f\"Action: {action} | Reward: {reward} | Done: {done}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335e62b-1e17-4517-b182-a9b5e5154316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output results from one model\n",
    "\n",
    "buy_signals = []\n",
    "sell_signals = []\n",
    "\n",
    "print(\"Recording signals\")\n",
    "\n",
    "test_agent(PPO.load(\"sb3_ppo_model\"), env, buy_signals, sell_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6cbf45-6811-4140-9ac2-805d2010a88a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Ensure directories exist\n",
    "import os\n",
    "import joblib  # or import pickle\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "os.makedirs(\"./tensorboard_logs/\", exist_ok=True)\n",
    "os.makedirs(\"./saved_models/\", exist_ok=True)\n",
    "\n",
    "def evaluate_model(model, env, num_episodes=10):\n",
    "    total_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        # Access the environment wrapped by DummyVecEnv and ActionMasker\n",
    "        env_inner = env.get_attr(\"env\", 0)[0]  # Get the first environment\n",
    "        \n",
    "        # Check if env_inner has an 'env' attribute (i.e., it's wrapped)\n",
    "        if hasattr(env_inner, \"env\"):  \n",
    "            env_inner = env_inner.env  # Unwrap ActionMasker if applicable\n",
    "\n",
    "        obs, info = env_inner.reset()  # Reset the environment\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            for i in range(30):  # Run for a fixed number of steps\n",
    "                # Extract the action mask from the original environment\n",
    "                action_mask = env_inner.get_action_mask()  # Access get_action_mask from TradingEnv inside ActionMasker\n",
    "    \n",
    "                # Convert observation and mask to tensors\n",
    "                obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=model.device).unsqueeze(0)\n",
    "                action_mask_tensor = torch.as_tensor(action_mask, dtype=torch.float32, device=model.device).unsqueeze(0)\n",
    "    \n",
    "                # Predict action with action masking\n",
    "                with torch.no_grad():\n",
    "                    action, _, _ = model.policy.forward(obs_tensor, action_mask=action_mask_tensor, deterministic=True)\n",
    "    \n",
    "                action = action.cpu().numpy()[0]  # Convert action tensor to numpy\n",
    "    \n",
    "                # Step the environment\n",
    "                obs, reward, done, truncated, info = env_inner.step(action)\n",
    "    \n",
    "                episode_reward += reward\n",
    "\n",
    "                if done or truncated:  # If the episode ends, reset the environment\n",
    "                    print(f\"🎯 Total Reward: {episode_reward:.2f}\")\n",
    "                    obs = env_inner.reset()  # Reset environment, which only returns the observation\n",
    "                    break  # Exit loop if episode ends\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "ent_coef_range = [0.0]       # Lower values for stability\n",
    "gamma_range = [0.99]              # Lower values for short-term focus\n",
    "gae_lambda_range = [0.9]        # Wider range to test variance\n",
    "n_steps_range = [4096]           # Larger values for more experiences\n",
    "clip_range_range = [0.1]         # Narrower range for stability\n",
    "n_epochs_range = [5]               # Fewer epochs for faster training\n",
    "batch_size_range = [128]           # Larger batches for better performance\n",
    "\n",
    "force = True\n",
    "\n",
    "timesteps = 500000\n",
    "\n",
    "for ent_coef, gamma, gae_lambda, n_steps, clip_range, n_epochs, batch_size in product(\n",
    "    ent_coef_range, gamma_range, gae_lambda_range, n_steps_range, clip_range_range, n_epochs_range, batch_size_range\n",
    "):\n",
    "    model_filename = f\"./saved_models/model_ent_coef={ent_coef}_gamma={gamma}_gae_lambda={gae_lambda}_n_steps={n_steps}_clip_range={clip_range}_n_epochs={n_epochs}_batch_size={batch_size}_timesteps={timesteps}.zip\"\n",
    "    \n",
    "    if os.path.exists(model_filename) and not force:\n",
    "        print(f\"Model already exists: {model_filename}, loading instead of training...\")\n",
    "        model = PPO.load(model_filename, env=env)  # Load existing model\n",
    "    else:\n",
    "        print(f\"Training new model with: ent_coef={ent_coef}, gamma={gamma}, gae_lambda={gae_lambda}, \"\n",
    "              f\"n_steps={n_steps}, clip_range={clip_range}, n_epochs={n_epochs}, batch_size={batch_size}, timesteps={timesteps}\")\n",
    "    \n",
    "        model = PPO(\n",
    "            MaskedPPOPolicy,  # Replace with your policy (e.g., \"MlpPolicy\" or \"CnnPolicy\")\n",
    "            env,\n",
    "            ent_coef=ent_coef,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            n_steps=n_steps,\n",
    "            clip_range=clip_range,\n",
    "            n_epochs=n_epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,  # Set to 0 for less output\n",
    "            tensorboard_log=\"./tensorboard_logs/\"\n",
    "        )\n",
    "        \n",
    "        # Train for x timesteps\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "    \n",
    "        # Save the model\n",
    "        model.save(model_filename)\n",
    "        print(f\"Model saved: {model_filename}\")\n",
    "    \n",
    "        # Store the model and its parameters\n",
    "        models.append({\n",
    "            \"model_filename\": model_filename,  # Store the path to the saved model\n",
    "            \"params\": {\n",
    "                \"ent_coef\": ent_coef,\n",
    "                \"gamma\": gamma,\n",
    "                \"gae_lambda\": gae_lambda,\n",
    "                \"n_steps\": n_steps,\n",
    "                \"clip_range\": clip_range,\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Evaluate the model\n",
    "    total_reward = evaluate_model(model, env)\n",
    "    print('Average total reward:', total_reward)\n",
    "\n",
    "    env.close()  # Close all subprocesses\n",
    "    \n",
    "    results.append({\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"clip_range\": clip_range,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_reward\": total_reward,\n",
    "        \"model_filename\": model_filename  # Link results to the saved model\n",
    "    })\n",
    "\n",
    "# Save results to a CSV file\n",
    "results_df = pd.DataFrame(results)\n",
    "csv_filename = f\"grid_search_results_{timesteps}_steps.csv\"\n",
    "results_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "base_csv, csv_ext = os.path.splitext(csv_filename)\n",
    "\n",
    "# Append a counter if the file already exists\n",
    "counter = 1\n",
    "while os.path.exists(csv_filename):\n",
    "    csv_filename = f\"{base_csv}_{counter}{csv_ext}\"\n",
    "    counter += 1\n",
    "\n",
    "# Save the results DataFrame to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Results saved to {csv_filename}\")\n",
    "\n",
    "# Now, do the same for the models file\n",
    "models_filename = \"./saved_models/models_list.joblib\"\n",
    "base_model, model_ext = os.path.splitext(models_filename)\n",
    "\n",
    "counter = 1\n",
    "while os.path.exists(models_filename):\n",
    "    models_filename = f\"{base_model}_{counter}{model_ext}\"\n",
    "    counter += 1\n",
    "\n",
    "# Save the models variable\n",
    "joblib.dump(models, models_filename)\n",
    "print(f\"Models saved to {models_filename}\")\n",
    "\n",
    "print(\"Grid search completed. Results saved to\", csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631d76c-4b56-4592-b082-8b32156deaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2050ff4-c41c-4847-b210-7b76c430d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0259a3-365c-4ab6-8960-72a9c66c3959",
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4db54d-abf3-4ea8-a13f-bff4d876a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show learned buy and sell signals\n",
    "\n",
    "import mplfinance as mpf\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame, buy_signals_1000, sell_signals_1000, buy_signals_10000, and sell_signals_10000 are your lists of timestamps\n",
    "\n",
    "# Function to plot the signals for a given buy and sell signal list\n",
    "def plot_signals(buy_signals, sell_signals, title):\n",
    "    # Convert buy/sell signals to DatetimeIndex\n",
    "    buy_signals = pd.to_datetime(buy_signals)  \n",
    "    buy_signals = buy_signals[buy_signals.isin(df_original.index)]  # Filter buy signals to match df.index\n",
    "\n",
    "    sell_signals = pd.to_datetime(sell_signals)  \n",
    "    sell_signals = sell_signals[sell_signals.isin(df_original.index)]  # Filter sell signals to match df.index\n",
    "\n",
    "    # Create new columns to mark the buy and sell signals with NaN for non-signals\n",
    "    buy_signal_prices = df_original['close'].copy()\n",
    "    buy_signal_prices[~df_original.index.isin(buy_signals)] = float('nan')  # Set non-buy signals to NaN\n",
    "    \n",
    "    sell_signal_prices = df_original['close'].copy()\n",
    "    sell_signal_prices[~df_original.index.isin(sell_signals)] = float('nan')  # Set non-sell signals to NaN\n",
    "\n",
    "    # Create addplots for buy and sell signals\n",
    "    buy_signal_plot = mpf.make_addplot(buy_signal_prices, type='scatter', markersize=40, marker='^', color='green')\n",
    "    sell_signal_plot = mpf.make_addplot(sell_signal_prices, type='scatter', markersize=40, marker='v', color='red')\n",
    "\n",
    "    # Plot with the added buy and sell signals\n",
    "    fig, axes = mpf.plot(\n",
    "        df_original,\n",
    "        type='ohlc',\n",
    "        datetime_format='%Y-%m-%d %H:%M',\n",
    "        addplot=[buy_signal_plot, sell_signal_plot],\n",
    "        returnfig=True,\n",
    "        figsize=(16, 8),\n",
    "        warn_too_much_data=10000,\n",
    "        title=title #Adjust the size again to maintain consistency\n",
    "    )\n",
    "\n",
    "plot_signals(buy_signals, sell_signals, title=\"Buy/Sell Signals\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728cfc9-e606-41b0-92a8-f4d05d577650",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import mplfinance as mpf\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate profit based on buy and sell signals\n",
    "def calculate_profit(buy_signals, sell_signals, df):\n",
    "    buy_prices = df.loc[buy_signals, 'close']\n",
    "    sell_prices = df.loc[sell_signals, 'close']\n",
    "\n",
    "    # Ensure that the number of buy signals is less than or equal to the number of sell signals\n",
    "    min_length = min(len(buy_prices), len(sell_prices))\n",
    "\n",
    "    # Calculate profit for each pair of buy and sell signals\n",
    "    profits = []\n",
    "    for i in range(min_length):\n",
    "        profit = sell_prices.iloc[i] - buy_prices.iloc[i]  # Profit = Sell Price - Buy Price\n",
    "        profits.append(profit)\n",
    "\n",
    "    total_profit = sum(profits)\n",
    "    return total_profit, profits\n",
    "\n",
    "# Function to plot the signals and compare profits\n",
    "def plot_signals_and_compare_profit(buy_signals, sell_signals, df, title):\n",
    "    # Convert buy/sell signals to DatetimeIndex\n",
    "    buy_signals = pd.to_datetime(buy_signals)  \n",
    "    buy_signals = buy_signals[buy_signals.isin(df.index)]  # Filter buy signals to match df.index\n",
    "\n",
    "    sell_signals = pd.to_datetime(sell_signals)  \n",
    "    sell_signals = sell_signals[sell_signals.isin(df.index)]  # Filter sell signals to match df.index\n",
    "\n",
    "    # Create new columns to mark the buy and sell signals with NaN for non-signals\n",
    "    buy_signal_prices = df['close'].copy()\n",
    "    buy_signal_prices[~df.index.isin(buy_signals)] = float('nan')  # Set non-buy signals to NaN\n",
    "\n",
    "    sell_signal_prices = df['close'].copy()\n",
    "    sell_signal_prices[~df.index.isin(sell_signals)] = float('nan')  # Set non-sell signals to NaN\n",
    "\n",
    "    # Create addplots for buy and sell signals\n",
    "    buy_signal_plot = mpf.make_addplot(buy_signal_prices, type='scatter', markersize=40, marker='^', color='green')\n",
    "    sell_signal_plot = mpf.make_addplot(sell_signal_prices, type='scatter', markersize=40, marker='v', color='red')\n",
    "\n",
    "    # Plot with the added buy and sell signals\n",
    "    fig, axes = mpf.plot(\n",
    "        df,\n",
    "        type='ohlc',\n",
    "        style='charles',\n",
    "        datetime_format='%Y-%m-%d %H:%M',\n",
    "        addplot=[buy_signal_plot, sell_signal_plot],\n",
    "        returnfig=True,\n",
    "        figsize=(20, 10),\n",
    "        warn_too_much_data=1000  # Adjust the size again to maintain consistency\n",
    "    )\n",
    "\n",
    "    # Customize x-axis\n",
    "    #axes[0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    #plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "    # Add legend manually\n",
    "    #plt.legend(['Buy Signals', 'Sell Signals'], loc='upper left')\n",
    "\n",
    "    # Add title\n",
    "    #plt.title(title)\n",
    "\n",
    "    # Show the plot\n",
    "    #plt.show()\n",
    "\n",
    "    # Calculate and print profit for the signals\n",
    "    total_profit, profits = calculate_profit(buy_signals, sell_signals, df)\n",
    "    print(f\"Total Profit for {title}: {total_profit:.2f}\")\n",
    "    return total_profit\n",
    "\n",
    "# Calculate and plot for 1000 timesteps signals\n",
    "profit_1000 = plot_signals_and_compare_profit(buy_signals_1000, sell_signals_1000, df, title=\"Profit for 1000 Timesteps\")\n",
    "\n",
    "# Calculate and plot for 10000 timesteps signals\n",
    "profit_10000 = plot_signals_and_compare_profit(buy_signals_10000, sell_signals_10000, df, title=\"Profit for 10000 Timesteps\")\n",
    "\n",
    "# Calculate and plot for 100000 timesteps signals\n",
    "#profit_100000 = plot_signals_and_compare_profit(buy_signals_100000, sell_signals_100000, df, title=\"Profit for 100000 Timesteps\")\n",
    "\n",
    "\n",
    "# Compare the total profits\n",
    "print(f\"Total Profit for 1000 Timesteps: {profit_1000:.2f}\")\n",
    "print(f\"Total Profit for 10000 Timesteps: {profit_10000:.2f}\")\n",
    "#print(f\"Total Profit for 100000 Timesteps: {profit_100000:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c85f7e-e0a3-41ac-ac12-f0759ffe732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7484208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:41:52) [Clang 15.0.7 ]\n",
      "/Users/larka/jupyter_env/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.executable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
