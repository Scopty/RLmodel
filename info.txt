# RL Trading Model Project

This project implements a Reinforcement Learning (RL) trading system using Proximal Policy Optimization (PPO) from Stable Baselines 3 (SB3). The model is designed for intraday trading on stocks that have gapped up 50%.

## Project Structure

```
RLmodel/
├── common_imports.py           # Shared imports and utilities
├── common_imports_ver1a.py     # Version-specific imports
├── trading_env.py             # Trading environment (gym.Env implementation)
├── training_script.py         # Script to train new models
├── test_script.py             # Script to test trained models
├── plot_signals.py            # Visualization of trading signals
├── debug_logs/                # Debug logs from test runs
│   └── [input_dir_name]/      # Logs organized by input directory
│       └── *.txt              # Debug log files
└── training_output/           # Output from training runs
    └── A[ID]_output_[bars]_[steps]_[timestamp]/
        ├── best_model_[suffix].zip    # Best model checkpoint
        ├── final_model_[suffix].zip   # Final trained model
        ├── vec_normalize_[suffix].pkl # Normalization stats
        ├── training_script.py         # Copy of training script
        ├── training_config.json       # Training configuration
        └── test_results/              # Test results (if tested)
            ├── trade_signals_*.csv    # Trade signals
            └── *.png                   # Performance plots
```

## Key Components

### 1. Trading Environment (`trading_env.py`)
- Implements the trading environment using OpenAI Gym's `gym.Env`
- Action space: 0=HOLD, 1=BUY, 2=SELL
- Observation space: 13 features including OHLCV, position info, and time features
- Reward function includes:
  - Small positive reward (+0.01) for BUY actions
  - Position-based rewards for SELL actions
  - Penalties for invalid actions

### 2. Training Script (`training_script.py`)
- Handles model training with PPO
- Supports parallel environments for faster training
- Saves checkpoints and training metrics
- Configurable hyperparameters

### 3. Testing Script (`test_script.py`)
- Evaluates trained models
- Generates trading signals and performance metrics
- Saves detailed test results and debug logs
- Supports visualization of trades

### 4. Visualization (`plot_signals.py`)
- Plots trading signals on price charts
- Visualizes buy/sell points
- Saves performance charts

## Setup

1. Activate the Python environment:
   ```bash
   source ~/jupyter_env/bin/activate
   ```

2. Install required packages (from `requirements.txt`)

## Usage

### Training a New Model
```bash
python training_script.py \
    --max_steps 100 \
    --total_timesteps 1000000 \
    --num_cpu 12 \
    --debug
```

### Testing a Trained Model
```bash
python test_script.py \
    --input training_output/A0001_output_100_bars_500000_timesteps_20250612_120000 \
    --output_dir test_results \
    --debug
```

### Plotting Trading Signals
```bash
python plot_signals.py \
    --input training_output/.../test_results/trade_signals_*.csv \
    --output_dir plots/
```

## Debugging
- Set `--debug` flag for verbose output
- Check `debug_logs/` for detailed logs
- Debug logs are organized by input directory and model name

## Notes
- Models are saved with sequential IDs (A0001, A0002, etc.)
- Each training run creates a timestamped directory in `training_output/`
- Test results include trade signals and performance metrics
- The reward function was modified to encourage more BUY actions (see memory for details)
